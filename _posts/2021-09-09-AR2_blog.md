---
toc: false 
layout: post
description: Sanchit Vohra (sv4@illinois.edu)
categories: [PixelRNN, Quantiles]
title: Review on Pixel recurrent neural networks
---

<div class="stackedit__html"><h1 id="pixel-recurrent-neural-networks">Pixel Recurrent Neural Networks</h1>
<p>Modeling the distribution of high-dimensional data is a central problem in unsupervised machine learning. Since images are high-dimensional and highly structured, estimating their underlying distribution is notoriously challenging. With the recent advances in deep learning, there has been significant progress in developing expressive, scalable, and tractable methods to tackle generative modeling problems. In this blog, we are going to explore the PixelRNN <sup class="footnote-ref"><a href="#fn1" id="fnref1">1</a></sup> and GatedPixelRNN <sup class="footnote-ref"><a href="#fn2" id="fnref2">2</a></sup> models for generating images.</p>
<h2 id="related-work">Related Work</h2>
<p>Perhaps the most popular technique for generative modeling in recent years has been the Generative Adversarial Network. These models generate rich and sharp images. However, GANs are notoriously hard to train because of instability due to the adversarial nature of training.<sup class="footnote-ref"><a href="#fn3" id="fnref3">3</a></sup></p>
<p>On the other hand, stochastic latent variable models such as the Variational Auto-Encoder produce blurry samples due to the nature of its reconstruction loss. Additionally, the VAE exproximates a lower bound (ELBO) to the desired probability distribution. <sup class="footnote-ref"><a href="#fn4" id="fnref4">4</a></sup></p>
<p>Previous methods that model the distribution as a product of conditionals such as NADE/MADE are limited because they lack sophisticated recurrent units like LSTM cells. <sup class="footnote-ref"><a href="#fn5" id="fnref5">5</a></sup> <sup class="footnote-ref"><a href="#fn6" id="fnref6">6</a></sup> By using sophisticated auto-regressive modeling techniques, the PixelRNN is able to achieve state-of-the-art performance on image generation benchmarks.</p>
<h2 id="background">Background</h2>
<h3 id="pixelrnn-model">PixelRNN Model</h3>
<p>In PixelRNN, each pixel is conditionally dependent on previous pixels from top to bottom and left to right. We model the joint probablity distribution of the image as a product of the conditional probabilities.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:1">1</a></sup> .</p>
<p><img src="https://i.imgur.com/WH2X2bY.png" alt="Joint probability as a product of conditionals"><br>
In the following image, the pixel <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.58056em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> depends on all pixels <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_1, x_2, ..., x_{i-1}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.208331em;"><span class=""></span></span></span></span></span></span><span class="mclose">]</span></span></span></span></span> from top to bottom and left to right. Visualization taken from <sup class="footnote-ref"><a href="#fn7" id="fnref7">7</a></sup> .</p>
<p><img src="https://i.imgur.com/54XNch7.png" alt="enter image description here"><br>
Additionally, each channel RGB is conditionally dependent on previous channels. For example, the green channel is conditionally dependent on the red channel of the same pixel.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:2">1</a></sup> .</p>
<p><img src="https://i.imgur.com/yf8LATD.png" alt="conditional rgb"><br>
As we will see later in this blog, ensuring this auto-regressive property holds requires clever masking of inputs in the network.</p>
<h3 id="discrete-softmax-output">Discrete Softmax Output</h3>
<p>The output layer of the network has a <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi><mo>×</mo><mn>3</mn><mo>×</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">n \times  n \times 3 \times 256</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">256</span></span></span></span></span> shape. This can be interpreted as each channel for each pixel having a 256 channel output. This 256 channel output is normalized via softmax and represents the discrete multinomial probability distribution for channel values. The following is a visualization of the softmax output for one channel for one pixel.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:3">1</a></sup> .</p>
<p><img src="https://i.imgur.com/SQqP1vm.png" alt="enter image description here"></p>
<h3 id="pixelrnn-generation">PixelRNN Generation</h3>
<p>Images are generated sequentially pixel-by-pixel and channel-by-channel from top-to-bottom and left-to-right. This makes the generation process extremely slow which is a big weakness of this model. However, training the PixelRNN model can be done in parallel since all the conditional inputs are present. The inputs just need to be masked to preserve the autoregressive property. Visualization taken from <sup class="footnote-ref"><a href="#fn7" id="fnref7:1">7</a></sup> .</p>
<p><img src="https://i.imgur.com/ha3CuJr.png" alt="enter image description here"></p>
<h2 id="pixelrnn-network-architecture">PixelRNN Network Architecture</h2>
<p>The model always start with a <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">7 \times 7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">7</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span></span></span></span></span> masked convolution. This is then followed by several residual blocks which can either be convolutional, RowLSTM, or DiagonalBiLSTM. Finally, there are two <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> convolutional layers to generate the final output.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:4">1</a></sup> .</p>
<p><img src="https://i.imgur.com/VEntbhZ.png" alt="enter image description here"></p>
<h3 id="input-masking">Input Masking</h3>
<p>There are two types of masks in the PixelRNN network. The first type (Mask A) exists to maintain the autoregressive property for the first <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>7</mn><mo>×</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">7 \times 7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">7</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">7</span></span></span></span></span> convolutional layer. In this mask, the output of the layer depends on all information from previous pixels and only information from previous channels of the same pixel. The second variant (Mask B) is applied to subsequent layers. This variant also allows the output of the layer to depend on the information from the same channel of the same pixel. This is because the channel values for subsequent layers only depends on inputs from previous channel and can be used without violating the autoregressive property. Below is a visualization of these two masking schemes.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:5">1</a></sup> .</p>
<p><img src="https://i.imgur.com/xHggSe9.png" alt="enter image description here"><br>
Below is a visualization of Mask A for the red, green, and blue channels respectively. We are masking the inputs to generate the center pixel for every channel.</p>
<p><img src="https://i.imgur.com/QxMWAP0.png" alt="enter image description here"><br>
Below is a visualization of Mask B for the red, green, and blue channels respectively. Note how the pixel of the same channel can be used this time.</p>
<p><img src="https://i.imgur.com/71YVLOq.png" alt="enter image description here"></p>
<h3 id="pixelcnn">PixelCNN</h3>
<p>In the PixelCNN each residual block is masked <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span> convolution. PixelCNN is heavily parallelizable due to its convolutional layers. However, as we are only looking at a <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span> neighborhood for the convolution, we are not capturing information from all previous pixels. While the receptive field of the convolution grows linearly with the depth of the network, in one particular layer, the masked <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span> has a small receptive field. The image below is a visualization of the receptive field of the masked <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">3</span></span></span></span></span> convolution.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:6">1</a></sup> .</p>
<p><img src="https://i.imgur.com/2NePePv.png" alt="enter image description here"></p>
<h3 id="rowlstm">RowLSTM</h3>
<p>RowLSTM generates its output row-by-row from top-to-bottom and left-to-right. To model the output, RowLSTM modifies the traditional LSTM cell to compute all hidden outputs via convolutions. RowLSTM uses <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">3 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> convolutions for the he state-to-state kernel <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mi>s</mi><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K^{ss}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.68333em; vertical-align: 0em;"></span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathnormal">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ss</span></span></span></span></span></span></span></span></span></span></span></span></span> and input-to-state kernel <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mi>i</mi><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K^{is}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathnormal">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span></span></span>. Note that the input-to-state component depends on the input and must be appropriately masked to ensure the autoregressive property. Additionally, since the input-to-state component depends only on the input, it can be computed for the entire <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">n</span></span></span></span></span> input in parallel. However, the state-to-state component of the RowLSTM convolution must be computed sequentially using previous hidden states. Below is the mathematical notation for the convolutional LSTM cell in RowLSTM.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:7">1</a></sup> .</p>
<p><img src="https://i.imgur.com/iJPlgsn.png" alt="enter image description here"><br>
The image below is a visualization of the convolutions in the RowLSTM. The <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">3 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span>  convolution slides left-to-right row-by-row. Visualization taken from <sup class="footnote-ref"><a href="#fn7" id="fnref7:2">7</a></sup> .</p>
<p><img src="https://i.imgur.com/kYLVloi.png" alt="enter image description here"><br>
Because of the sequential nature of the computation, RowLSTM is more computational intensive than convolutional layers. However, the hidden state for RowLSTM encapsulates a much larger context than convolutional layers. Specifically, the RowLSTM captures the entire triangular context above the output pixel. The image below is a visualization of the receptive field of the RowLSTM.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:8">1</a></sup> .</p>
<p><img src="https://i.imgur.com/EpwghH6.png" alt="enter image description here"></p>
<h3 id="diagonalbilstm">DiagonalBiLSTM</h3>
<p>While the RowLSTM is an improvement on the convolutional layers in terms of receptive field, there is still room for improvement. This is where the DiagonalBiLSTM comes in. The goal of the DiagonalBiLSTM is to capture all the available context. In order to accomplish this, the DiagonalBiLSTM scans the diagonals of the image from two directions; top-left to bottom-right and top-right to bottom-left. The outputs from these two scans are added together for the final output.<br>
Similar to RowLSTM, the DiagonalBiLSTM uses a convolutional LSTM framework with a <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> convolution for the input-to-state kernel <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mi>i</mi><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K^{is}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathnormal">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span></span></span> and a <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> convolution for the state-to-state kernel. Additionally, the <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mi>i</mi><mi>s</mi></mrow></msup></mrow><annotation encoding="application/x-tex">K^{is}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.824664em; vertical-align: 0em;"></span><span class="mord"><span style="margin-right: 0.07153em;" class="mord mathnormal">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">s</span></span></span></span></span></span></span></span></span></span></span></span></span> convolution must be masked to preserve the autoregressive property and can be precomuted for the entire output. Visualization taken from <sup class="footnote-ref"><a href="#fn7" id="fnref7:3">7</a></sup> .</p>
<p><img src="https://i.imgur.com/tUQkyd2.png" alt="enter image description here"><br>
The above image shows how the DiagonalBiLSTM generates its output from the top-left to bottom-right diagonal. Implementing this diagonal <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> is tricky. In order to simplify the compute, the image is skewed to rearrange the convolution as shown in the image below. Visualization taken from <sup class="footnote-ref"><a href="#fn7" id="fnref7:4">7</a></sup> .</p>
<p><img src="https://i.imgur.com/K1ExNlv.png" alt="enter image description here"><br>
Note that the the DiagonalBiLSTM computes this operation for both diagonals. As a result, it is able to capture all the available context to generate outputs and has a complete receptive field. However, the DiagonalBiLSTM has even more computational overhead because of computing two outputs.  Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:9">1</a></sup> .</p>
<p><img src="https://i.imgur.com/BRmLyjc.png" alt="enter image description here"><br>
The image above shows how the receptive field for the DiagonalBiLSTM is able to capture the entire available context to generate its output.</p>
<h3 id="residual-connections">Residual Connections</h3>
<p>As mentioned earlier, each of the blocks (convolutional, RowLSTM, DiagonalBiLSTM) are residual. Residual connections enable training deeper PixelRNN networks. These residual connections increase both convergence speed by propagating signals more directly through the network. The image below is a visualization on how residual connection are setup in the convolutional and LSTM cells. Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:10">1</a></sup> .</p>
<p><img src="https://i.imgur.com/GWAPBsM.png" alt="enter image description here"></p>
<h3 id="pixelrnn-model-summary">PixelRNN Model Summary</h3>
<p>This visualization summarizes the model architecture of the PixelCNN, RowLSTM, and Diagonal BiLSTM variants of the model. Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:11">1</a></sup> .</p>
<p><img src="https://i.imgur.com/fjVqw0T.png" alt="enter image description here"></p>
<h2 id="preliminary-results">Preliminary Results</h2>
<p>As shown in the image below, PixelRNN variants achieve state-of-the-art performance in common datasets (MNIST, CIFAR-10, and ImageNet). The best variant model is the DiagonalBiLSTM, which is expected since it has the largest receptive field. Image taken from paper <sup class="footnote-ref"><a href="#fn1" id="fnref1:12">1</a></sup> .</p>
<p><img src="https://i.imgur.com/Pkhh3rA.png" alt="enter image description here"></p>
<h2 id="gated-pixelcnn-motivation">Gated PixelCNN Motivation</h2>
<p>The authors of the PixelRNN paper released another paper shortly after the first one that improved upon the design on the PixelCNN. The authors reasoned that PixelRNN variants (RowLSTM and DiagonalBiLSTM) are outperforming PixelCNN for two main reasons:</p>
<ol>
<li>The element-wise multiplicative units are able to model more complex interactions. The absence of multiplicative operations in PixelCNN is limiting its performance.</li>
<li>PixelRNN’s capture much larger receptive fields. While the receptive fields of the PixelCNN grows linearly with the number of layers, there is a blind spot that forms in the receptive field of masked CNNs (more information below).</li>
</ol>
<p>The authors proposed modification to the PixelCNN architecture to fix its shortcomings. First, they added Gated Activation Units that contained multiplicative operations to add more sophistication to the model. Second, they fixed the receptive field blind-spot problem by splitting up the convolution into an unmasked vertical stack and a masked horizontal stack that takes the output of the vertical stack as input.</p>
<h3 id="horizontal-and-vertical-stack">Horizontal and Vertical Stack</h3>
<p>As mentioned above, the receptive field of the PixelCNN, while increasing linearly with depth, contains a growing blind-spot. Pixels in this blind-spot are never used as context regardless of how many layers are stacked. The blind-spot problem is caused because of the masking in the convolutions to maintain the autoregressive property.  The image below is a visualization of the blind-spot problem. Image taken from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:1">2</a></sup> .</p>
<p><img src="https://i.imgur.com/MmEJeWh.png" alt="enter image description here"></p>
<p>To fix the receptive field blind-spot, the single masked convolution is replaced with a horizontal and vertical stack. The vertical stack is an unmasked operation that captures the entire receptive field in the rows above the output pixel. The horizontal stack is a masked operation that captures the context to the left of the output pixels and uses the vertical stack output as input. Also, the authors add an additional residual connection in the horizontal stack convolution. Splitting up the convolution in this way fixes the receptive field blind-spot as shown in the figure below. Image taken from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:2">2</a></sup> .</p>
<p><img src="https://i.imgur.com/yzFCvln.png" alt="enter image description here"></p>
<h3 id="gated-activation-units">Gated Activation Units</h3>
<p>Additionally, the authors replace the ReLU activation between convolutional blocks with a more sophisticated gated activation unit. This new activation computes two different convolutions with half the feature maps. The output of the two convolutions are subjected to two different non-linear activation functions (tanh and sigmoid) and multiplied together element-wise for the final output.  Image taken from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:3">2</a></sup> .</p>
<p><img src="https://i.imgur.com/hgGY2eY.png" alt="enter image description here"><br>
The image above shows the mathematical definition of the gated activation unit. The final convolutional block is shown in the image below. The green <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">n</span></span></span></span></span> convolution is the vertical stack and the green <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.66666em; vertical-align: -0.08333em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> convolution is the horizontal stack. The output of the vertical stack is fed into the horizontal stack as mentioned above. Image taken from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:4">2</a></sup> .</p>
<p><img src="https://i.imgur.com/00Dz4I1.png" alt="enter image description here"></p>
<h3 id="conditional-pixelcnn">Conditional PixelCNN</h3>
<p>In my opinion, the most interesting part of the paper is adding the ability to conditionally generate images. You can condition the output probability distribution of the images on a high-dimensional latent vector <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">h</span></span></span></span></span> that behaves as the image description. For example, the <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">h</span></span></span></span></span> vector could be a one-hot encoded vector of class labels in the ImageNet dataset. The PixelCNN network would then learn to conditionally generate specific classes of ImageNet data. So passing in the latent vector <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">h</span></span></span></span></span> corresponding to the class “Dog” would generate images of dogs! The equation given below shows how the output probability is now conditionally dependent on <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">h</span></span></span></span></span>. Image taken from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:5">2</a></sup> .</p>
<p><img src="https://i.imgur.com/ccs7GDV.png" alt="enter image description here"></p>
<p>However, the latent vector <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">h</span></span></span></span></span> does not contain any spatial information about the object. So in the above example, while images of dogs would be generated, the dog could appear anywhere in the image. Fortunately, the authors of the paper had a solution to this problem. The latent vector <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">h</span></span></span></span></span> can be passed through a deconvolution network to produce output <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mi>m</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s = m(h)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mclose">)</span></span></span></span></span> such that <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">s</span></span></span></span></span> has the same spatial dimensions as the image but arbitrary channels. <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">s</span></span></span></span></span> contains spatial information about the generated object. Now you can control where in the image the dog is generated!</p>
<p>The equations below show how the gated activation unit in the network is modified to accommodate conditional generation. For the <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">h</span></span></span></span></span>, the <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">V_{k,f}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span style="margin-right: 0.22222em;" class="mord mathnormal">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span style="margin-right: 0.03148em;" class="mord mathnormal mtight">k</span><span class="mpunct mtight">,</span><span style="margin-right: 0.10764em;" class="mord mathnormal mtight">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span> is a linear layer and for <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">s</span></span></span></span></span> the <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">V_{k,f}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.969438em; vertical-align: -0.286108em;"></span><span class="mord"><span style="margin-right: 0.22222em;" class="mord mathnormal">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em;"><span class="" style="top: -2.55em; margin-left: -0.22222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span style="margin-right: 0.03148em;" class="mord mathnormal mtight">k</span><span class="mpunct mtight">,</span><span style="margin-right: 0.10764em;" class="mord mathnormal mtight">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em;"><span class=""></span></span></span></span></span></span></span></span></span></span> is a <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.64444em; vertical-align: 0em;"></span><span class="mord">1</span></span></span></span></span> convolution. Image taken from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:6">2</a></sup> .</p>
<p><img src="https://i.imgur.com/v5FIIrP.png" alt="enter image description here"></p>
<h2 id="final-results">Final Results</h2>
<p>The image below shows the performance of the GatedPixelCNN on CIFAR-10 (left) and ImageNet (right) from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:7">2</a></sup> .</p>
<p><img src="https://i.imgur.com/Ibl9oK7.png" alt="enter image description here"></p>
<h2 id="conditional-generation-examples">Conditional Generation Examples</h2>
<p>Here are some examples from the paper for condtional image generation from the ImageNet dataset from paper <sup class="footnote-ref"><a href="#fn2" id="fnref2:8">2</a></sup> .</p>
<p><img src="https://i.imgur.com/Ty2OcKn.png" alt="Image description"></p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Oord, Aaron van den, et al. “Pixel Recurrent Neural Networks.” ArXiv:1601.06759 [Cs], Aug. 2016. <a href="https://urldefense.com/v3/__http://arXiv.org__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf08BnoCLA$">arXiv.org</a>, <a href="https://urldefense.com/v3/__https://arxiv.org/abs/1601.06759__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf1FRBUCWg$">http://arxiv.org/abs/1601.06759</a> <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a> <a href="#fnref1:2" class="footnote-backref">↩︎</a> <a href="#fnref1:3" class="footnote-backref">↩︎</a> <a href="#fnref1:4" class="footnote-backref">↩︎</a> <a href="#fnref1:5" class="footnote-backref">↩︎</a> <a href="#fnref1:6" class="footnote-backref">↩︎</a> <a href="#fnref1:7" class="footnote-backref">↩︎</a> <a href="#fnref1:8" class="footnote-backref">↩︎</a> <a href="#fnref1:9" class="footnote-backref">↩︎</a> <a href="#fnref1:10" class="footnote-backref">↩︎</a> <a href="#fnref1:11" class="footnote-backref">↩︎</a> <a href="#fnref1:12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Oord, Aaron van den, et al. “Conditional Image Generation with PixelCNN Decoders.” ArXiv:1606.05328 [Cs], June 2016. <a href="https://urldefense.com/v3/__http://arXiv.org__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf08BnoCLA$">arXiv.org</a>, <a href="https://urldefense.com/v3/__https://arxiv.org/abs/1606.05328__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf1yhkvdBg$">http://arxiv.org/abs/1606.05328</a> <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a> <a href="#fnref2:2" class="footnote-backref">↩︎</a> <a href="#fnref2:3" class="footnote-backref">↩︎</a> <a href="#fnref2:4" class="footnote-backref">↩︎</a> <a href="#fnref2:5" class="footnote-backref">↩︎</a> <a href="#fnref2:6" class="footnote-backref">↩︎</a> <a href="#fnref2:7" class="footnote-backref">↩︎</a> <a href="#fnref2:8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Recent research has made progress in demystifying the problems in training GANs. However, when the initial PixelRNN paper was published in 2016, training GANs for generative modeling was still a daunting task). <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>Advances in VAE have made it possible to generative sharp high-dimensional data by using hierarchical techniques and modifying the ELBO loss for better reconstructions. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Uria, Benigno, et al. “Neural Autoregressive Distribution Estimation.” <em>ArXiv:1605.02226 [Cs]</em>, May 2016. <em><a href="https://urldefense.com/v3/__http://arXiv.org__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf08BnoCLA$">arXiv.org</a></em>, <a href="https://urldefense.com/v3/__http://arxiv.org/abs/1605.02226__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf1Kckw54w$">http://arxiv.org/abs/1605.02226</a>. <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Germain, Mathieu, et al. “MADE: Masked Autoencoder for Distribution Estimation.” <em>ArXiv:1502.03509 [Cs, Stat]</em>, June 2015. <em><a href="https://urldefense.com/v3/__http://arXiv.org__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf08BnoCLA$">arXiv.org</a></em>, <a href="https://urldefense.com/v3/__http://arxiv.org/abs/1502.03509__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf1HcTuTSQ$">http://arxiv.org/abs/1502.03509</a>. <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Slides from UCF PixelRNN presentation by Logan Lebanoff 2/22/17. <a href="https://urldefense.com/v3/__https://www.crcv.ucf.edu/wp-content/uploads/2019/03/CAP6412_Spring2018_Pixel-Recurrent-Neural-Networks.pdf__;!!DZ3fjg!vB4FVPCPBjpvKyd9NAPO1XCiq0V9iTtXeJvEbjsVbD5D-pqlRbVpR1yOvf04wyF4LA$">https://www.crcv.ucf.edu/wp-content/uploads/2019/03/CAP6412_Spring2018_Pixel-Recurrent-Neural-Networks.pdf</a> <a href="#fnref7" class="footnote-backref">↩︎</a> <a href="#fnref7:1" class="footnote-backref">↩︎</a> <a href="#fnref7:2" class="footnote-backref">↩︎</a> <a href="#fnref7:3" class="footnote-backref">↩︎</a> <a href="#fnref7:4" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</div>