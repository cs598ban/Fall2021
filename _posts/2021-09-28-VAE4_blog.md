---
toc: false 
layout: post
description: Shengyu Feng (shengyu8@illinois.edu)
categories: [Variational Autoencoder]
title: Review on Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations 

---

  <div class="stackedit__html"><h1 id="challenging-common-assumptions-in-the-unsupervised-learning-of-disentangled-representations">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</h1>
<p>This is the best paper [2] in ICML 2019, which incurred huge controversy at that time. It heavily criticizes the previous works on disentanglement, but some claims of it are regarded to be too strong. I will introduce those assumptions challenged by this paper. Although I find some arguments not well supported, most of the conclusions from this paper are actually valuable and inspiring for the later works on the disentanglement.</p>
<h2 id="introduction-to-disentanglement">Introduction to disentanglement</h2>
<p>There’s actually no formal definition of disentanglement right now. Intuitively, disentangled representation should be compact and interpretable, where each dimension of the representation is informative and independent. Consider two independent random variables <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">a</span></span></span></span></span> and <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">b</span></span></span></span></span>, then <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>=</mo><mo stretchy="false">[</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo separator="true">,</mo><mi>a</mi><mo>−</mo><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{x}=[a+b,a-b]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.44444em; vertical-align: 0em;"></span><span class="mord mathbf">x</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">]</span></span></span></span></span> is an entangled representation while <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>=</mo><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{x}=[a,b]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.44444em; vertical-align: 0em;"></span><span class="mord mathbf">x</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em;"></span><span class="mord mathnormal">b</span><span class="mclose">]</span></span></span></span></span> is a disentangled representation. These two representations actually contain the same information about <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">a</span></span></span></span></span> and <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord mathnormal">b</span></span></span></span></span> but the disentangled representation is expected to be more interpretable and more useful for downstream tasks, such as controllable sample generation and robot manipulation.</p>
<p>For a long period, many VAE-based methods like <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathnormal" style="margin-right: 0.05278em;">β</span></span></span></span></span>-VAE [1], with additional tricks to encourage the dimension independence of the latent representation, have been proposed for disentanglement. But all these methods are based on some common assumptions and they are not carefully verified.</p>
<h2 id="disentanglement-is-impossible-without-inductive-bias">Disentanglement is impossible without inductive bias</h2>
<p>This paper claims that, for an arbitrary generative model, the disentanglement is actually impossible. For each disentanglement representation <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal" style="margin-right: 0.04398em;">z</span></span></span></span></span>, there exists an inifinite family of bijective functions <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.04398em;">z</span><span class="mclose">)</span></span></span></span></span>, where <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.04398em;">z</span><span class="mclose">)</span></span></span></span></span> is entangled but it shares the same marginal distribution with <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal" style="margin-right: 0.04398em;">z</span></span></span></span></span>. In other words, there are infinitely many generative models which have the same marginal distribution for the observation <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.43056em; vertical-align: 0em;"></span><span class="mord mathnormal">x</span></span></span></span></span>, and without inductive bias, there’s no guarantee the one we obtain gives the disentangled representation. This theorem is also similar to the well-known “No free lunch theorem” [9]. Therefore, it’s necessary for each disentanglement method to clearly define its inductive bias.</p>
<h2 id="challenging-the-common-assumptions-behind-disentanglement-learning">Challenging the common assumptions behind disentanglement learning</h2>
<p>This paper investigates several assumptions behind the disentanglement learning. It considers 6 distanglement methods, including <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathnormal" style="margin-right: 0.05278em;">β</span></span></span></span></span>-VAE [1], AnnealedVAE [6], FactorVAE [5], <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathnormal" style="margin-right: 0.05278em;">β</span></span></span></span></span>-TCVAE [3], DIP-VAE-I and DIP-AVE-II [4]. It also uses 6 metrics for measuring disentanglement, including <em>BetaVAE</em> metric [1], <em>FactorVAE</em> metric [5], <em>Mutual Information GAP (MIG)</em> [3], <em>Modularity</em> [7], <em>DCI Disentanglement gap</em> (named as “disentanglement metric” originally) [8], and <em>SAP score</em> [4]. The experiments are conducted on datasets <em>dSprites</em>, <em>Cars3D</em>, <em>SmallNORB</em>, <em>Shapes3D</em>, <em>Color-dSprites</em>, <em>Noisy-dSprites</em> and <em>Scream-dSprites</em>.</p>
<h3 id="mean-representation-of-the-latent-variables-are-correlated">Mean representation of the latent variables are correlated</h3>
<p>It’s a common practice to use the mean vector of the Gaussian encoder as the representation of the latent variable for evaluation. However, it turns out that although the samples from the Gaussian encoder have uncorrelated dimensions, the mean vector doesn’t internally have this property. Constrained by a stronger regularization, as shown in Fig 1, the total correlation, which measures the correlation among dimensions, of the sampled representation indeed goes down (left) but the total correlation of the mean representation increases (right) instead, except for DIP-VAE-I which directly optimizes the covariance matrix of the mean representation to be diagonal.</p>
<figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig1.png" width="600">
<figcaption>Fig 1. Total correlation among dimensions of latent representations, mean representation (left) and sampled representation (right). Source: Locatello et al. [2]</figcaption>
</figure>    
<h3 id="disentanglement-metrics-are-correlated">Disentanglement metrics are correlated</h3>
<p>The second question is whether all these metrics measuring the disentanglement are correlated. And the results give the positive answer. All metrics except <em>Modularity</em> are mildly correlated.</p>
<figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig2.png" width="500">
<figcaption>Fig 2. Correlation among metrics. Source: Locatello et al. [2]</figcaption>
</figure>   
<h3 id="importance-of-models-and-hyperparameters">Importance of models and hyperparameters</h3>
<p>All these methods claim that they get a better disentangled representation, but whether the improvement in their metrics is from more disentanglement remains unknown. In the experiment, each model is run over different random seeds, but it turns out that these methods have large overlappings (left in Fig 3) in their performances. In other words, a good random seed is more meaningful than a good objective. The same conclusion holds for the hyperparameter (right in Fig 3).</p>
 <figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig3.png" width="600">
<figcaption>Fig 3. Violin plots of disentanglement scores over random seeds for different models (left) and different hyperparameters (right). Source: Locatello et al. [2]</figcaption>
</figure>  
<h3 id="recipes-for-hyperparameter-selection">Recipes for hyperparameter selection</h3>
<p>The paper now considers the strategy to select a good hyperparameter for a model. However, all these metrics require a substantial amount of labels or a full generative model, so we need to consider the hyperparameter selection in an unsupervised manner. Unfortunately, no model could dominate others all the time and there does not exist a hyperparameter selection strategy that works consistently well as shown in Fig 4. Additionally, there’s also no strategy to identify a good and a bad run for different random seeds.</p>
  <figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig4.png" width="1000">
<figcaption>Fig 4. Model performances under different hyperparameters on different datasets. Source: Locatello et al. [2]</figcaption>
</figure>    
<p>Specifically, the paper investigates the unsupervised losses and transfer performances, which can also serve as a strategy to select the hyperparamter without supervision on the target dataset. For the unsupervised losses, including the reconstruction error, KL divergence between the prior and the approximate posterior, evidence lower bound (ELBO), and the estimated total correlation of the sampled representation, none of them are actually correlated with the disentanglement metrics (Fig 5).</p>
<figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig5.png" width="900">
<figcaption>Fig 5. Correlation between disentanglement scores and unsupervised losses. Source: Locatello et al. [2]</figcaption>
</figure>    
The transferring fails as well. When the model is transferred across the same metric and same dataset (different random seeds), there's 80.7% chance the model performance is not worse than the random model selection. However, this result drops to 59.3% for different datasets and further drops to 54.9% when metrics are also different. One example is shown in Fig 6.
<figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig6.png" width="500">
<figcaption>Fig 6. Model performance after transferring. Source: Locatello et al. [2]</figcaption>
</figure> 
<h3 id="benefits-of-disentanglement">Benefits of disentanglement</h3>
<p>Finally, this paper explores the benefits of the disentanglement. The disentangled representation is intuitively believed to be more useful for downstream tasks, and able to reduce the sample complexity of learning. In the experiments, the downstream performances show high correlation with the disentanglement scores (Fig 7), but the authors are careful with the conclusion and doubts the source of the correlation, which could be either the disentanglement or the relevant information embedded in the representation. I think the experiments here are incomplete, where authors can actually build entangled representations from the disentangled ones and evaluate the performance of the entangled representations. This comparison could give the idea where the correlation comes from.</p>
<figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig7.png" width="500">
<figcaption>Fig 7. Correlation between downstream performances and disentanglement scores. Source: Locatello et al. [2]</figcaption>
</figure> 
Besides, the experimental results show no clear correlation between the disentanglement scores and sample efficiencies (Fig 8).
<figure align="center">
<img src="https://raw.githubusercontent.com/Shengyu-Feng/Public_images/master/blog_vae4/fig8.png" width="500">
<figcaption>Fig 8. Correlation between sample efficiencies and disentanglement scores. Source: Locatello et al. [2]</figcaption>
</figure> 
<h2 id="future-directions">Future directions</h2>
<p>This paper proposes three principles for the future work on the disentanglement based on the experiments before.</p>
<ol>
<li>
<p><strong>Inductive biases and implicit and explicit supervision.</strong> As proved by this paper, the inductive bias is necessary for the disentangled methods, which should be made clear in the later works. Besides, it’s demonstrated by the experimental results that it’s impossible for the hyperparameter selection under no supervision, the supervision parts should also be explicitly specified.</p>
</li>
<li>
<p><strong>Concrete practical benefits of disentangled representations.</strong> Previous works take it for grant that disentangled representation is better, however, this paper points out its benefits is not clear yet and quite data dependent. Therefore, the concrete benefits of disentangled representations should be specified under each context.</p>
</li>
<li>
<p><strong>Experimental setup and diversity of data sets.</strong> It’s shown that no model can consistently outperform others on all datasets, so it’s questionable whether these models really improve the disentanglement. A sound, robust, and reproducible experimental setup on a diverse set of data sets is needed to demonstrate the advantage of a disentangled method.</p>
</li>
</ol>
<h2 id="references">References</h2>
<ul>
<li>[1] 2017 (ICLR): I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, A. Lerchner. <a href="https://openreview.net/forum?id=Sy2fzU9gl">beta-VAE: Learning basic visual concepts with a constrained variational framework</a>. ICLR, 2017.</li>
<li>[2] 2019 (ICML): F. Locatello, S. Bauer, M. Lucic, G. RÃ¤tsch, S. Gelly, B. Scholkopf, O. Bachem. <a href="https://arxiv.org/abs/1811.12359">Challenging common assumptions in the unsupervised learning of disentangled representations</a>. ICML, 2019.</li>
<li>[3] 2018 (NeurIPS): T. Chen, X. Li, R. Grosse, D. Duvenaud.  <a href="https://arxiv.org/abs/1802.04942">Isolating sources of disentanglement in variational autoencoders</a>. NeurIPS, 2018.</li>
<li>[4] 2018 (ICLR): A. Kumar, P. Sattigeri, A. Balakrishnan. <a href="https://arxiv.org/abs/1711.00848">Variational inference of disentangled latent concepts from unlabeled observations</a>. ICLR, 2018.</li>
<li>[5] 2018 (ICML): H. Kim, A. Mnih. <a href="https://arxiv.org/abs/1802.05983">Disentangling by factorising</a>. NIPS, 2017.</li>
<li>[6] 2017 (NIPS): C. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, A. Lerchner. <a href="https://arxiv.org/abs/1804.03599">Understanding disentangling in <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.88888em; vertical-align: -0.19444em;"></span><span class="mord mathnormal" style="margin-right: 0.05278em;">β</span></span></span></span></span>-VAE</a>. NIPS, 2017.</li>
<li>[7] 2018 (NIPS):  K. Ridgeway, M. Mozer. <a href="https://arxiv.org/abs/1802.05312">Learning deep disentangled embeddings with the f-statistic loss</a>. NIPS, 2018.</li>
<li>[8] 2018 (ICLR): C. Eastwood, C. Williams. <a href="https://openreview.net/pdf?id=By-7dz-AZ">A framework for the quantitative evaluation of disentangled representations</a>. ICLR, 2018.</li>
<li>[9] 1997 (IEEE): D. Wolpert, W. Macready. <a href="https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf">No Free Lunch Theorems for Optimization</a>. IEEE Transactions on Evolutionary Computation, 1997.</li>
</ul>
</div>
