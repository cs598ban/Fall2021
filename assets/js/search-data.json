{
  
    
        "post0": {
            "title": "Review on Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
            "content": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations . This is the best paper [2] in ICML 2019, which incurred huge controversy at that time. It heavily criticizes the previous works on disentanglement, but some claims of it are regarded to be too strong. I will introduce those assumptions challenged by this paper. Although I find some arguments not well supported, most of the conclusions from this paper are actually valuable and inspiring for the later works on the disentanglement. . Introduction to disentanglement . There’s actually no formal definition of disentanglement right now. Intuitively, disentangled representation should be compact and interpretable, where each dimension of the representation is informative and independent. Consider two independent random variables aaa and bbb, then x=[a+b,a−b] mathbf{x}=[a+b,a-b]x=[a+b,a−b] is an entangled representation while x=[a,b] mathbf{x}=[a,b]x=[a,b] is a disentangled representation. These two representations actually contain the same information about aaa and bbb but the disentangled representation is expected to be more interpretable and more useful for downstream tasks, such as controllable sample generation and robot manipulation. . For a long period, many VAE-based methods like β betaβ-VAE [1], with additional tricks to encourage the dimension independence of the latent representation, have been proposed for disentanglement. But all these methods are based on some common assumptions and they are not carefully verified. . Disentanglement is impossible without inductive bias . This paper claims that, for an arbitrary generative model, the disentanglement is actually impossible. For each disentanglement representation zzz, there exists an inifinite family of bijective functions f(z)f(z)f(z), where f(z)f(z)f(z) is entangled but it shares the same marginal distribution with zzz. In other words, there are infinitely many generative models which have the same marginal distribution for the observation xxx, and without inductive bias, there’s no guarantee the one we obtain gives the disentangled representation. This theorem is also similar to the well-known “No free lunch theorem” [9]. Therefore, it’s necessary for each disentanglement method to clearly define its inductive bias. . Challenging the common assumptions behind disentanglement learning . This paper investigates several assumptions behind the disentanglement learning. It considers 6 distanglement methods, including β betaβ-VAE [1], AnnealedVAE [6], FactorVAE [5], β betaβ-TCVAE [3], DIP-VAE-I and DIP-AVE-II [4]. It also uses 6 metrics for measuring disentanglement, including BetaVAE metric [1], FactorVAE metric [5], Mutual Information GAP (MIG) [3], Modularity [7], DCI Disentanglement gap (named as “disentanglement metric” originally) [8], and SAP score [4]. The experiments are conducted on datasets dSprites, Cars3D, SmallNORB, Shapes3D, Color-dSprites, Noisy-dSprites and Scream-dSprites. . Mean representation of the latent variables are correlated . It’s a common practice to use the mean vector of the Gaussian encoder as the representation of the latent variable for evaluation. However, it turns out that although the samples from the Gaussian encoder have uncorrelated dimensions, the mean vector doesn’t internally have this property. Constrained by a stronger regularization, as shown in Fig 1, the total correlation, which measures the correlation among dimensions, of the sampled representation indeed goes down (left) but the total correlation of the mean representation increases (right) instead, except for DIP-VAE-I which directly optimizes the covariance matrix of the mean representation to be diagonal. . Fig 1. Total correlation among dimensions of latent representations, mean representation (left) and sampled representation (right). Source: Locatello et al. [2] Disentanglement metrics are correlated . The second question is whether all these metrics measuring the disentanglement are correlated. And the results give the positive answer. All metrics except Modularity are mildly correlated. . Fig 2. Correlation among metrics. Source: Locatello et al. [2] Importance of models and hyperparameters . All these methods claim that they get a better disentangled representation, but whether the improvement in their metrics is from more disentanglement remains unknown. In the experiment, each model is run over different random seeds, but it turns out that these methods have large overlappings (left in Fig 3) in their performances. In other words, a good random seed is more meaningful than a good objective. The same conclusion holds for the hyperparameter (right in Fig 3). . Fig 3. Violin plots of disentanglement scores over random seeds for different models (left) and different hyperparameters (right). Source: Locatello et al. [2] Recipes for hyperparameter selection . The paper now considers the strategy to select a good hyperparameter for a model. However, all these metrics require a substantial amount of labels or a full generative model, so we need to consider the hyperparameter selection in an unsupervised manner. Unfortunately, no model could dominate others all the time and there does not exist a hyperparameter selection strategy that works consistently well as shown in Fig 4. Additionally, there’s also no strategy to identify a good and a bad run for different random seeds. . Fig 4. Model performances under different hyperparameters on different datasets. Source: Locatello et al. [2] Specifically, the paper investigates the unsupervised losses and transfer performances, which can also serve as a strategy to select the hyperparamter without supervision on the target dataset. For the unsupervised losses, including the reconstruction error, KL divergence between the prior and the approximate posterior, evidence lower bound (ELBO), and the estimated total correlation of the sampled representation, none of them are actually correlated with the disentanglement metrics (Fig 5). . Fig 5. Correlation between disentanglement scores and unsupervised losses. Source: Locatello et al. [2] The transferring fails as well. When the model is transferred across the same metric and same dataset (different random seeds), there&#39;s 80.7% chance the model performance is not worse than the random model selection. However, this result drops to 59.3% for different datasets and further drops to 54.9% when metrics are also different. One example is shown in Fig 6. Fig 6. Model performance after transferring. Source: Locatello et al. [2] Benefits of disentanglement . Finally, this paper explores the benefits of the disentanglement. The disentangled representation is intuitively believed to be more useful for downstream tasks, and able to reduce the sample complexity of learning. In the experiments, the downstream performances show high correlation with the disentanglement scores (Fig 7), but the authors are careful with the conclusion and doubts the source of the correlation, which could be either the disentanglement or the relevant information embedded in the representation. I think the experiments here are incomplete, where authors can actually build entangled representations from the disentangled ones and evaluate the performance of the entangled representations. This comparison could give the idea where the correlation comes from. . Fig 7. Correlation between downstream performances and disentanglement scores. Source: Locatello et al. [2] Besides, the experimental results show no clear correlation between the disentanglement scores and sample efficiencies (Fig 8). Fig 8. Correlation between sample efficiencies and disentanglement scores. Source: Locatello et al. [2] Future directions . This paper proposes three principles for the future work on the disentanglement based on the experiments before. . Inductive biases and implicit and explicit supervision. As proved by this paper, the inductive bias is necessary for the disentangled methods, which should be made clear in the later works. Besides, it’s demonstrated by the experimental results that it’s impossible for the hyperparameter selection under no supervision, the supervision parts should also be explicitly specified. . | Concrete practical benefits of disentangled representations. Previous works take it for grant that disentangled representation is better, however, this paper points out its benefits is not clear yet and quite data dependent. Therefore, the concrete benefits of disentangled representations should be specified under each context. . | Experimental setup and diversity of data sets. It’s shown that no model can consistently outperform others on all datasets, so it’s questionable whether these models really improve the disentanglement. A sound, robust, and reproducible experimental setup on a diverse set of data sets is needed to demonstrate the advantage of a disentangled method. . | References . [1] 2017 (ICLR): I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, A. Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017. | [2] 2019 (ICML): F. Locatello, S. Bauer, M. Lucic, G. RÃ¤tsch, S. Gelly, B. Scholkopf, O. Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. ICML, 2019. | [3] 2018 (NeurIPS): T. Chen, X. Li, R. Grosse, D. Duvenaud. Isolating sources of disentanglement in variational autoencoders. NeurIPS, 2018. | [4] 2018 (ICLR): A. Kumar, P. Sattigeri, A. Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. ICLR, 2018. | [5] 2018 (ICML): H. Kim, A. Mnih. Disentangling by factorising. NIPS, 2017. | [6] 2017 (NIPS): C. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, A. Lerchner. Understanding disentangling in β betaβ-VAE. NIPS, 2017. | [7] 2018 (NIPS): K. Ridgeway, M. Mozer. Learning deep disentangled embeddings with the f-statistic loss. NIPS, 2018. | [8] 2018 (ICLR): C. Eastwood, C. Williams. A framework for the quantitative evaluation of disentangled representations. ICLR, 2018. | [9] 1997 (IEEE): D. Wolpert, W. Macready. No Free Lunch Theorems for Optimization. IEEE Transactions on Evolutionary Computation, 1997. | .",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/28/VAE4_blog.html",
            "relUrl": "/variational%20autoencoder/2021/09/28/VAE4_blog.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Review on Diagnosing and Enhancing VAE Models",
            "content": "Diagnosing and Enhancing VAE Models (ICLR &#39;19)1 . Introduction . Even though variational autoencoders (VAEs)2 have a wide variety applications in deep generative models, many aspects of the underlying energy function remain poorly understod. It is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. . In this paper, the authors rigorously analyzed that reaching the global optimum does not guarantee that if VAE model can learn the true distribution of data, i.e., there could exist alternative solutions that both reach the global optimum and yet do not assign the same probability measure as ground-truth probability distribution. And it also proposed a two-stage remedy model, i.e., a two-stage VAE model to address the above issues and enhance the original VAE so that any gloablly minimizing solution is uniquely matched to the ground-truth distribution. . Problem Definition: . The starting point is the desire to learn a probabilistic generative model of observable variables x∈Xx in mathcal Xx∈Xwhere X mathcal XX is a r-dimensional manifold embedded in Rd mathbb R ^dRd | Denote a ground-truth probability measure on X mathcal{X}X as μgt mu_{gt}μgt​ where ∫Xμgtdx=1 int_{ mathcal{X}} mu_{gt} d mathbf{x} = 1∫X​μgt​dx=1 | The canonical VAE attempts to approximate this ground-truth measure using parameterized density pθ(x)p_{ theta}( mathbf{x})pθ​(x) where pθ(x)=∫pθ(x∣z)p(z)dzp_{ theta}(x) = int p_{ theta}(x | z) p(z) dzpθ​(x)=∫pθ​(x∣z)p(z)dz, z∈Rκz in mathbb{R}^ kappaz∈Rκ with κ≈r kappa approx rκ≈r and p(z)=N(z∣0,I)p(z) = mathcal{N}(z | 0, mathbf{I})p(z)=N(z∣0,I) | . We will consider two situations where r&lt;dr &lt; dr&lt;d and r=dr = dr=d to illustrate the aforementioned non-uniqueness issues. . VAE Objective . In the vanilla VAE model, we normally write the objective function to be optimized as evidence lower bound (ELBO): Lθ,ϕ(x)=−Eqϕ(z∣x)[log⁡pθ(x,z)−log⁡qϕ(z∣x)]=KL(qϕ(z∣x)∣∣pθ(z))+Eqϕ(z∣x)[−log⁡pθ(x,z)] begin{align*} mathcal{L}_{ theta, phi}(x) &amp; = - mathbb{E}_{q_{ phi}(z|x)}[ log p_{ theta}(x, z) - log q_{ phi}(z|x)] &amp; = mathbb{KL}(q_{ phi}(z|x) || p_{ theta}(z)) + mathbb{E}_{q_{ phi}(z|x)}[- log p_{ theta}(x, z)] end{align*}Lθ,ϕ​(x)​=−Eqϕ​(z∣x)​[logpθ​(x,z)−logqϕ​(z∣x)]=KL(qϕ​(z∣x)∣∣pθ​(z))+Eqϕ​(z∣x)​[−logpθ​(x,z)]​ | In this case, based on the ground-truth probability measure μgt mu_{gt}μgt​, we can rewrite it into: Lθ,ϕ(x)=∫X{−log⁡pθ(x)+KL[qϕ(z∣x)∣∣pθ(z∣x)]}μgtdx≥∫X−log⁡pθ(x)μgtdxLθ,ϕ(x)=∫X{−Eqϕ(z∣x)[log⁡pθ(z∣x)]+KL[qϕ(z∣x)∣∣p(z)]}μgtdx begin{align*} mathcal{L}_{ theta, phi}(x) &amp; = int_{ mathcal{X}} {- log p_{ theta}(x) + mathbb{KL}[q_{ phi}(z|x) || p_{ theta}(z|x)] } mu_{gt} dx geq int_{ mathcal{X}} - log p_{ theta}(x) mu_{gt} dx mathcal{L}_{ theta, phi}(x) &amp; = int_{ mathcal{X}} {- mathbb{E}_{q_{ phi}(z|x)} [ log p_{ theta}(z|x)] + mathbb{KL}[q_{ phi}(z|x) || p(z)] } mu_{gt} dx end{align*}Lθ,ϕ​(x)Lθ,ϕ​(x)​=∫X​{−logpθ​(x)+KL[qϕ​(z∣x)∣∣pθ​(z∣x)]}μgt​dx≥∫X​−logpθ​(x)μgt​dx=∫X​{−Eqϕ​(z∣x)​[logpθ​(z∣x)]+KL[qϕ​(z∣x)∣∣p(z)]}μgt​dx​ | In principle, qϕ(z∣x)q_{ phi}(z|x)qϕ​(z∣x) and pθ(x∣z)p_{ theta}(x|z)pθ​(x∣z) can be arbitrary distributions. In the practical implementation, a commonly adopted distributional assumption is that both distribution are Gaussian, which was previously considered as a limitation of VAE. | . Diagnosing the Non-uniqueness . Ideas: Even with the stated Gaussian distributions, there exist parameters θ,ϕ theta, phiθ,ϕ that can simultaneously: . Globally optimize the VAE object | Recover the ground-truth probability measure in a certain sense | Definition 1: A κ kappaκ-simple VAE is defined as a VAE model with dim[z mathbf{z}z] = κ kappaκ latent dimensions, the Gaussian encoder qϕ(z∣X)=N(z∣μz,Σz)q_{ phi}(z|X) = mathcal{N}(z | mu_z, Sigma_z)qϕ​(z∣X)=N(z∣μz​,Σz​) and the Gaussian decoder pθ(x∣z)=N(x∣μx,Σx)p_{ theta}(x|z) = mathcal{N}(x | mu_x, Sigma_x)pθ​(x∣z)=N(x∣μx​,Σx​) With these definitions, we can now move to the discussion of κ kappaκ-simple VAE with κ≥r kappa geq rκ≥r can achieve the above optimality criteria from the simpler case where r=dr = dr=d followed by the extended scenario with r&lt;dr &lt; dr&lt;d. . When r=d . Assuming pgt(x)=μgt(dx)/dxp_{gt}(x) = mu_{gt}(dx) / dxpgt​(x)=μgt​(dx)/dx exists everywhere in Rd mathbb{R}^dRd, the minimal possible value of negative log-likelihood will necessarily occur if KL[qϕ(z∣x)∣∣pθ(z∣x)]=0&nbsp;and&nbsp;pθ(x)=pgt(x)&nbsp;almost&nbsp;everywhere mathbb{KL}[q_{ phi}(z|x) || p_{ theta}(z|x)] = 0 text{ and } p_{ theta}(x) = p_{gt}(x) text{ almost everywhere}KL[qϕ​(z∣x)∣∣pθ​(z∣x)]=0&nbsp;and&nbsp;pθ​(x)=pgt​(x)&nbsp;almost&nbsp;everywhere Naturally we will conclude that . Theorem 2: Suppose that r=dr=dr=d and there exists a density pgt(x)p_{gt}(x)pgt​(x) associated with the ground-truth measure μgt mu_{gt}μgt​ that is nonzero everywhere on Rd mathbb{R}^dRd. Then for any κ≥r kappa geq rκ≥r, there is a sequence of κ kappaκ-simple VAE model parameters {θt⋆,ϕt⋆} { theta_t^ star, phi_t^ star }{θt⋆​,ϕt⋆​} such that lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&nbsp;and&nbsp;lim⁡t→∞pθt⋆(x)=pgt(x)&nbsp;almost&nbsp;everywhere lim_{t to infty} mathbb{KL}[q_{ phi_t^ star}(z|x) || p_{ theta_t^ star}(z|x)] = 0 text{ and } lim_{t to infty} p_{ theta_t^ star}(x) = p_{gt}(x) text{ almost everywhere} t→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&nbsp;and&nbsp;t→∞lim​pθt⋆​​(x)=pgt​(x)&nbsp;almost&nbsp;everywhere The theorem implies that as long as latent dimension is sufficiently large (i.e., κ≥r kappa geq rκ≥r), the optimal ground-truth probability measure can be recovered, whether the encoder and decder has Gaussian assumptions or not, since the ground-truth probability measure being recovered almost everywhere is the necessary conditions for optimized objective value. . When r &lt; d . When both qϕ(z∣x)q_ phi(z|x)qϕ​(z∣x) and pθ(x∣z)p_{ theta}(x|z)pθ​(x∣z) are arbitrary/unconstrained, i.e., without Gaussian assumptions, then inf⁡ϕ,θL(θ,ϕ)=−∞ inf_{ phi, theta} mathcal{L}( theta, phi) = - inftyinfϕ,θ​L(θ,ϕ)=−∞ by forcing qϕ(z∣x)=pθ(z∣x)q_{ phi}(z|x) = p_{ theta}(z|x)qϕ​(z∣x)=pθ​(z∣x). | To show that this does not need to happen, define a manifold density p~gt(x) tilde p_{gt}(x)p~​gt​(x) as the probability density of μgt mu_{gt}μgt​ with respect to the volume measure of the manifold X mathcal{X}X. If d=rd = rd=r then this volume is the standard Lebesgue measure in Rd mathbb{R}^dRd and p~gt(x)=pgt(x) tilde p_{gt}(x) = p_{gt}(x)p~​gt​(x)=pgt​(x) since when r&lt;dr &lt; dr&lt;d, pgt(x)p_{gt}(x)pgt​(x) may not exist everywhere in the ambient space. | . Theorem 3: Assume r&lt;dr &lt; dr&lt;d and that there exists a manifold density p~gt(x) tilde p_{gt}(x)p~​gt​(x) associated with the ground-truth measure μgt mu_{gt}μgt​ that is nonzero everywhere on X mathcal{X}X. Then for any κ≥r kappa geq rκ≥r, there is a sequence of κ kappaκ-simple VAE model parameters {θt⋆,ϕt⋆} { theta_t^ star, phi_t^ star }{θt⋆​,ϕt⋆​} such that . lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&nbsp;and&nbsp;lim⁡t→∞∫X−log⁡pθt⋆(x)μgtdx=−∞ lim_{t to infty} mathbb{KL}[q_{ phi_t^ star}(z|x) || p_{ theta_t^ star}(z|x)] = 0 text{ and } lim_{t to infty} int_{ mathcal{X}} - log p_{ theta_t^ star}(x) mu_{gt} dx = - inftyt→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&nbsp;and&nbsp;t→∞lim​∫X​−logpθt⋆​​(x)μgt​dx=−∞ | lim⁡t→∞∫X∈Apθt⋆(x)dx=μgt(A∪X) lim_{t to infty} int_{ mathcal{X} in A} p_{ theta_t^ star} (x) dx = mu_{gt} (A cup mathcal{X})t→∞lim​∫X∈A​pθt⋆​​(x)dx=μgt​(A∪X) for all measurable sets A⊆RdA subseteq mathbb{R}^dA⊆Rd with μgt(∂A∪X)=0 mu_{gt}( partial A cup mathcal{X}) = 0μgt​(∂A∪X)=0 where ∂A partial A∂A is the boundary of AAA. | . Implications of this theorem: . From (1), the VAE Gaussian assumptions do not prevent minimization of L(θ,ϕ) mathcal{L}( theta, phi)L(θ,ϕ) from converging to minus infinity. | From (2), there exists solutions that assign a probability mass to most all measurable subsets of Rd mathbb{R}^dRd that is distinguishable from the ground-truth measure. | In r=dr = dr=d situation, the theorem necessitates that the ground-truth probability measure has been recovered almost everywhere. | In r&lt;dr &lt; dr&lt;d situation, we have not ruled out the possibility that a different set of parameters {θ,ϕ} { theta, phi }{θ,ϕ} can push the lost to −∞- infty−∞ and not achieve (2), i.e., the VAE can reach the lower bound of negative log-likelihood but fail to closely approximate μgt mu_{gt}μgt​. | . Optimal Solutions . The necessary conditions for VAE optimal value would be induced from the following theorems. . Theorem 4: Let {θγ⋆,ϕγ⋆} { theta^ star_ gamma, phi_ gamma^ star }{θγ⋆​,ϕγ⋆​} denote an optimal κ kappaκ-simple VAE solution (with κ≥r kappa geq rκ≥r) where the decoder variance γ gammaγ is fixed. Moreover, we assume that μgt mu_{gt}μgt​ is not a Gaussian distribution when d=rd = rd=r. Then for any γ&gt;0 gamma &gt; 0γ&gt;0, there exists a γ′&lt;γ gamma&#39; &lt; gammaγ′&lt;γ such that L(θγ′⋆,ϕγ′⋆)&lt;L(θγ⋆,ϕγ⋆) mathcal{L}( theta_{ gamma&#39;}^ star, phi_{ gamma&#39;}^ star) &lt; mathcal{L}( theta_{ gamma}^ star, phi_{ gamma}^ star)L(θγ′⋆​,ϕγ′⋆​)&lt;L(θγ⋆​,ϕγ⋆​) . The theorem implies that if γ gammaγ is not constrained, it must be that γ→0 gamma to 0γ→0 if we wish to minimize the VAE objective. While in existing practical VAE applications, it is standard to fix γ≈1 gamma approx 1γ≈1 with the standard Gaussian assumptions during training. . Theorem 5: Applying the same conditions and definitions in Theorem 4, then for all xxx drawn from μgt mu_{gt}μgt​, we also have that lim⁡γ→0fμx[fμz(x;ϕγ⋆)+fSz(x;θγ⋆)ϵ;ϕγ⋆]=lim⁡γ→0fμx[fμz(x;ϕγ⋆);θγ⋆]=x,∀ϵ∈Rκ lim_{ gamma to 0} f_{ mu_x} [f_{ mu_z}(x; phi_{ gamma}^ star) + f_{S_z}(x; theta{ gamma}^ star) epsilon; phi_ gamma^ star] = lim_{ gamma to 0} f_{ mu_x}[f_{ mu_z}(x; phi_ gamma^ star); theta_ gamma^ star] = x, forall epsilon in mathbb{R}^ kappaγ→0lim​fμx​​[fμz​​(x;ϕγ⋆​)+fSz​​(x;θγ⋆)ϵ;ϕγ⋆​]=γ→0lim​fμx​​[fμz​​(x;ϕγ⋆​);θγ⋆​]=x,∀ϵ∈Rκ . With this theorem, it indicates that any x∈X mathbf{x} in mathcal{X}x∈X will be perfectly reconstructed by the VAE model at globally optimal solutions. | Adding dimensions to latent dimension cannot improve the value of the VAE data term in meaningful way. In the training process, there are likely to be rrr eigenvalues of the decoder covariance converging to 0 and κ−r kappa - rκ−r converging to one. This demonstrats that VAE has the ability to detect the manifold dimension and select the proper number of latent dimensionsin practical environments. | If VAE model parameters have learned a near optimal mapping onto X mathcal{X}X using γ≈0 gamma approx 0γ≈0, then the VAE cost will scale as (d−r)log⁡γ(d - r) log gamma(d−r)logγ regardless of μgt mu_{gt}μgt​. | . Two-Stage VAE Model . The above analysis suggests the following two-stage remedy: . Given nnn observed samples {x(i)}i=1n {x^{(i)} }^n_{i=1}{x(i)}i=1n​, train a κ kappaκ-simple VAE, with κ≥r kappa geq rκ≥r, to estimate the unknown rrr-dimensional ground-truth manifold X mathcal{X}X embedded in Rd mathcal{R}^dRd using a minimal number of active latent dimensions. Generate latent samples {z(i)}i=1n {z^{(i)} }^n_{i=1}{z(i)}i=1n​ via z(i)∼qϕ(z∣x(i))z_{(i)} sim q_{ phi}(z|x^{(i)})z(i)​∼qϕ​(z∣x(i)). | Train a second κ kappaκ-simple VAE, with independent parameters {θ′,ϕ′} { theta&#39;, phi&#39; }{θ′,ϕ′} and latent representation uuu, to learn the unknown distribution qϕ(z)q_ phi(z)qϕ​(z) as a new ground-truth distribution and use samples {z(i)}i=1n {z^{(i)} }^n_{i=1}{z(i)}i=1n​ to learn it. | Samples approximating the original ground-truth μgt mu_{gt}μgt​ can then be formed via the extended ancestral process u∼N(u∣0,I),z∼pθ′(z∣u),x∼pθ(x∣z)u sim mathcal{N}(u | 0, mathbf{I}), z sim p_{ theta&#39;}(z | u), x sim p_{ theta}(x|z)u∼N(u∣0,I),z∼pθ′​(z∣u),x∼pθ​(x∣z) | The structure of the first-stage of the Two-Stage VAE Model . Analysis: . If the first stage was successful, then even though they will not generally resemble N(z∣0,I) mathcal{N}(z|0, mathbf{I})N(z∣0,I), samples from qϕ(z)q_ phi(z)qϕ​(z) will have nonzero measure across the full ambient space Rκ mathbb{R}^ kappaRκ. | If κ&gt;r kappa &gt; rκ&gt;r, then the extra latent dimensions will be naturally filled in via randomness. | Consequently, as long as we set κ≥r kappa geq rκ≥r, the operational regime of the second-stage VAE is effectively equivalent to the situation that the manifold dimension is equal to the ambient dimension, and reaching global optimum solutions would recover the ground-truth probability measure almost everywhere. | . Experiment Results . The following table indicates the performance evaluation results of the experiments conducted on four significantly different datasets: MNIST, Fash-ion MNIST, CIFAR-10 and CelebA. The evaluation metrics used Frchet Inception Distance (FID)3 Score: used to assess the quality of images created by a generative model, comparing the generated images with the distribution of real images. Note: The training of two stages need to be separate. Concatenating two stages and jointly training does not improve the performance. . Another set of experiments were conducted on the same datasets with different evaluation metrics. Kernel Inception Distance (KID)4 applies a polynomial-kernel Maximum Mean Discrepancy (MMD) measure to estimate the inception distance, as FID score is believed to exhibit bias in certain circumstances. . Analysis of the Results: . The second stage of Two-Stage VAE model can reduce the gap between q(z)q(z)q(z) and p(z)p(z)p(z), resulting in better manifold reconstruction. | γ gammaγ will converge to zero at any global minimum of the VAE objective, allowing for tighter image reconstructions with better manifold fit. | . . Contributions and Conclusions . This paper rigorously proved that VAE global optimum can in fact uniquely learn a mapping to the correct ground-truth manifold when r&lt;dr &lt; dr&lt;d, but not necessarily the correct probability measure within this manifold. | The proposed Two-Stage VAE model can resolve this issue and better recover the ground-truth manifold and reduce the gap between pθ(z∣x)p_ theta(z|x)pθ​(z∣x) and qϕ(z∣x)q_ phi(z|x)qϕ​(z∣x). And this is the first demonstration of a VAE pipeline that can produce stable FID scores that are comparable to at least some popular GAN models under neutral testing conditions. | The two-stage mechanism can improve the reconstruction of original distribution so that it has comparable performance with GAN models. This work narrows the gap between VAE and GAN models in terms of the realism of generated samples so that VAEs are worth considering in a broader range of applications. | No need Gaussian assumption in the canonical VAE model to achieve the optimal solutions. | References . . Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In 7th International Conferenceon Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. ↩︎ . | Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. ↩︎ . | Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and SeppHochreiter. GANs trained by a two time-scale update rule converge to a local Nashequilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637,2017. ↩︎ . | Miko laj Bi ́nkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. arXiv:1801.01401, 2018 ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoders/2021/09/21/VAE2_blog.html",
            "relUrl": "/variational%20autoencoders/2021/09/21/VAE2_blog.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Review on Importance Weighted Autoencoders",
            "content": "Importance Weighted Autoencoders: what makes a good ELBO in VAE? . Variational AutoEncoders (VAE) [1] is a powerful generative model which combines the variational inference and autoencoders together. It approximates the posterior distribution with a simple and tractable one, and optimize the lower bound of the true data distribution, which is called evidence lower bound (ELBO). Althoug optimizing ELBO is effective in practice, this estimation is actually biased, and it’s shown that this bias actually cannot be eliminated in vanilla VAE. Here we introduce a work that tries to minimize this bias called Importance Weighted Autoencoders (IWAE) [2], along with its variants which combines the objective in VAE and IWAE. . Introduction to VAE and ELBO . VAE consists of the encoder qϕq_{ phi}qϕ​ and the decoder pθp_{ theta}pθ​. It first encodes each sample xxx into a distribution of the latent variables qϕ(⋅∣x)q_{ phi}( cdot|x)qϕ​(⋅∣x). Then the latent variables are sampled from the distibution as z∼qϕ(z∣x)z sim q_{ phi}(z|x)z∼qϕ​(z∣x). The latent variables serve as the input to the decoder where the reconstructed output is x^∼pθ(x∣z) hat{x} sim p_{ theta}(x|z)x^∼pθ​(x∣z). The overview of VAE is shown in Fig 1. . Fig 1. Overview of VAE (source from [1]) The training objective of VAE is to maximize ELBO. There are multiple ways to derivate ELBO, and one way is through the Bayesian theory. log⁡pθ(x) log{p_{ theta}(x)}logpθ​(x) can be rewritten as . log⁡pθ(x)=Eqϕ(z∣x)[log⁡pθ(x)]=Eqϕ(z∣x)[log⁡pθ(x,z)pθ(z∣x)]=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)qϕ(z∣x)pθ(z∣x)]=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)]+Eqϕ(z∣x)[qϕ(z∣x)pθ(z∣x)]. begin{align} log{p_{ theta}(x)}&amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{p_{ theta}(x)}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{p_{ theta}(z|x)}}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}} frac{q_{ phi}(z|x)}{p_{ theta}(z|x)}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}}]+ mathbb{E}_{q_{ phi}(z|x)}[ frac{q_{ phi}(z|x)}{p_{ theta}(z|x)}]. end{align}logpθ​(x)​=Eqϕ​(z∣x)​[logpθ​(x)]=Eqϕ​(z∣x)​[logpθ​(z∣x)pθ​(x,z)​]=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​pθ​(z∣x)qϕ​(z∣x)​]=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​]+Eqϕ​(z∣x)​[pθ​(z∣x)qϕ​(z∣x)​].​​ . Here the second term in Equation (4) is actually the the KL divergence DKL(qϕ(z∣x)∥pθ(z∣x))D_{KL}(q_{ phi}(z|x) |p_{ theta}(z|x))DKL​(qϕ​(z∣x)∥pθ​(z∣x)) that is always non-negative. Therefore, the first term Lθ,ϕ(x)=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)] mathcal{L}_{ theta, phi}(x)= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}}]Lθ,ϕ​(x)=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​] actually serve as an lower-bound of log⁡pθ(x) log{p_{ theta}(x)}logpθ​(x), which is exactly the ELBO. Furthermore, ELBO can be written in the regularized reconstruction form as . Lθ,ϕ(x)=−DKL(qϕ(z∣x)∥pθ(z))+Eqϕ(z∣x)[−log⁡pθ(x∣z)], begin{align} mathcal{L}_{ theta, phi}(x)=-D_{KL}(q_{ phi}(z|x) |p_{ theta}(z)) + mathbb{E}_{q_{ phi}(z|x)}[- log{p_{ theta}(x|z)}], end{align}Lθ,ϕ​(x)=−DKL​(qϕ​(z∣x)∥pθ​(z))+Eqϕ​(z∣x)​[−logpθ​(x∣z)],​​ . where the first term regularizes the posterior distribution towards the prior which is usually set as a standard normal distribution, and the second term corresponds to the reconstruction. . Nonetheloss, the regularization term actually has a conflict with the second term in Equation (4). When the regularization term is perfectly optimized, qϕ(z∣x)q_{ phi}(z|x)qϕ​(z∣x) will stay close to the prior p(z)p(z)p(z), meanwhile, it makes hard for qϕ(z∣x)q_{ phi}(z|x)qϕ​(z∣x) to be close enough to the true posterior distribution pθ(z∣x)p_{ theta}(z|x)pθ​(z∣x). Therefore, the gap between ELBO and the true data distribution, namely DKL(qϕ(z∣x)∥pθ(z∣x))D_{KL}(q_{ phi}(z|x) |p_{ theta}(z|x))DKL​(qϕ​(z∣x)∥pθ​(z∣x)), will always exists, which prevents ELBO from being a tighter lower bound. . Fig 2. Example of a heavy penalization in VAE We can understand this in the other view. When a latent variable is sampled from the low-probability region of a latent distribution, it would inevitably lead to a bad reconstruction. . For the example in Fig 2, if we unfortunately sample a latent variable from the distribution of digit “5” (red) in the orange point, it turns out that this latent variable actually lies in the high probability region of the latent distribution generated by digit “3” (black) and it’s highly possible that we get a final reconstruction more similar to “3” rather than “5”. To make the posterior distribution close to the normal distribution, the regularizer will penalize this sample heavily by decreasing the variance, leading to a small spearout of the latent distribution. This drawback motivates the work of Importance Weight Autoencoders (IWAE) to introduce the importance weights into VAE, where a sampled latent variable which is far away from the mean will get assigned a lower weight during updates since it is known to give a bad reconstruction with high probability. . Importance Weighted Autoencoders . Another way to derivate ELBO is through the Jensen’s Inequality. Since log⁡(⋅) log{( cdot)}log(⋅) is a concave function, we have . log⁡pθ(x)=log⁡Eqϕ(z∣x)[pθ(x,z)qϕ(z∣x)]≥Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)]=Lθ,ϕ(x). begin{align} log{p_{ theta}(x)}&amp;= log{ mathbb{E}_{q_{ phi}(z|x)}[ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}]} &amp; geq mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}}] &amp;= mathcal{L}_{ theta, phi}(x). end{align}logpθ​(x)​=logEqϕ​(z∣x)​[qϕ​(z∣x)pθ​(x,z)​]≥Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​]=Lθ,ϕ​(x).​​ . A simple example is shown in Fig 3. Consider a random variable XXX taking value from {x1,x2} {x_1, x_2 }{x1​,x2​}, and we want to estimate log⁡E[X] log{ mathbb{E}[X]}logE[X]. If we use E[log⁡X] mathbb{E}[ log{X}]E[logX] to estimate it, then the estimation will converge at log⁡x1+log⁡x22 frac{ log{x_1}+ log{x_2}}{2}2logx1​+logx2​​, and the bias term cannot be eliminated by simply increasing the sampling times. . Fig 3. Bias in log expectation estimation If we instead use E[1k∑i=1klog⁡Xi] mathbb{E}[ frac{1}{k} sum_{i=1}^k{ log{X_i}}]E[k1​∑i=1k​logXi​] for estimation, when we gradually increase the sampling times kkk, the bias will become smaller. And when k→+∞k rightarrow+ inftyk→+∞, the term inside the expectation actually becomes a constant which is exactly log⁡E[X] log{ mathbb{E}[X]}logE[X], as shown in Fig 4. . Fig 4. Reducing the bias in the log expectation If we apply this property on ELBO estimation, let wi=pθ(x,zi)qϕ(zi∣x)w_i= frac{p_{ theta}(x,z_i)}{q_{ phi}(z_i|x)}wi​=qϕ​(zi​∣x)pθ​(x,zi​)​ and Lk=Eqϕ(z∣x)[log⁡1k∑i=1kwi] mathcal{L}_k= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{1}{k} sum_{i=1}^kw_i}]Lk​=Eqϕ​(z∣x)​[logk1​∑i=1k​wi​], we actually have the theorem . log⁡pθ(x)≥Lk+1≥Lk. begin{align} log{p_{ theta}(x)} geq mathcal{L}_{k+1} geq mathcal{L}_k. end{align}logpθ​(x)≥Lk+1​≥Lk​.​​ . And Lk mathcal{L}_kLk​ will converge to log⁡pθ(x) log{p_{ theta}(x)}logpθ​(x) when pθ(x,z)qϕ(z∣x) frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}qϕ​(z∣x)pθ​(x,z)​ is bounded. . Equipped with this theorem, IWAE simply replace sthe objective in VAE with Lk mathcal{L}_kLk​, where k&gt;1k&gt;1k&gt;1. This gives a tighter lower bound compared with ELBO. And when k=1k=1k=1, IWAE is reduced to VAE. . In the backward pass, the gradient of Lk mathcal{L}_kLk​ can be written as . ∇θ,ϕLk=Eqϕ(z∣x)[∇θ,ϕlog⁡1k∑i=1kwi]=Eqϕ(z∣x)[∑i=1kwi~log⁡∇θ,ϕwi], begin{align} nabla_{ theta, phi} mathcal{L}_k&amp;= mathbb{E}_{q_{ phi}(z|x)}[ nabla_{ theta, phi} log{ frac{1}{k} sum_{i=1}^kw_i}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ sum_{i=1}^k tilde{w_i} log{ nabla_{ theta, phi}w_i}], end{align}∇θ,ϕ​Lk​​=Eqϕ​(z∣x)​[∇θ,ϕ​logk1​i=1∑k​wi​]=Eqϕ​(z∣x)​[i=1∑k​wi​~​log∇θ,ϕ​wi​],​​ . where wj~=wj∑i=1kwi tilde{w_j}= frac{w_j}{ sum_{i=1}^kw_i}wj​~​=∑i=1k​wi​wj​​ is the normalized importance weights, which makes the model name “Importance Weighted” Autoencoders. In VAE, wj~ tilde{w_j}wj​~​ takes the value 111. . The meaning of the importance weights could be interpreted as this: if a latent sample itself has low probability in the latent distribution, then it should get assigned a lower weight in the gradient update since it’s known to cause a bad reconstruction with high probability. Introducing importance weights can effectively lower the risk shown in Fig 2. . Variants of IWAE . To make a straightforward comparison with ELBO in VAE, we fix the sampling times for both VAE and IWAE as kkk. The ELBO for IWAE and VAE become . ELBOIWAE=log⁡1k∑i=1kwiELBOVAE=1k∑i=1klog⁡wi. begin{align} text{ELBO}_{ text{IWAE}}&amp;= log{ frac{1}{k} sum_{i=1}^kw_i} text{ELBO}_{ text{VAE}}&amp;= frac{1}{k} sum_{i=1}^k log{w_i}. end{align} ELBOIWAE​ELBOVAE​​=logk1​i=1∑k​wi​=k1​i=1∑k​logwi​.​​ . The main difference here is the position of the average operation, either insider or outside log⁡(⋅) log{( cdot)}log(⋅). IWAE regards the sampling outside log⁡(⋅) log{( cdot)}log(⋅) as the variance reduction, and it’s shown that IWAE actually doesn’t suffer from the large variance, so IWAE puts all sampling inside log⁡(⋅) log{( cdot)}log(⋅) to reduce the bias as much as possible. . However, a follow-up work [3] of IWAE theoretically proves that the sampling outside log⁡(⋅) log{( cdot)}log(⋅) is crucial to the training of the encoder. A tighter bound used by IWAE helps the generative network (decoder) but hurts the inference network (encoder). . Based on this discovery, three new models combining ELBOIWAE text{ELBO}_{ text{IWAE}}ELBOIWAE​ and ELBOVAE text{ELBO}_{ text{VAE}}ELBOVAE​ are proposed. For the following text, we fix the total sampling times to be MKMKMK, where MMM is the sampling times outside log⁡(⋅) log{( cdot)}log(⋅) and KKK is the sampling times inside log⁡(⋅) log{( cdot)}log(⋅). . MIWAE. MIWAE simply uses an ELBO objective with both M&gt;1M&gt;1M&gt;1 and K&gt;1K&gt;1K&gt;1, i.e., ELBOMIWAE=1M∑m=1Mlog⁡1K∑k=1Kwm,k. begin{align} text{ELBO}_{ text{MIWAE}}= frac{1}{M} sum_{m=1}^M log{ frac{1}{K} sum_{k=1}^Kw_{m,k}}. end{align} ELBOMIWAE​=M1​m=1∑M​logK1​k=1∑K​wm,k​.​​ | CIWAE. CIWAE uses a convex combination of two ELBOs, i.e., ELBOCIWAE=βELBOVAE+(1−β)ELBOIWAE. begin{align} text{ELBO}_{ text{CIWAE}}= beta text{ELBO}_{ text{VAE}}+(1- beta) text{ELBO}_{ text{IWAE}}. end{align} ELBOCIWAE​=βELBOVAE​+(1−β)ELBOIWAE​.​​ | PIWAE. PIWAE uses different objectives for the inference network and generative network. For the generative network, it keeps the objective of IWAE, ELBOIWAE text{ELBO}_{ text{IWAE}}ELBOIWAE​. While for the inference network, it switches to the objective ELBOMIWAE text{ELBO}_{ text{MIWAE}}ELBOMIWAE​. | . Experimental Results . The experimental results demonstrate the advantage of IWAE against VAE as Table 1 shows. IWAE achieves lower negative log-likelihood (NLL) and more active units (active units captures data infomation) on all datasets and model architectures. And as kkk increases, the performance is better since the lower bound is tighter. . Table 1. IWAE results For the qualitative analysis in Fig 5, it’s worth noting that for IWAE, it has a larger spredout of the latent distribution and sometimes different output digits, e.g., “6” for “0”. This demonstrates the relaxation of the heavy panelization on the outliers, contrary to the example in Fig 2. . Fig 5. Ouput samples from VAE and IWAE In a grid search of different combinations of (M,K)(M,K)(M,K) with MKMKMK fixed as 646464, we can see neither M=1M=1M=1 or K=1K=1K=1 makes the optimal solution in Fig 6. In other words, the ELBO objective should consider both the sampling inside and outside log⁡(⋅) log{( cdot)}log(⋅), which are beneficial to the generative network and inference network respectively. . Fig 6. Ouput samples from VAE and IWAE Conclusion . IWAE uses a simple technique, moving the average operation inside log⁡(⋅) log{( cdot)}log(⋅), to achieve a tighter lower bound. The importance weights relaxes the heavy penalization on the posterior samples which fail to explain the observation. Although IWAE effectively reduces the bias, it’s shown that the sampling inside log⁡(⋅) log{( cdot)}log(⋅) is only beneficial to the generative network, but hurts the inference network. Therefore, to combine the ELBO in VAE and IWAE, the IWAE variants, MIWAE, CIWAE and PIWAE are proposed. The final results demonstrate that the optimal objective needs the sampling inside and outside log⁡(⋅) log{( cdot)}log(⋅) to be both greater than one. These works takes a deep look into the ELBO objective in VAE and reveal its role in the learning process. . References . [1] 2014 (ICLR): D. Kingma, M. Welling, Auto-Encoding Variational Bayes, ICLR, 2014. | [2] 2016 (ICLR): Y. Burda, R. Grosse, R. Salakhutdinov. Importance Weighted Autoencoders. ICLR, 2016. | [3] 2018 (ICML): T. Rainforth, A. Kosiorek, T. Le, C. Maddison, M. Igl, F. Wood, Y. Teh, Tighter Variational Bounds are Not Necessarily Better. ICML, 2018. | .",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/16/VAE1_blog.html",
            "relUrl": "/variational%20autoencoder/2021/09/16/VAE1_blog.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Review on Attention Is All You Need",
            "content": "Attention Is All You Need . Introduction . In the last three years, the Transformer architecture has become an influential paradigm within deep learning. It has been applied prolifically within natural language processing (NLP), is beginning to see promising applications in computer vision (CV), and is also used within many other modalities and fields of deep learning. The paper which introduced the Transformer is “Attention is All You Need” [1] by Vaswani et al. Attention is All You Need (from here, AAYN) uses the Transformer architecture to perform machine translation. . Historically, the work in AAYN was done when recurrent neural networks (RNN) were the dominant force in NLP. Common modifications of these included the Long short-term memory (LSTM) [5] and gated recurrent unit (GRU) [6]. However, these models have a big problem—they compute along the length of a sequence, so they cannot be parallelized easily. Additionally, RNNs struggle to learn long-term dependencies. In order to rectify this issue, the authors propose the key idea (and title): attention is all you need. Although there had been previous work on using attention, most of those papers combined it with RNNs, so it still had the drawbacks from that method. . Task – Machine Translation . In AAYN, the primary goal of the model is translation, making this a sequence-to-sequence generation problem. In particular, they focus on English to German and English to French tasks from WMT2014. Machine translation is trained on bitext – data where each sample consists of the same sentence in the source and the target language. Then, the model is evaluated on a test set where it has to translate sentences. The results are compared to several human reference translations which are used to compute the BLEU score. It is defined as follows [2,7] . . Definition of BLEU from [2] Here, the key things to note are the brevity and n-gram overlap. Note that if the model outputs something very short, then it has a high probability of completely overlapping with n-grams in a reference translation. To penalize this, the brevity penalty is added, so when the output translation is shorter than the reference translation, then the exponent will be to the power of a negative number making a smaller brevity term. If the reverse is true then the brevity will be 1 due to the minimum and be ignored. The other important term in BLEU is the n-gram overlap. This essentially measures how well an output matches the references. The different lengths of n-grams measure different things; unigrams measure adequacy and the longer n-grams measure fluency. Note that this definition allows the candidate output to combine parts from different reference translations and have a good score. . . BLEU score interpretation from [2] Preliminaries . The Transformer model uses an encoder-decoder sequence-to-sequence architecture. It can be described as mathematically as follows: . Input: Length nnn sequence of symbolic representations: (x1,...,xn)(x_1,...,x_n)(x1​,...,xn​) | Encoder: produces latent representations: z=(z1,...,zn)z=(z_1,...,z_n)z=(z1​,...,zn​) | Decoder: uses zzz to produce length mmm output sequence: (y1,...,ym)(y_1,...,y_m)(y1​,...,ym​) | . Model . The Transformer is the following model: . . In the model, the encoder is on the left, and it feeds into the decoder on the right. In AAYN, the encoder and decoder are each stacked six times. We will examine the construction of both the encoder and decoder, so first let’s look at how the pieces of each layer are built. . Scaled Dot-Product Attention . Attention is at the key of the Transformer architecture. The intuition behind this approach is that it allows the model to decide what other symbols in the sequence are most important to look at for solving whatever problem. In AAYN, the attention mechanism is implemented using multiplicative attention (dot product). In addition, the major modification from the paper is to scale the dot products—this is done by dividing by the square root of the number of dimensions. This is due to the author’s observation that the the dot product grows too large in magnitude for a high number of dimensions, which would limit the model’s efficacy. . . . Dot-product attention works by learning a query, key, and value projection from some input. The query and key values are used to compute the attention—how much weight the model gives each token in a sequence. The multiplication between QQQ and KTK^TKT produces a sequence length by sequence length array of logits. Then Softmax is applied, which essentially turns the logits into probability distributions (one for each symbol in the sequence). These probabilities are used to compute a weighted average of the values VVV. VVV contains a representation for each symbol in the sequence, so the attention distribution decides how much one symbol iii should pay to any other symbol jjj. A visual of this will be shown later in the results section. . Multi-Head Attention . The authors notice that the weighted average in attention prevents the model from looking at different representation subspaces—it can’t consider multiple different parts of the sequence without averaging them. To fix this, the authors propose multi-head attention. . . Multi-head attention essentially allows the model to look at multiple things at the same time. Each attention head can learn to look for different things, such as connecting adjectives to nouns or connecting verbs and objects. Naively using multi-head attention, however, would increase the computational costs of the model. . . To address this issue, the authors decrease the representation dimension of each head by hhh, where hhh is the number of heads. This results in the same total number of parameters in the attention mechanism. The following dimensions are used for each head: . . The output representation from each head is concatenated together to create a vector of the original length, dmodeld_{model}dmodel​. This is projected again. . Positional Embeddings . One key issue of only using attention is that, according to attention, one symbol in a sequence is the same distance away from any other symbol. To allow the model to determine distance, the authors introduce positional embeddings. . . Visualization of sinusoidal positional embeddings from [3]. Each column is a positional embedding. . Equations for the sinusoidal position embeddings The authors selected a sinusoidal embedding function because the offset between embeddings can be represented as a linear function. However, many positional embeddings are possible and the authors also experiment with learned positional embeddings (achieving similar results). The sinusoidal embeddings are used in the paper because the authors hypothesize that they will allow the model to extrapolate to sequence lengths longer than those the model was trained on. Note that later work shows that position is not necessarily as important as intuition suggests [8]. . Types of Attention . In AAYN, three types of attention are used: . Encoder-decoder attention: Queries QQQ come from the last layer of the decoder, keys KKK and values VVV come from encoder. This allows the decoder to look at the input source language in order to translate it. . | Encoder self-attention layer: Each position can attend to every other position in the previous layer of the encoder. . | Decoder self-attention layer: Same as encoder self-attention but also mask out all connections in the Softmax that cannot have been seen. . This maintains the autoregressive property of the model by preventing the model from looking at words it hasn’t seen yet. | . | . Why Self-Attention? . Self-attention allows the model to learn dependencies between different symbols in the sequence. This is shown in the following table: . . Note that self-attention achieves the best complexity in terms of both maximum path length and sequential operations (which indicates parallelizability). Additionally, the complexity per layer is low if nnn is significantly smaller than ddd, which often occurs in practice (although many researchers are working on models where this assumption no longer holds). . Putting It All Together . Alright, we’ve looked at all the pieces. Now, let’s put everything together! . The Encoder . . The encoder combines all the parts we talked about. Then, it is stacked NNN times (6 in the base Transformer model). A couple notable things that we didn’t mention yet: . The model uses either byte-pair or word-piece tokenization to create token sequences from raw character strings. . | The model uses residual connections by adding an earlier representation. It follows this up with layer normalization. . | The model uses a simple feed-forward network with two layers after attention. . | . The Decoder . . The decoder is mostly the same as the encoder. However, it uses masking to ignore symbols that the model shouldn’t have seen yet from the input. For example, if I say “The cat sat” then the model would need to generate the next word. This is called autoregression. If this model generates “on”, then “The cat sat on” would be fed in the model to generate the next word (maybe “the”). When we’re training the model, we have the full sentence that the model is learning to generate, so we can’t let it know what’s coming. . In addition, the decoder also has an attention mechanism that looks at what it’s trying to translate. This allows it to see what it should be doing, and the decoder can use attention to connect the source input to its translated output. For example, “gato”, or cat in Spanish, might be connected to “cat” in the example above using attention. . Training . The training details for the model are as follows: . Sentences are encoded (convert a string of characters to a sequence of symbols): . English-German uses BytePair encoding for 37,000 tokens on 4.5M sentence pairs. | English-French uses WordPiece encoding for 32,000 tokens on 36M sentence pairs. | . | Batch size is determined in order to have 25,000 source and target tokens (symbols). . | 8 NVIDIA T100 GPUs are used to train the model. . Base models trained for 12 hours, big models for 3.5 days. | . | Adam optimizer is used with a special learning rate: . Linear warmup followed by inverse square root decay. | . | Regularization: Dropout of 0.1 applied to residual connections and sum of positional encoding and embeddings. Label smoothing is performed. . | The last 5 checkpoints are averaged (for the base model). Beam search is used to select the best translation. . | . The most interesting details are that the batch size is dynamic so that the source and target tokens number approximately 25,000. The summation of the last checkpoints and using a learning rate decay are also interesting. . Results . The details are finally out of the way! Let’s look at some pretty pictures and tables. . . As can be seen in the above figure, Transformer is able to set record BLEU scores in much less computation. Additionally, the “big” variant can even beat some expensive ensemble models! We can see the effect of different hyperparameters (such as those used in base versus big) on the performance in the following table: . . Alright, now time for some visualizations from the paper: . . . . . As shown in the visualizations, the attention heads learn different, meaningful tasks, such as anaphora resolution or connecting determiners and their objects. In fact, the authors show that Transformer can be used directly for this type of task—English constituency parsing (extracting the syntactic structure of a sentence in the form of a tree). . . Results are competitive to previous methods, even without task-specific fine-tuning. . Final Takeaway . Transformers precipitated a major change in the landscape of natural language processing and even other fields like computer vision. They lead to even more powerful general language models, such as BERT [4]. The paper can be summarized by the following points. . Motivation: RNNs are not easily parallelizable and don’t learn long dependencies well. . | Models that only use attention are more effective and train faster. . | Transformer can generalize to other tasks. . | Multi-head attention helps address some of the problems of traditional attention. It allows multiple different attention tasks to be learned. . | Transformers have a constant dependency path from one position to any other position. . | . References . [1] Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017. . [2] BLEU score definition: https://cloud.google.com/translate/automl/docs/evaluate . [3] https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ . [4] Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. . [5] Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780. . [6] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. . [7] Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W. J. (2002, July). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (pp. 311-318). . [8] Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., &amp; Kiela, D. (2021). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. arXiv preprint arXiv:2104.06644. .",
            "url": "https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog3.html",
            "relUrl": "/transformers/2021/09/14/AR3_blog3.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Review on Generating Long Sequences with Sparse Transformers",
            "content": "Generating Long Sequences with Sparse Transformers . Transformers and attention-based methods have skyrocketed in popularity in recent years. These models excel at modelling long-term dependencies and are highly parallelizable, overcoming the shortcomings of prior LSTM based models. However, vanilla transformers1 scale poorly with increasing sequence length; since the attention is done globally between all inputs, the computation grows quadratically with input length. In this post, I will go over the Sparse Transformer2 model which reduces the computation to O(nn)O(n sqrt n)O(nn . ​) where nnn is the sequence length. Additionally, unlike prior works that propose model for specific generation tasks, the sparse transformer model can be used to generate text, images, and audio! . Background . The sparse transformer is an autoregressive model. It models the joint probability distribution as a product of conditional probability distributions. The iiith output depends all on previous inputs x1,...,xi−1x_1, ..., x_{i-1}x1​,...,xi−1​. This autoregressive property is embedded into the attention operation, which cannot use future values to generate an output. Image taken from 3 . . . Factorized Self-Attention Intuition . To understand the motivation behind the sparse transformer model, we take a look at the learned attention patterns for a 128-layer dense transformer network on the CIFAR-10 dataset. The authors observed that the attention pattern of the early layers resembled convolution operations. For layers 19-20, the attention pattern is arranged in discrete rows and columns. In other layers, the attention pattern is extremely complex, global, and data dependent. Finally, in the the layers 64-128, the attention pattern is extremely sparse. Visualization taken from 2 . . Looking at these attention patterns, we observe that most attention patterns are sparse. The authors reasoned that to model high-dimensional data, dense global attention is not required. Instead, sparser attention operations can capture most of the information needed to model the underlying distribution. . Sparse Transformer Model . Factorized Self-Attention . The factorized self-attention operation forms the backbone of the sparse transformer model. The authors break down the dense self-attention operation with several sparse attention operations. In particular, an attention operation can be written as (equations taken from 2): . where SiS_iSi​ denotes the set of indices of input vectors which the iiith output vector attends. For dense self-attention, Si={j:j≤i}S_i= {j:j le i }Si​={j:j≤i} which allows every element to attend to all previous positions and its own position. This pattern can be visualized in the image given below. Visualization of attention taken from 2 . . The bottom image is the connectivity matrix where the i=ji=ji=j index represents the output and the other indices in the same row represent the input that the output attends to. . Instead, factorized self attention uses ppp separate attention heads each defining a subset of indices Ai(m)⊂{j:j≤i}A_i^{(m)} subset {j:j le i }Ai(m)​⊂{j:j≤i}. We want choice of AAA such that ∣Ai(m)∣∝ap|A_i^{(m)}| propto sqrt[p] a∣Ai(m)​∣∝pa . ​ so that our computation scales the way we want. This paper considers choices with two heads p=2p=2p=2. Additionally, we add the constraint that there is path from each input connection to all future output positions across ppp steps of attention (this will become more clear in the representations for the factorized attention patterns) so that all input signals are being propagated to output positions in a constant number of steps. . Strided Attention Pattern . In this factorized attention pattern, one head attends to lll previous locations and the other head attends to every lllth previous location. lll is the stride parameter and is chosen to be close to n sqrt nn . ​. More formally, the index sets are defined as Ai(1)={t:t,t+1,...,i}A_i^{(1)} = {t:t,t+1,...,i }Ai(1)​={t:t,t+1,...,i} for t=max(0,i−l)t=max(0,i-l)t=max(0,i−l) and Ai(2)={j:(i−j) mod l=0}A_i^{(2)} = {j:(i-j) , mod , l = 0 }Ai(2)​={j:(i−j)modl=0}. This strided attention pattern is visualized below. Visualization of attention taken from 2 . . This pattern works well when the data naturally has a structure that aligns with the stide. For example, images and audio have periodic structure that can be modeled effectively using strided attention patterns. However, for data without this natural structure such as text, the strided pattern does not perform well. . Fixed Attention Pattern . In the fixed attention pattern, we model the AAA matrices as follows: Ai(1)={j:(⌊j/l⌋=⌊i/l⌋)}A_i^{(1)} = {j: ( lfloor j/l rfloor = lfloor i/l rfloor) }Ai(1)​={j:(⌊j/l⌋=⌊i/l⌋)} and Ai(2)={j:j mod l∈{t:t,t+1,...,l}}A_i^{(2)} = {j:j , mod , l in {t:t,t+1,...,l } }Ai(2)​={j:jmodl∈{t:t,t+1,...,l}}. This pattern is easier to visualize than understanding the math. Visualization of attention taken from 2 . . This attention pattern works better for data without perdiodic structure like text. . Note: For both strided and fixed attention pattern, notice how every input signal is propagated to arbitrary output after 2 steps of attention, satisfying the constraints. . Incorporating Factorized Self-Attention . The authors proposed several ideas on how to incorporate these factorized attention models in the transformer network. These methods can be summarized in the image below. . . Gradient Recomputation during Backward Pass . During the gradient backpropagation step while training, the results from the forward pass are stored in memory and used for computation. However, for sparse attention, the memory usage to store the result is far greater than the computational cost of the forward pass. Hence, we don’t save all the forward pass results in memory but recompute the forward pass during training the compute the gradients. This reduces the memory usage of the model and enables networks with hundreds of layers and sequences of up to 16384 in length. . New residual block architecture . Transformers are notoriously difficult to scale to many layers. The authors of this paper experiment with using a different kind of residual connection which enables the sparse transformer model to scale to hundred of layers. The NNN layer transformer network is defined as follows. . . This residual architecture with the gradient recomputation is visualized in the image below. Visualization of model architecture taken from 2 . . . Results . Image generation examples taken from 2 . . . Sparse transformer NLL metrics on common datasets taken from paper 2 . . . . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017; http://arxiv.org/abs/1706.03762 arXiv:1706.03762. ↩︎ . | Child, Rewon, et al. “Generating Long Sequences with Sparse Transformers.” ArXiv:1904.10509 [Cs, Stat], Apr. 2019. arXiv.org, http://arxiv.org/abs/1904.10509 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Oord, Aaron van den, et al. “Pixel Recurrent Neural Networks.” ArXiv:1601.06759 [Cs], Aug. 2016. arXiv.org, http://arxiv.org/abs/1601.06759 ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog2.html",
            "relUrl": "/transformers/2021/09/14/AR3_blog2.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Review on An image is 16 x 16 words",
            "content": "An image is 16 x 16 words . Transformers and attention-based methods have skyrocketed in popularity in recent years. These models are the current state-of-the-art in natural language processing applications (BERT, GPT). However, in computer vision, convolutional patterns still remain dominant. Applying transformers directly to image pixels is not practical because the self-attention operation scales quadratically. Many recent works experiment with hybrid convolutional and attention based methods. Other works that replace convolutions for attention all together, like the Sparse Transformer, use specialized attention patterns that are difficult to scale on hardware accelerators. This paper demonstrates that the vanilla transformer 1, with minimal modifications, can achieve state-of-the-art performance in image classification when trained on large datasets. . Vision Transformer Architecture . Input format . The input image x∈RH×W×Cx in R^{H times W times C }x∈RH×W×C is reshaped into a sequence of flattened patches xp=RN×(P2C)x_p = R^{N times (P^2 C )}xp​=RN×(P2C) where PPP is that patch size. Since the transformer uses constant latent vector size D through all of its layers, the xpx_pxp​ flattened patches are linearly projected into DDD dimensions using a trainable linear layer. This forms the patch embeddings. . Positional Embeddings . Similar to 1, positional embeddings are added to the patch embeddings to convey positional information to the model. The authors use learnable 1D embeddings and found that 2D embeddings don’t improve performance. The 1D embeddings use the index of the patch row-by-row top-to-bottom. . Class Token . Similar to BERT 2, the authors prepend a learnable class token z00z_0^0z00​ embedding along with its learnable positional encoding to the input. The state of this token at the output zL0z_L^0zL0​ is the input of a classification head MLP which outputs the class probabilities for image classification. . Transformer Encoder . The inputs defined above are fed directly into the transformer encoder from1. The transformer encoder consists of alternating layers of multihead self-attention and MLP blocks. The image taken from paper 2 below summarizes the transformer encoder model: . . Hybrid Model . Instead of the image, the input patches can be formed from the output feature maps of a CNN. In this scheme, the linear projection is applied to the patches from the CNN to form the patch emdeddings. The other parts of the architecture remain the same. . Model Visualization . The image below shows the entire model for the vision transformer. Notice how the model is very similar to the transformer encoder form 1. . . Results . The authors evaluate 3 variations of their Vision Transformer (ViT) on image classification datasets. The image below summarizes the 3 ViT models. Additionally, the authors experiment with different patch sizes, reporting the results for 16×1616 times 1616×16 patches and 14×1414 times 1414×14 patches. . As seen from the image taken from 2 below, the Vision Transformer outperforms CNN based approaches on across multiple datasets. The image taken from 2 below shows how the ViT takes significantly less compute to pre-train than its CNN counterparts. . . Intuition . If you’ve read this paper so far, you must certainly be confused about the results of the ViT. How does the vanilla vision transformer working on image patches learn to solve computer vision task better than the state-of-the-art CNN models? The authors seems to think its because of the inherent inductive bias in CNNs. The convolution operation exploits locality and two-dimensional spatial structure of images. The idea of looking at neighboring pixels to extract meaningful representation of image data is what made CNNs rise in popularity many years ago. However, because CNNs are so highly specialized, they are not as good as transformers at learning features that do not depend on nearby pixels. Because of the global attention layers and minimal image-specific inductive bias, the vision transformer is able to learn features that the CNN model misses out because of its specialized convolution operation. The image below is a comparison of the linear embedding in the ViT to convolutional layers in CNN. Visualizations taken from paper 2. . As you can see, the learned linear layer closely resembles convolutional filters learned by CNNs. But the transformer model can also learn much more than that because it is not limited by convolutional operations. . It is to be noted that when training on mid-sized datasets, the CNN based model still outperforms ViT because the specialized convolutional operations quickly learn representations that frequently occur in images. However, when training with very large datasets, the transformer is able to learn features that the convolution misses out on because of observing enough samples. . . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017; http://arxiv.org/abs/1706.03762 arXiv:1706.03762. ↩︎ ↩︎ ↩︎ ↩︎ . | Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, 2018; http://arxiv.org/abs/1810.04805 arXiv:1810.04805. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog.html",
            "relUrl": "/transformers/2021/09/14/AR3_blog.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Review on Pixel recurrent neural networks",
            "content": "Pixel Recurrent Neural Networks . Modeling the distribution of high-dimensional data is a central problem in unsupervised machine learning. Since images are high-dimensional and highly structured, estimating their underlying distribution is notoriously challenging. With the recent advances in deep learning, there has been significant progress in developing expressive, scalable, and tractable methods to tackle generative modeling problems. In this blog, we are going to explore the PixelRNN 1 and GatedPixelRNN 2 models for generating images. . Related Work . Perhaps the most popular technique for generative modeling in recent years has been the Generative Adversarial Network. These models generate rich and sharp images. However, GANs are notoriously hard to train because of instability due to the adversarial nature of training.3 . On the other hand, stochastic latent variable models such as the Variational Auto-Encoder produce blurry samples due to the nature of its reconstruction loss. Additionally, the VAE exproximates a lower bound (ELBO) to the desired probability distribution. 4 . Previous methods that model the distribution as a product of conditionals such as NADE/MADE are limited because they lack sophisticated recurrent units like LSTM cells. 5 6 By using sophisticated auto-regressive modeling techniques, the PixelRNN is able to achieve state-of-the-art performance on image generation benchmarks. . Background . PixelRNN Model . In PixelRNN, each pixel is conditionally dependent on previous pixels from top to bottom and left to right. We model the joint probablity distribution of the image as a product of the conditional probabilities. Image taken from paper 1 . . In the following image, the pixel xix_ixi​ depends on all pixels [x1,x2,...,xi−1][x_1, x_2, ..., x_{i-1}][x1​,x2​,...,xi−1​] from top to bottom and left to right. Visualization taken from 7 . . Additionally, each channel RGB is conditionally dependent on previous channels. For example, the green channel is conditionally dependent on the red channel of the same pixel. Image taken from paper 1 . . As we will see later in this blog, ensuring this auto-regressive property holds requires clever masking of inputs in the network. . Discrete Softmax Output . The output layer of the network has a n×n×3×256n times n times 3 times 256n×n×3×256 shape. This can be interpreted as each channel for each pixel having a 256 channel output. This 256 channel output is normalized via softmax and represents the discrete multinomial probability distribution for channel values. The following is a visualization of the softmax output for one channel for one pixel. Image taken from paper 1 . . . PixelRNN Generation . Images are generated sequentially pixel-by-pixel and channel-by-channel from top-to-bottom and left-to-right. This makes the generation process extremely slow which is a big weakness of this model. However, training the PixelRNN model can be done in parallel since all the conditional inputs are present. The inputs just need to be masked to preserve the autoregressive property. Visualization taken from 7 . . . PixelRNN Network Architecture . The model always start with a 7×77 times 77×7 masked convolution. This is then followed by several residual blocks which can either be convolutional, RowLSTM, or DiagonalBiLSTM. Finally, there are two 1×11 times 11×1 convolutional layers to generate the final output. Image taken from paper 1 . . . Input Masking . There are two types of masks in the PixelRNN network. The first type (Mask A) exists to maintain the autoregressive property for the first 7×77 times 77×7 convolutional layer. In this mask, the output of the layer depends on all information from previous pixels and only information from previous channels of the same pixel. The second variant (Mask B) is applied to subsequent layers. This variant also allows the output of the layer to depend on the information from the same channel of the same pixel. This is because the channel values for subsequent layers only depends on inputs from previous channel and can be used without violating the autoregressive property. Below is a visualization of these two masking schemes. Image taken from paper 1 . . Below is a visualization of Mask A for the red, green, and blue channels respectively. We are masking the inputs to generate the center pixel for every channel. . Below is a visualization of Mask B for the red, green, and blue channels respectively. Note how the pixel of the same channel can be used this time. . . PixelCNN . In the PixelCNN each residual block is masked 3×33 times 33×3 convolution. PixelCNN is heavily parallelizable due to its convolutional layers. However, as we are only looking at a 3×33 times 33×3 neighborhood for the convolution, we are not capturing information from all previous pixels. While the receptive field of the convolution grows linearly with the depth of the network, in one particular layer, the masked 3×33 times 33×3 has a small receptive field. The image below is a visualization of the receptive field of the masked 3×33 times 33×3 convolution. Image taken from paper 1 . . . RowLSTM . RowLSTM generates its output row-by-row from top-to-bottom and left-to-right. To model the output, RowLSTM modifies the traditional LSTM cell to compute all hidden outputs via convolutions. RowLSTM uses 3×13 times 13×1 convolutions for the he state-to-state kernel KssK^{ss}Kss and input-to-state kernel KisK^{is}Kis. Note that the input-to-state component depends on the input and must be appropriately masked to ensure the autoregressive property. Additionally, since the input-to-state component depends only on the input, it can be computed for the entire n×nn times nn×n input in parallel. However, the state-to-state component of the RowLSTM convolution must be computed sequentially using previous hidden states. Below is the mathematical notation for the convolutional LSTM cell in RowLSTM. Image taken from paper 1 . . The image below is a visualization of the convolutions in the RowLSTM. The 3×13 times 13×1 convolution slides left-to-right row-by-row. Visualization taken from 7 . . Because of the sequential nature of the computation, RowLSTM is more computational intensive than convolutional layers. However, the hidden state for RowLSTM encapsulates a much larger context than convolutional layers. Specifically, the RowLSTM captures the entire triangular context above the output pixel. The image below is a visualization of the receptive field of the RowLSTM. Image taken from paper 1 . . . DiagonalBiLSTM . While the RowLSTM is an improvement on the convolutional layers in terms of receptive field, there is still room for improvement. This is where the DiagonalBiLSTM comes in. The goal of the DiagonalBiLSTM is to capture all the available context. In order to accomplish this, the DiagonalBiLSTM scans the diagonals of the image from two directions; top-left to bottom-right and top-right to bottom-left. The outputs from these two scans are added together for the final output. Similar to RowLSTM, the DiagonalBiLSTM uses a convolutional LSTM framework with a 1×11 times 11×1 convolution for the input-to-state kernel KisK^{is}Kis and a 2×12 times 12×1 convolution for the state-to-state kernel. Additionally, the KisK^{is}Kis convolution must be masked to preserve the autoregressive property and can be precomuted for the entire output. Visualization taken from 7 . . The above image shows how the DiagonalBiLSTM generates its output from the top-left to bottom-right diagonal. Implementing this diagonal 2×12 times 12×1 is tricky. In order to simplify the compute, the image is skewed to rearrange the convolution as shown in the image below. Visualization taken from 7 . . Note that the the DiagonalBiLSTM computes this operation for both diagonals. As a result, it is able to capture all the available context to generate outputs and has a complete receptive field. However, the DiagonalBiLSTM has even more computational overhead because of computing two outputs. Image taken from paper 1 . . The image above shows how the receptive field for the DiagonalBiLSTM is able to capture the entire available context to generate its output. . Residual Connections . As mentioned earlier, each of the blocks (convolutional, RowLSTM, DiagonalBiLSTM) are residual. Residual connections enable training deeper PixelRNN networks. These residual connections increase both convergence speed by propagating signals more directly through the network. The image below is a visualization on how residual connection are setup in the convolutional and LSTM cells. Image taken from paper 1 . . . PixelRNN Model Summary . This visualization summarizes the model architecture of the PixelCNN, RowLSTM, and Diagonal BiLSTM variants of the model. Image taken from paper 1 . . . Preliminary Results . As shown in the image below, PixelRNN variants achieve state-of-the-art performance in common datasets (MNIST, CIFAR-10, and ImageNet). The best variant model is the DiagonalBiLSTM, which is expected since it has the largest receptive field. Image taken from paper 1 . . . Gated PixelCNN Motivation . The authors of the PixelRNN paper released another paper shortly after the first one that improved upon the design on the PixelCNN. The authors reasoned that PixelRNN variants (RowLSTM and DiagonalBiLSTM) are outperforming PixelCNN for two main reasons: . The element-wise multiplicative units are able to model more complex interactions. The absence of multiplicative operations in PixelCNN is limiting its performance. | PixelRNN’s capture much larger receptive fields. While the receptive fields of the PixelCNN grows linearly with the number of layers, there is a blind spot that forms in the receptive field of masked CNNs (more information below). | The authors proposed modification to the PixelCNN architecture to fix its shortcomings. First, they added Gated Activation Units that contained multiplicative operations to add more sophistication to the model. Second, they fixed the receptive field blind-spot problem by splitting up the convolution into an unmasked vertical stack and a masked horizontal stack that takes the output of the vertical stack as input. . Horizontal and Vertical Stack . As mentioned above, the receptive field of the PixelCNN, while increasing linearly with depth, contains a growing blind-spot. Pixels in this blind-spot are never used as context regardless of how many layers are stacked. The blind-spot problem is caused because of the masking in the convolutions to maintain the autoregressive property. The image below is a visualization of the blind-spot problem. Image taken from paper 2 . . . To fix the receptive field blind-spot, the single masked convolution is replaced with a horizontal and vertical stack. The vertical stack is an unmasked operation that captures the entire receptive field in the rows above the output pixel. The horizontal stack is a masked operation that captures the context to the left of the output pixels and uses the vertical stack output as input. Also, the authors add an additional residual connection in the horizontal stack convolution. Splitting up the convolution in this way fixes the receptive field blind-spot as shown in the figure below. Image taken from paper 2 . . . Gated Activation Units . Additionally, the authors replace the ReLU activation between convolutional blocks with a more sophisticated gated activation unit. This new activation computes two different convolutions with half the feature maps. The output of the two convolutions are subjected to two different non-linear activation functions (tanh and sigmoid) and multiplied together element-wise for the final output. Image taken from paper 2 . . The image above shows the mathematical definition of the gated activation unit. The final convolutional block is shown in the image below. The green n×nn times nn×n convolution is the vertical stack and the green n×1n times 1n×1 convolution is the horizontal stack. The output of the vertical stack is fed into the horizontal stack as mentioned above. Image taken from paper 2 . . . Conditional PixelCNN . In my opinion, the most interesting part of the paper is adding the ability to conditionally generate images. You can condition the output probability distribution of the images on a high-dimensional latent vector hhh that behaves as the image description. For example, the hhh vector could be a one-hot encoded vector of class labels in the ImageNet dataset. The PixelCNN network would then learn to conditionally generate specific classes of ImageNet data. So passing in the latent vector hhh corresponding to the class “Dog” would generate images of dogs! The equation given below shows how the output probability is now conditionally dependent on hhh. Image taken from paper 2 . . . However, the latent vector hhh does not contain any spatial information about the object. So in the above example, while images of dogs would be generated, the dog could appear anywhere in the image. Fortunately, the authors of the paper had a solution to this problem. The latent vector hhh can be passed through a deconvolution network to produce output s=m(h)s = m(h)s=m(h) such that sss has the same spatial dimensions as the image but arbitrary channels. sss contains spatial information about the generated object. Now you can control where in the image the dog is generated! . The equations below show how the gated activation unit in the network is modified to accommodate conditional generation. For the hhh, the Vk,fV_{k,f}Vk,f​ is a linear layer and for sss the Vk,fV_{k,f}Vk,f​ is a 1×11 times 11×1 convolution. Image taken from paper 2 . . . Final Results . The image below shows the performance of the GatedPixelCNN on CIFAR-10 (left) and ImageNet (right) from paper 2 . . . Conditional Generation Examples . Here are some examples from the paper for condtional image generation from the ImageNet dataset from paper 2 . . . . Oord, Aaron van den, et al. “Pixel Recurrent Neural Networks.” ArXiv:1601.06759 [Cs], Aug. 2016. arXiv.org, http://arxiv.org/abs/1601.06759 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Oord, Aaron van den, et al. “Conditional Image Generation with PixelCNN Decoders.” ArXiv:1606.05328 [Cs], June 2016. arXiv.org, http://arxiv.org/abs/1606.05328 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Recent research has made progress in demystifying the problems in training GANs. However, when the initial PixelRNN paper was published in 2016, training GANs for generative modeling was still a daunting task). ↩︎ . | Advances in VAE have made it possible to generative sharp high-dimensional data by using hierarchical techniques and modifying the ELBO loss for better reconstructions. ↩︎ . | Uria, Benigno, et al. “Neural Autoregressive Distribution Estimation.” ArXiv:1605.02226 [Cs], May 2016. arXiv.org, http://arxiv.org/abs/1605.02226. ↩︎ . | Germain, Mathieu, et al. “MADE: Masked Autoencoder for Distribution Estimation.” ArXiv:1502.03509 [Cs, Stat], June 2015. arXiv.org, http://arxiv.org/abs/1502.03509. ↩︎ . | Slides from UCF PixelRNN presentation by Logan Lebanoff 2/22/17. https://www.crcv.ucf.edu/wp-content/uploads/2019/03/CAP6412_Spring2018_Pixel-Recurrent-Neural-Networks.pdf ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/pixelrnn/quantiles/2021/09/09/AR2_blog.html",
            "relUrl": "/pixelrnn/quantiles/2021/09/09/AR2_blog.html",
            "date": " • Sep 9, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cs598ban.github.io/Fall2021/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}