{
  
    
        "post0": {
            "title": "DS1 Physics Informed Neural Networks",
            "content": "Introduction . This blog covers Physics Informed Neural Networks, which are basically neural networks that are constrained to obey physical constraints. The blog broadly discusses two directions: . Data-driven solutions of PDEs | Data-driven discovery of PDEs | . In particular, it deals with two distinct types of algorithms, namely . A new family of data-efficient spatio-temporal function approximators | Arbitrary accurate RK time steppers with potentially unlimited number of stages | . We know that deep neural networks are powerful function approximators. They can approximate a nonlinear map from a few – potentially very high dimensional – input and output pairs of data. However, the approximation does not take into account any underlying physical constraints imposed by fundamental principles (such as conservation laws) which are often given in terms of PDEs. PINNs precisely solve this problem by regularizing the loss functions with these constraints. The resulting approximation enjoys both the power of deep neural nets as function approximators and obeying the underlying physical conservation laws. . . PDEs . Let us consider a parametric nonlinear PDE . ut+N[u;λ]=0,x∈Ω⊂RD, t∈[0,T];(⋅)t:=∂(⋅)∂t color{BrickRed} u_{t}+ mathcal{N}[u ; lambda]=0, x in Omega subset mathbb{R}^D, , t in[0, T]; quad ({ cdot})_t := frac{ partial ({ cdot})}{ partial t} ut​+N[u;λ]=0,x∈Ω⊂RD,t∈[0,T];(⋅)t​:=∂t∂(⋅)​ where u(t,x)u(t, x)u(t,x) denotes the latent (hidden) solution and N[⋅;λ] mathcal{N}[ cdot ; lambda]N[⋅;λ] is a nonlinear operator parametrized by λ lambdaλ. This encapsulation covers a wide range of PDEs in math, physics, atmospheric sciences, biology, finance, including conservation laws, diffusion, reac-diff-advec. PDEs, kinetics etc. The burger’s equation in 222d is an apt starting step to investigate if indeed PINNs can efficiently solve PDEs, and therefore the authors begin by applying PINNs to it. . N[u;λ]=λ1uux−λ2uxx&nbsp;and&nbsp;λ=(λ1,λ2);(⋅)x:=∂(⋅)∂x(⋅)xx:=∂2(⋅)∂x2 mathcal{N}[u ; lambda]= lambda_{1} u u_{x}- lambda_{2} u_{x x} text { and } lambda= left( lambda_{1}, lambda_{2} right); quad ({ cdot})_x := frac{ partial ({ cdot})}{ partial x} quad ({ cdot})_{xx} := frac{ partial^2 ({ cdot})}{ partial x^2} N[u;λ]=λ1​uux​−λ2​uxx​&nbsp;and&nbsp;λ=(λ1​,λ2​);(⋅)x​:=∂x∂(⋅)​(⋅)xx​:=∂x2∂2(⋅)​ . The two directions highlighted above can now be restated in context of Burger’s equation as follows: . Given λ lambdaλ what is u(t,x)u(t, x)u(t,x) (data-driven solutions of PDEs) | Find λ lambdaλ that best describes observations u(ti,xj)u({t_i, x_j})u(ti​,xj​) (data-driven discovery of PDEs) | . . PDEs + PINNs . First, let us rewrite the PDE as f(u;t,x)=0f(u; t,x) = 0f(u;t,x)=0 f(u;t,x)≐ut+N[u],along&nbsp;withu=uθ(t,x) f(u; t,x) doteq u_{t}+ mathcal{N}[u], quad text{along with} quad u = u_ theta ({t, x}) f(u;t,x)≐ut​+N[u],along&nbsp;withu=uθ​(t,x) . The next step consists of appending the loss function used for the training process (finding the parameters θ thetaθ) as follows: . L=Lu+Lf mathcal{L}= mathcal{L}_{u}+ mathcal{L}_{f} L=Lu​+Lf​ . where . Lu=1Nu∑i=1Nu∣u(tui,xui)−ui∣2 ;Lf=1Nf∑i=1Nf∣f(tfi,xfi)∣2. mathcal{L}_{u}= frac{1}{N_{u}} sum_{i=1}^{N_{u}} left|u left(t_{u}^{i}, x_{u}^{i} right)-u^{i} right|^{2} , ; quad mathcal{L}_{f}= frac{1}{N_{f}} sum_{i=1}^{N_{f}} left|f left(t_{f}^{i}, x_{f}^{i} right) right|^{2}. Lu​=Nu​1​i=1∑Nu​​∣ . ∣​u(tui​,xui​)−ui∣ . ∣​2;Lf​=Nf​1​i=1∑Nf​​∣ . ∣​f(tfi​,xfi​)∣ . ∣​2. . Here Lu mathcal{L}_{u}Lu​ consists of the approximation error at the boundary of the domain (initial and boundary conditions), and Lf mathcal{L}_{f}Lf​ denotes the error incurred inside the domain. Also, {tfi,xfi}i=1Nf left {t_{f}^{i}, x_{f}^{i} right }_{i=1}^{N_{f}}{tfi​,xfi​}i=1Nf​​ specify the collocation points — the discrete points at which the physical constraints are imposed through a regularized (penalty) formulation. Another way to look at this is that Lu mathcal{L}_uLu​ helps to enforce initial and boundary data accurately, while Lf mathcal{L}_fLf​ imposes the structure of the PDE into the training process. An important thing to consider is the relative weighting of the two losses, namely, L=λ Lu+(2−λ) Lf mathcal{L}= lambda , mathcal{L}_{u}+(2- lambda) , mathcal{L}_{f} L=λLu​+(2−λ)Lf​ where λ=1 lambda = 1λ=1 corresponds to the original loss above. Increasing λ lambdaλ towards 222 leads to a an approximation that is accurate inside the domain but performs poorly on the boundaries. Similarly, λ→0+ lambda rightarrow 0^+λ→0+ leads to an approximation that is accurate on the boundary but poor inside the domain. . . Implementation details and Code . Since most of the examples considered here involve a small number of training data points, the authors choose to exploit quasi-second order optimization methods such as L-BFGS. They do not consider mini-batching for the same reason and consider the full batch on the update. There are no theoretical guarantees on the existence of the minimizer but the authors observe empirically that as long as the PDE is well-posed the optimization algorithm converges to the correct solution. . The original implementation can be accessed at https://github.com/maziarraissi/PINNs which builds on top of Tensorflow. Corresponding PyTorch and Julia (Flux) implementations can be accessed at https://github.com/idrl-lab/idrlnet and https://neuralpde.sciml.ai/dev/ . . Examples: Schrodinger Equation . As a first example, consider a complex-valued differential equation (h(t,x)=u(t,x)+i v(t,x)h(t, x) = u(t, x) + mathrm{i} , v(t,x)h(t,x)=u(t,x)+iv(t,x)), namely the Schrodinger equation, . f≐ iht+0.5hxx+∣h∣2h=0,x∈[−5,5],t∈[0,π/2]h(0,x)=2sech⁡(x)h(t,−5)=h(t,5)hx(t,−5)=hx(t,5) begin{aligned} f doteq , &amp;i h_{t}+0.5 h_{x x}+|h|^{2} h=0, quad x in[-5,5], quad t in[0, pi / 2] &amp;h(0, x)=2 operatorname{sech}(x) &amp;h(t,-5)=h(t, 5) &amp;h_{x}(t,-5)=h_{x}(t, 5) end{aligned} f≐​iht​+0.5hxx​+∣h∣2h=0,x∈[−5,5],t∈[0,π/2]h(0,x)=2sech(x)h(t,−5)=h(t,5)hx​(t,−5)=hx​(t,5)​ . In order to generate the training data, the authors employ a classical pseudo-spectral solver in space (xxx) coupled with a high-order RK time stepper. The results are shown in the figure below (Fig. 1 in the paper). PINN provides an accurate solution to Schrodinger equation and can handle periodic boundary conditions, nonlinearities and complex valued nature of the PDE efficiently. . . Although PINN provides a good approximation to the solution above, training the network requires a large number of data points. This is where the adaptive time-stepping using RK methods comes into the picture. The authors propose an adaptive time-stepping with a neural net at each time-step. This significantly improves the approximation quality, and allows one to take much larger time-steps compared to traditional solvers. . . Examples: Allen-Cahn Equation . In order to test the adaptive time-stepping scheme, the authors next take a look at the Allen-Cahn equation. . ut−0.0001uxx+5u3−5u=0,x∈[−1,1],t∈[0,1],u(0,x)=x2cos⁡(πx),u(t,−1)=u(t,1),ux(t,−1)=ux(t,1). begin{aligned} &amp;u_{t}-0.0001 u_{x x}+5 u^{3}-5 u=0, quad x in[-1,1], quad t in[0,1], &amp;u(0, x)=x^{2} cos ( pi x), &amp;u(t,-1)=u(t, 1), &amp;u_{x}(t,-1)=u_{x}(t, 1) . end{aligned} ​ut​−0.0001uxx​+5u3−5u=0,x∈[−1,1],t∈[0,1],u(0,x)=x2cos(πx),u(t,−1)=u(t,1),ux​(t,−1)=ux​(t,1).​ . The results show excellent agreement between the predicted and exact (numerical) solutions. The only difference here is that the authors consider the sum-of-squared errors as the loss function for training, instead of the MSE used before, i.e. . SSEn=∑j=1q+1∑i=1Nn∣ujn(xn,i)−un,i∣2 S S E_{n}= sum_{j=1}^{q+1} sum_{i=1}^{N_{n}} left|u_{j}^{n} left(x^{n, i} right)-u^{n, i} right|^{2} SSEn​=j=1∑q+1​i=1∑Nn​​∣ . ∣​ujn​(xn,i)−un,i∣ . ∣​2 SSEb=∑i=1q∣un+ci(−1)−un+ci(1)∣2+∣un+1(−1)−un+1(1)∣2+∑i=1q∣uxn+ci(−1)−uxn+ci(1)∣2+∣uxn+1(−1)−uxn+1(1)∣2 begin{aligned} S S E_{b} &amp;= sum_{i=1}^{q} left|u^{n+c_{i}}(-1)-u^{n+c_{i}}(1) right|^{2}+ left|u^{n+1}(-1)-u^{n+1}(1) right|^{2} &amp;+ sum_{i=1}^{q} left|u_{x}^{n+c_{i}}(-1)-u_{x}^{n+c_{i}}(1) right|^{2}+ left|u_{x}^{n+1}(-1)-u_{x}^{n+1}(1) right|^{2} end{aligned} SSEb​​=i=1∑q​∣ . ∣​un+ci​(−1)−un+ci​(1)∣ . ∣​2+∣ . ∣​un+1(−1)−un+1(1)∣ . ∣​2+i=1∑q​∣ . ∣​uxn+ci​​(−1)−uxn+ci​​(1)∣ . ∣​2+∣ . ∣​uxn+1​(−1)−uxn+1​(1)∣ . ∣​2​ . . . Examples: Navier-Stokes Equation . In this section, we take a look at how data-driven discovery of PDEs can be carried out using PINNs. Consider the NS equations in 2-dimensions, . ut+λ1(uux+vuy)=−px+λ2(uxx+uyy)vt+λ1(uvx+vvy)=−py+λ2(vxx+vyy);where(⋅)x=∂(⋅)∂x. begin{aligned} &amp;u_{t}+ lambda_{1} left(u u_{x}+v u_{y} right)=-p_{x}+ lambda_{2} left(u_{x x}+u_{y y} right) &amp;v_{t}+ lambda_{1} left(u v_{x}+v v_{y} right)=-p_{y}+ lambda_{2} left(v_{x x}+v_{y y} right) end{aligned}; quad text{where} quad ({ cdot})_x = frac{ partial({ cdot})}{ partial x}. ​ut​+λ1​(uux​+vuy​)=−px​+λ2​(uxx​+uyy​)vt​+λ1​(uvx​+vvy​)=−py​+λ2​(vxx​+vyy​)​;where(⋅)x​=∂x∂(⋅)​. . Here u(t,x,y)u(t, x, y)u(t,x,y) denotes the xxx-component of the velocity, v(t,x,y)v(t, x, y)v(t,x,y) denotes the yyy component and p(t,x,y)p(t, x, y)p(t,x,y) the pressure field. | Conservation of mass requires ux+vy=0  ⟹  u_{x}+v_{y}=0 impliesux​+vy​=0⟹ u=ψy,v=−ψxu= psi_{y}, quad v=- psi_{x}u=ψy​,v=−ψx​ | Given a set of observations: {ti,xi,yi,ui,vi}i=1N left {t^{i}, x^{i}, y^{i}, u^{i}, v^{i} right }_{i=1}^{N}{ti,xi,yi,ui,vi}i=1N​ f≐ut+λ1(uux+vuy)+px−λ2(uxx+uyy)g≐vt+λ1(uvx+vvy)+py−λ2(vxx+vyy) begin{aligned} &amp;f doteq u_{t}+ lambda_{1} left(u u_{x}+v u_{y} right)+p_{x}- lambda_{2} left(u_{x x}+u_{y y} right) &amp;g doteq v_{t}+ lambda_{1} left(u v_{x}+v v_{y} right)+p_{y}- lambda_{2} left(v_{x x}+v_{y y} right) end{aligned} ​f≐ut​+λ1​(uux​+vuy​)+px​−λ2​(uxx​+uyy​)g≐vt​+λ1​(uvx​+vvy​)+py​−λ2​(vxx​+vyy​)​ | The goal then is to learn λ={λ1,λ2} lambda = { lambda_1, lambda_2 }λ={λ1​,λ2​}, and pressure field p(t,x,y)p(t, x, y)p(t,x,y) by jointly approximating [ψ(t,x,y)p(t,x,y)][ psi(t, x, y) quad p(t, x, y)][ψ(t,x,y)p(t,x,y)] with a single NN with two outputs. The total loss function is given by | . L≐1N∑i=1N(∣u(ti,xi,yi)−ui∣2+∣v(ti,xi,yi)−vi∣2)+1N∑i=1N(∣f(ti,xi,yi)∣2+∣g(ti,xi,yi)∣2) begin{aligned} mathcal{L} &amp; doteq frac{1}{N} sum_{i=1}^{N} left( left|u left(t^{i}, x^{i}, y^{i} right)-u^{i} right|^{2}+ left|v left(t^{i}, x^{i}, y^{i} right)-v^{i} right|^{2} right) &amp;+ frac{1}{N} sum_{i=1}^{N} left( left|f left(t^{i}, x^{i}, y^{i} right) right|^{2}+ left|g left(t^{i}, x^{i}, y^{i} right) right|^{2} right) end{aligned} L​≐N1​i=1∑N​(∣ . ∣​u(ti,xi,yi)−ui∣ . ∣​2+∣ . ∣​v(ti,xi,yi)−vi∣ . ∣​2)+N1​i=1∑N​(∣ . ∣​f(ti,xi,yi)∣ . ∣​2+∣ . ∣​g(ti,xi,yi)∣ . ∣​2)​ . The training is carried out using a spectral solver NekTar and then randomly sampling points out of the grid for collocation. The figure on the bottom shows the locations of the training data-points. . . PINN is able to successfully predict the pressure field with just 1%1 %1% of the available data as collocation points as shown below (and also able to learn the parameters λj lambda_jλj​ in the process) . . . Examples: KDv Equation . As a final example illustrating data-driven discovery of PDEs, the authors choose an equation with higher order derivatives, namely the Korteweg-de Vries equation, that is encountered in the modeling of shallow water waves, . ut+λ1uux+λ2uxxx=0 u_{t}+ lambda_{1} u u_{x}+ lambda_{2} u_{x x x}=0 ut​+λ1​uux​+λ2​uxxx​=0 . The problem is to learn the parameters in a similar fashion as done for Navier-Stokes Equation . N[un+cj]=λ1un+cjuxn+cj−λ2uxxxn+cj mathcal{N} left[u^{n+c_{j}} right]= lambda_{1} u^{n+c_{j}} u_{x}^{n+c_{j}}- lambda_{2} u_{x x x}^{n+c_{j}} N[un+cj​]=λ1​un+cj​uxn+cj​​−λ2​uxxxn+cj​​ . PINN again is able to learn the parameters to the desired accuracy and provide an accurate resolution of the dynamics of the system. . . . Conclusion . The major take-away of the paper can be summarized in the following concluding remarks. . The authors introduced PINNs, a new class of universal function approximators that are capable of encoding any underlying physical laws that govern a given data-set (described by PDEs) | They also prove a design for data-driven algorithms that can be used for inferring solutions to general nonlinear PDEs, and constructing computationally efficient physics-informed surrogate models. | . They also rightly point to some lingering questions that still remain unanswered in the original paper: . How deep/wide should the neural network be ? How much data is really needed ? | Why does the algorithm converge to unique values for the parameters of the differential operators, i.e., why is the algorithm not suffering from local optima for the parameters of the differential operator? | Does the network suffer from vanishing gradients for deeper architectures and higher order differential operators? Could this be mitigated by using different activation functions? | Can we improve on initializing the network weights or normalizing the data? How about the choices for the loss function choices (MSE, SSE)? What about the robustness of these networks especially when applied to solve chaotic PDEs/ODEs? | . . References . Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M.Raissi, P.Perdikaris, G.E.Karniadakis . | Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations M.Raissi, P.Perdikaris, G.E.Karniadakis . | Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations M.Raissi, P.Perdikaris, G.E.Karniadakis . | .",
            "url": "https://cs598ban.github.io/Fall2021/ds/physics+ml/2021/11/18/DS1_blog2.html",
            "relUrl": "/ds/physics+ml/2021/11/18/DS1_blog2.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "DS1 Discovering Equations",
            "content": ". Abstract . Today we’ll talk about on how we can use compressive sensive and optimization to learn the dynamics of an underlying physical system. This blog is based on the paper from Prof. Kutz’s lab - Discovering governing equations from data by sparse identification of nonlinear dynamical systems. The paper develops a dictionary learning framework with overrepresented dictionary which enables us for interpretable non-linearity. We’ll discuss about it in the details below. . . History : Modeling Dynamics from Data . Science started from astronomy, which started by observing the movements of stars, planets and heavenly bodies. Copernicus presented heliocentric theory which proposed sun as the center of the universe instead of earth. Later, Kepler gathered huge amount of planetary motion data. This enabled him to come up with three Kepler’s laws, . . Finally, Newton developed a unified theory of motion across the universe. While Kepler’s contribution was primarily in generating detailed data, Newton’s contribution was in developing a unified theory which explains data. . Background: Symbolic Regression and Compressive Sensing . Symbolic regression which is a generalization of regression, aims to learn the underlying interaction in data. It searches over the space of all possible mathematical formulas which best predict the output variable, starting from a set of base functions like addition, trigonometric functions, and exponentials. However, it is computationally expensive, does not clearly scale well to large-scale dynamical systems of interest, and may be prone to over-fitting. . . This paper utilizes ideas from sparse regression which has been used to find solutions for underdetermined linear systems. Sparsity helps us break the Nyquist–Shannon sampling theorem. In sparse regression, signal xxx is K sparse, x=Ψsx = Psi sx=Ψs l1l1l1 norm is often imposed as regularization for sparsity. Extensive research in this area has been done often referred as Compressive Sensing. Terrence Tao, who is a professor at UCLA actively works on this. Their methods bypasees our need to perform a combinatorially intractable bruteforce search for sparse solution. It is found with high probability using convex methods that scale to large problems. . Problem Set up: Dynamical Systems . ddtx(t)=f(x(t)) frac{d}{dt} mathbf{x}(t) = f( mathbf{x}(t))dtd​x(t)=f(x(t)) Let’s assume x(t) mathbf{x}(t)x(t) denotes state at time ttt, fff denotes with dynamics f( mathbf{x}(t). In most physical systems, only a few nonlinear terms in the dynamics exist. Therefore it’s sparse in a high-dimensional nonlinear function space. So, if we can perform non-linear transformation, we can cast problem of identification of dynamical system as sparse regresssion. . Dynamical Systems as Sparse Regression . Let’s denote X mathbf{X}X and X˙ dot{ mathbf{X}}X˙ as state and dynamic matrix. kkk -th row of X mathbf{X}X contains observation vector at time kkk. Now, let’s do many transformation of X mathbf{X}X , such as linear, polynomial, cosine and exponentials. This should be guided by the applications where the possible non-linearities is known. Finally we append these matrices to contain a big matrix Θ ThetaΘ. Then we can solve the following problem, X˙=ΘW dot{ mathbf{X}} = Theta WX˙=ΘW where, weight WWW has sparse coefficients. It can be seen that each row has separate optimization. Therefore, complexity increaseslinearly with the number of time instants and not on the number of non-linear transformations. Basis functions are really important here. So we should test many different function bases and use the sparsity and accuracy of the resulting model as a diagnostic tool to determine the correct basis to represent the dynamics. . Approximating Derivatives . In reality we only observe discrete values of X mathbf{X}X and X˙ dot{ mathbf{X}}X˙ is not known. Therefore, we need to approximate it. Total variation regularization is used to denoise derivative. To counter the noise due to approximation, we should rather solve the following problem, X˙=ΘW+σZ dot{ mathbf{X}} = Theta W + sigma ZX˙=ΘW+σZ where, ZZZ is iid and normally distributed. Traditionally, people use Lasso for sparse regression. However, it is computationally expensive. The paper uses Sequential thresholded least-squares algorithm to find it. Depending on the noise, it may be necessary to filter X mathbf{X}X and X˙ dot{ mathbf{X}}X˙. . Dynamical Systems with PDE . Most of the physical systems are PDE and not ODE. This method poses a serious problem since numerical discretization on a spatial grid is exponentially large. For example, in fluid Dynamics, a simple 2D and 3D flows may require tens of thousands up to billions of variables to represent the discretized system. Therefore, current formulation is ill suited, since each row has separate optimization. . However, the good news that many high-dimensional systems of interest evolve on a low dimensional manifold or attractor that is well-approximated using a low-rank basis. Therefore, we can first use SVD to decompose X mathbf{X}X. This has attracted decades of research for fluid dymaics. Proper orthogonal decomposition is a state of the art method for decomposing the flow governed by the Navier stokes formulation. . Results: Lorenz System . . The paper shows that proposed method can identify lorenz attractor. It can capture rich and chaotic dynamics that evolve on an attractor. Only a few terms in the right-hand side of the equation are required . Results: Fluid Dynamics . . The paper simulates fluid flow data, Data are collected for the fluid flow past a cylinder using direct numerical simulations of the 2D Navier–Stokes equations. It then transforms to a lower dimension and perform sparse regression. We can see that it is able to identify dynamics. . Extensions . There can be a scenario where dynamics is also dependent on other parameters. In these scenario, we can solve, x˙=f(x,μ) dot{x} = f(x, mu)x˙=f(x,μ) . References . Brunton, S. L., Proctor, J. L., &amp; Kutz, J. N. (2016). Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15), 3932-3937. .",
            "url": "https://cs598ban.github.io/Fall2021/ds/physics+ml/2021/11/18/DS1_blog.html",
            "relUrl": "/ds/physics+ml/2021/11/18/DS1_blog.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "NODE2 Augmented Neural ODE",
            "content": "Introduction . This blog covers Augmented Neural ODEs, an improved and more expressive version of the celebrated Neural ODEs paper. Let’s start by revisiting the Neural ODEs idea, and even before that let us revisit the ResNet update, which is given by the relation . ht+1=ht+ft(ht)ht∈Rdandft:Rd→Rd mathbf{h}_{t+1}= mathbf{h}_{t} + mathbf{f}_{t} left( mathbf{h}_{t} right) quad mathbf{h}_{t} in mathbb{R}^{d} quad text{and} quad mathbf{f}_{t}: mathbb{R}^{d} rightarrow mathbb{R}^{d} ht+1​=ht​+ft​(ht​)ht​∈Rdandft​:Rd→Rd . where ht mathbf{h}_tht​ corresponds to the hidden state vector at the ttt-th layer, and ft mathbf{f}_tft​ corresponds to the residual mapping. This looks surprisingly similar to a forward euler discretization of an ODE . ht+1=ht+Δtft(ht)ht∈Rdandft:Rd→Rd mathbf{h}_{t+1}= mathbf{h}_{t} + { color{red}{ Delta} t} mathbf{f}_{t} left( mathbf{h}_{t} right) quad mathbf{h}_{t} in mathbb{R}^{d} quad text{and} quad mathbf{f}_{t}: mathbb{R}^{d} rightarrow mathbb{R}^{d} ht+1​=ht​+Δtft​(ht​)ht​∈Rdandft​:Rd→Rd . It is plain that with Δt=1 color{red} Delta t = 1Δt=1, we recover the ResNet update step. Now, if you instead consider ttt as a time-like variable, then I can take hth_tht​ on the LHS and take the limit of the step size going to zero, i.e. . lim⁡Δt→0+ht+Δt−htΔt=dh(t)dt=f(h(t),t) lim _{ Delta t rightarrow 0^+} frac{ mathbf{h}_{t+ Delta t}- mathbf{h}_{t}}{ Delta t}= frac{ mathrm{d} mathbf{h}(t)}{ mathrm{d} t}= mathbf{f}( mathbf{h}(t), t) Δt→0+lim​Δtht+Δt​−ht​​=dtdh(t)​=f(h(t),t) . We now have the hidden state parameterized by an ODE, x↦ϕ(x){ bf x} mapsto phi left( mathbf{x} right)x↦ϕ(x), . dh(t)dt=f(h(t),t),h(0)=xt∈(0,T] frac{ mathrm{d} mathbf{h}(t)}{ mathrm{d} t}= mathbf{f}( mathbf{h}(t), t), quad mathbf{h}(0)= mathbf{x} quad t in (0, T] dtdh(t)​=f(h(t),t),h(0)=xt∈(0,T] . The corresponding flow can be visualized to get an intuition of the transition from a ResNet to a Neural ODE (NODE), . . To put things in perspective, . In ResNets: we map an input x bf xx to output y bf yy by a forward pass through the network . | We tune the weights of the network to minimize d(y,ytrue)d({ bf y}, { bf y}_{ text{true}})d(y,ytrue​) . | For NODEs: we instead adjust the dynamics of the system encoded by f bf ff such that the ODE transforms input x bf xx to y bf yy to minimize d(y,ytrue)d({ bf y}, { bf y}_{ text{true}})d(y,ytrue​) . | . . ODE Flows . Before introducing the idea of Augmented Neural ODEs (ANODEs), we briefly revisit the notion of an ODE flow. The flow corresponding to a vector field f(h(t),t) mathbf{f}( mathbf{h}(t), t)f(h(t),t) is given by ϕ(t) phi(t)ϕ(t), such that, . ϕt:Rd→Rd,ϕt(x)=h(t)withh(0)=x phi_{t}: mathbb{R}^{d} rightarrow mathbb{R}^{d}, quad phi_{t}( mathbf{x})= mathbf{h}(t) quad text{with} quad mathbf{h}(0) = mathbf{x} ϕt​:Rd→Rd,ϕt​(x)=h(t)withh(0)=x . It is worth noting that the flow resulting from a Neural ODE is homeomorphic, i.e. it is continuous and bijective with a continuous inverse. Physically, the flow measures how the states of the ODE at a given time ttt depend on the initial conditions x bf xx. Note that for classification/regression problems, we often define a NODE g:Rd→Rg: mathbb{R}^{d} rightarrow mathbb{R}g:Rd→R as g(x)=L(ϕ(x))g( mathbf{x})= mathcal{L}( phi( mathbf{x}))g(x)=L(ϕ(x)), where L:Rd→R mathcal{L}: mathbb{R}^{d} rightarrow mathbb{R}L:Rd→R is a linear map and ϕ:Rd→Rd phi: mathbb{R}^{d} rightarrow mathbb{R}^{d}ϕ:Rd→Rd is the mapping from data to features. . . . Limitations of Neural ODEs/ODE Flows . It is important to note that not all functions can be approximated by a NODE/ODEFlow. Consider for instance g1&nbsp;d:R→Rg_{1 mathrm{~d}}: mathbb{R} rightarrow mathbb{R}g1&nbsp;d​:R→R, such that g1&nbsp;d(−1)=1g_{1 mathrm{~d}}(-1)=1g1&nbsp;d​(−1)=1 and g1&nbsp;d(1)=−1g_{1 mathrm{~d}}(1)=-1g1&nbsp;d​(1)=−1. It can be seen clearly from the figure below that a NODE cannot approximate this function, no matter how small a timestep or how large the terminal time TTT. This is due to the fact that the ODE trajectories cannot cross each other. A formal proof can be found in the appendix in Dupont et al., 2019, however it is simply built around the uniqueness of a solution to an ODE. An ODE cannot have two solutions that are different everywhere but at point. That is, the solutions are either identical or they do not intersect at any point. ResNets on the other hand do not suffer from this, as can be seen from the figure on the top-right. . . Having motivated through a 111D example, let us now consider the 222D version of it, i.e. . {g(x)=−1&nbsp;if&nbsp;∥x∥≤r1g(x)=1&nbsp;if&nbsp;r2≤∥x∥≤r3 begin{cases}g( mathbf{x})=-1 &amp; text { if } | mathbf{x} | leq r_{1} g( mathbf{x})=1 &amp; text { if } r_{2} leq | mathbf{x} | leq r_{3} end{cases} {g(x)=−1g(x)=1​&nbsp;if&nbsp;∥x∥≤r1​&nbsp;if&nbsp;r2​≤∥x∥≤r3​​ . . In theory Neural ODEs cannot represent the above function, since the red and blue regions are not linearly separable. In this case too ResNets can approximate the function. Plotting the loss function gives a more complete picture . . As it can bee seen from the above figure, in practice, Neural ODEs are able to approximate the function, but the resulting flow is much more complicated (see the time taken by the NODE to reach the same loss for the 222D example problem) . This motivates exploring an augmented space and seeing its effect the learned ODE. In other words, it turns out that zero padding the input, say with a ppp dimensional vector, dramatically improves the learning and the resulting Neural ODE (known as an Augmented Neural ODE) is able to gain expressivity and lead to simpler flows. . . . Augmented Neural ODEs (ANODEs) . As motivated above the idea is to augment the space on which the ODE is learned. In other words, . Rd→Rd+p mathbb{R}^d rightarrow mathbb{R}^{d+p}Rd→Rd+p which allows the ODE to lift points into additional dimensions to avoid trajectories from intersecting each other. Let a(t)∈Rp{ bf a}(t) in mathbb{R}^pa(t)∈Rp be a point in the augmented part of the space, the reformulation can be written as . ddt[h(t)a(t)]=f([h(t)a(t)],t),[h(0)a(0)]=[x0] frac{ mathrm{d}}{ mathrm{d} t} left[ begin{array}{l} mathbf{h}(t) mathbf{a}(t) end{array} right]= mathbf{f} left( left[ begin{array}{l} mathbf{h}(t) mathbf{a}(t) end{array} right], t right), quad left[ begin{array}{l} mathbf{h}(0) mathbf{a}(0) end{array} right]= left[ begin{array}{l} mathbf{x} mathbf{0} end{array} right] dtd​[h(t)a(t)​]=f([h(t)a(t)​],t),[h(0)a(0)​]=[x0​] . Plotting the loss function corresponding to each of the two toy examples verifies that ANODEs learn much simpler flows and the resulting loss function decays much faster compared to vanilla-Neural ODEs. . . It can be seen that the corresponding flows are almost linear for ANODEs and therefore the number of function evaluations are much fewer compared to NODEs. This point is further reinforced when we plot the number of function evaluations (and resulting evolution of the features) corresponding to each of the two approaches . . As we can see the number of function evaluations almost doubles for NODEs but remains roughly the same for ANODEs. . Generalization . In order to see the generalization properties of ANODEs the authors train both ANODE and NODE to have zero training loss and then visualize the points in the output space to which each point in the input gets mapped to. . . ANODEs again lead to flows that are much more plausible compared to NODEs. This is because NODEs can only continuously deform the input space. Therefore, the learned flow must squeeze points in the inner circle through the annulus leading to poor generalization. In order to test the generalization properties of ANODEs, the authors consider a further test. They create a validation set by removing random slices of the input space and train both NODEs and ANODEs on the training set and plot the evolution of the validation loss during training. The same thing emerges out, that is, ANODEs generalize better! . . Experiments . The authors carry out generative modeling experiments on the popular MNIST, CIFAR10 and SVHN datasets. The same story emerges from there as well. ANODEs outperform NODEs for the most part. For the figure below, p=0p=0p=0 corresponds to the base case (NODEs), where ppp denotes the number of extra channels in the augmented space. Results for MNIST and CIFAR 10 are given below . . Conclusions . Bottlenecks/limitations of ANODEs . A few additional insights that emerge from the experiments carried out by the authors are as follows . While ANODEs are faster than NODEs, they are still slower than ResNets (see the figure from their appendix below) . | Augmentation changes the dimension of the input space which, depending on the application, may not be desirable . | The augmented dimension ppp can be seen as an extra hyperparameter to tune. . | For excessively large augmented dimensions (e.g. adding 100100100 channels to MNIST), the model tends to perform worse with higher losses and NFEs . | . . The above figure corresponds to the 222D toy example, namely, . {g(x)=−1&nbsp;if&nbsp;∥x∥≤r1g(x)=1&nbsp;if&nbsp;r2≤∥x∥≤r3 begin{cases}g( mathbf{x})=-1 &amp; text { if } | mathbf{x} | leq r_{1} g( mathbf{x})=1 &amp; text { if } r_{2} leq | mathbf{x} | leq r_{3} end{cases} {g(x)=−1g(x)=1​&nbsp;if&nbsp;∥x∥≤r1​&nbsp;if&nbsp;r2​≤∥x∥≤r3​​ . Conclusion . There are classes of functions NODEs cannot represent and, in particular, that NODEs only learn features that are homeomorphic text{ color{red}homeomorphic}homeomorphic to the input space . | This leads to slower&nbsp;learning&nbsp;and&nbsp;complex&nbsp;flows text{ color{red}slower learning and complex flows}slower&nbsp;learning&nbsp;and&nbsp;complex&nbsp;flows which are computationall expensive . | Augmented Neural ODEs learn the flow from input to features in an augmented space and can therefore model more complex functions using simpler flows while at the same time achieving lower losses, incurring lower computational cost, and improved stability and generalization . | . . Code . The code to reproduce key findings from the paper is developed on top of a PyTorch library torchdiffeq and can be accessed at the authors’ git repository. . Several other open source implementations are available online. A fast and flexible implementation in Julia is available in the DiffEqFlux library here, which builds on top of the Flux.jl framework and as part of the larger SciML ecosystem in Julia. . . References . Augmented Neural ODEs. Emilien Dupont, Arnaud Doucet, Yee Whye Teh . | Neural ordinary differential equations. Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud . | FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models. Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud . | .",
            "url": "https://cs598ban.github.io/Fall2021/normalizing%20flows/2021/11/04/NODE2_blog.html",
            "relUrl": "/normalizing%20flows/2021/11/04/NODE2_blog.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "NODE1 Neural ODE",
            "content": ". Abstract . Today we’ll talk about the popular Neural ODE paper which won best paper award in Neurips 2018. This paper develops a foundational framework for infinitely many layered deep neural networks. The framework allows us to take advantage of the extensive research on ODE Solvers. . . Background : Ordinary Differential Equations . In physics, Ordinary Differential Equations(ODEs) have been often used to describe the dynamics and referred as a vector field of the underlying physical system. . . A system of ODEs can be represented as, dydt=f(t,y(t)) frac{dy}{dt} = f(t,y(t))dtdy​=f(t,y(t)) . Neural ODEs aims to replace explicit ODEs by an ODE with learnable parameters. In past, extensive research has been done on explicit and implicit ODESolvers which aims to solve an ODE(forward pass). The simplest ODE solver Forward Euler method works by moving along the gradient from a starting point: yn+1=yn+δf(tn,yn)y_{n+1} = y_n + delta f(t_n,y_n)yn+1​=yn​+δf(tn​,yn​) Sophisticated higher order explicit ODE solvers like Rungakutta have low error margin. An example of implicit ODE Solver is Backward Euler Method: yn+1=yn+δf(tn+1,yn+1)y_{n+1} = y_n + delta f(t_{n+1},y_{n+1})yn+1​=yn​+δf(tn+1​,yn+1​) The implicit solvers are computationally expensive but often has better approximation guarantees. Various adaptive-step size solvers have been developed which provide better error handling. . Problem Setup: Supervised learning . Traditional Machine learning solves: y=f(x)y = f(x)y=f(x), where, yyy is label and xxx is input features. Neural ODEs views the same problem as an ODE with Initial value problem, dydt=f′(x),y(0)=x frac{dy}{dt} = f&#39;(x), y(0) = xdtdy​=f′(x),y(0)=x where the value at initial time point is input features xxx and the label yyy is the value at final time point. For now, Let’s assume that xxx and yyy have same dimensionality for simplicity. Neural ODE aims to learn an invertible transformation between xxx and yyy. . The idea of viewing supervised learning as an ODE system came by observing the updates in Resnet. In Resnet, the updates in t+1t+1t+1-th layer is as follows, ht+1=ht+f(ht,θt) mathbf{h}_{t+1} = mathbf{h}_t + f( mathbf{h}_t, theta_t)ht+1​=ht​+f(ht​,θt​) We can view above update as the euler discretization of a continuous dynamic system, dh(t)dt=f(h(t),t,θ) frac{d mathbf{h}(t)}{dt} = f( mathbf{h}(t), t, theta)dtdh(t)​=f(h(t),t,θ) We can therefore view each layer as an euler update in an ODE solver with dynamics fff. . Infinite layers: ODE Solver as foward pass . Did you ever think that a DNN can have infinite layers? If we try to implement it naively by having different parameters in each layer, then there are two prominent issues we’ll be facing, i)Overfitting ii)Memory issues. This is because each layer with learnable parameters in DNN needs to store its input until the backward pass. So, it’s not practically feasible in traditional setting. . A natural question arises that “why can’t we have infinite updates in ODE solver instead of lll updates?”, where lll denotes number of euler steps or layers. We just need to use state of the art adaptive ODE Solvers which is memory efficient (more discussed later). . . Neural Ordinary Differential equations . Here we summarize the underlying idea of Neural ODE. Instead of trying to solve for y=F(x)y = F(x)y=F(x), Neural ODE solves, y=z(t1)y = z(t_1)y=z(t1​), given the initial condition z(0)=xz(0) = xz(0)=x. The parametrization is done as, dz(t)dt=f(z(t),t,θ) frac{dz(t)}{dt} = f(z(t), t, theta)dtdz(t)​=f(z(t),t,θ) The existing ODE Solvers are used for forward pass. . Backpropagation through Neural ODE . Ultimately we want to optimize some loss w.r.t. parameters θ,t0,t1 theta, t_0, t_1θ,t0​,t1​, L(z(t1))=L(z(t0)+∫t0Tf(z(t),t,θ)=L(ODESolve(z(t0),t0,t1,θ)L(z(t_1)) = L(z(t_0) + int_{t_0}^{T} f(z(t), t, theta) = L (ODESolve(z(t_0), t_0, t_1, theta)L(z(t1​))=L(z(t0​)+∫t0​T​f(z(t),t,θ)=L(ODESolve(z(t0​),t0​,t1​,θ) If loss is mean squared error, then we can write, L(z(t1))=E((y−z(t1))2L(z(t_1)) = E((y-z(t_1))^2L(z(t1​))=E((y−z(t1​))2. Without loss of generality, we’ll primary focus on computing dLdθ frac{d L}{d theta}dθdL​. If we know the ODE solver, then we can backprop through the solver using automatic differentiation. However, there are two issues with this approach, i) Backpropagation is dependent on ODE Solvers, this is not desirable. Ideally, we would like to treat ODE Solver as a black box. ii) If we use “implicit” solvers which often perform inner optimization, then the backpropagation using automatic differentiation is memory intensive. . Adjoint sensitivity analysis: Reverse-mode Autodiff . This paper borrows an age old idea of adjoint based methods from ODE literature to perform backprobagation with respect to parameters θ thetaθ in constant memory and without having the knowledge of ODE Solver. To compute dLdθ frac{d L}{d theta}dθdL​ we need to compute dLdz(t) frac{d L}{d mathbf{z}(t)}dz(t)dL​. Therefore, let’s define the adjoint state a(t)=dLdz(t) mathbf{a}(t) = frac{d L}{d mathbf{z}(t)}a(t)=dz(t)dL​. In the case of Resnet, the adjoint state has following updates during backpropagation, . a(t)=a(t+h)+ha(t+h)df(z(t))dz(t)a(t) = a(t+h) + h a(t+h) frac{d f(z(t))}{dz(t)}a(t)=a(t+h)+ha(t+h)dz(t)df(z(t))​ . You might have observed that updates of adjoint a(t)a(t)a(t) is an euler step in backward direction with a known dynamics. You guessed it, right. The adjoint state in continuous setting follows following dynamics in backward direction, . a(t)=a(t+1)+∫t+1ta(t)df(z(t),t,θ)dza(t) = a(t+1) + int_{t+1}^t a(t) frac{d f( mathbf{z}(t), t, theta)}{d mathbf{z}}a(t)=a(t+1)+∫t+1t​a(t)dzdf(z(t),t,θ)​ . da(t)dt=−a(t)Tdf(z(t),t,θ)dz frac{d mathbf{a}(t) }{dt} = - mathbf{a}(t)^T frac{d f( mathbf{z}(t), t, theta)}{d mathbf{z}}dtda(t)​=−a(t)Tdzdf(z(t),t,θ)​ . For Resnet, the updates of loss is, . dLdθ=ha(t+h)df(z(t),t,θ)dθ frac{d L}{d theta} = h a(t+h) frac{d f( mathbf{z}(t), t, theta)}{d theta}dθdL​=ha(t+h)dθdf(z(t),t,θ)​ . Similarly, loss in continuous setting follows a dynamics in the backward direction, . dLdθ=∫t1t0a(t)df(z(t),t,θ)dθ frac{d L}{d theta} = int_{t_1}^{t_0}a(t) frac{d f( mathbf{z}(t), t, theta)}{d theta}dθdL​=∫t1​t0​​a(t)dθdf(z(t),t,θ)​ . Thus, during the backward pass of z(t)z(t)z(t), we also need to do a backward pass on a(t)a(t)a(t) and dLdθ frac{d L}{d theta}dθdL​. The vector jacobian products a(t)T∂f(z(t),t,θ)∂z mathbf{a}(t)^T frac{ partial f( mathbf{z}(t), t, theta)}{ partial mathbf{z}}a(t)T∂z∂f(z(t),t,θ)​ and a(t)T∂f(z(t),t,θ)∂θ mathbf{a}(t)^T frac{ partial f( mathbf{z}(t), t, theta)}{ partial theta }a(t)T∂θ∂f(z(t),t,θ)​ can be computed using automatic differentiation in similar time cost as of fff. The full algorithms for all three backward dynamics is shown below, . . Results . Experiment 1 : Supervised Learning The paper performs a supervised learning on MNIST data. It uses resnet and multi-layered perceptron as baselines. The result is shown below, . . RK-Net uses Runga kutta for forward pass and automatic differentiation for backward pass. We can see that ODENet has fewer parameters with similar accuracy and constant memory. . Experiment 2 : Normalizing flows The paper proposes a continuous version of normalizing flows. Traditionally normalizing flows is enabled using change of variable formula. This paper proposes instantaneous change of variables formula using ODE dynamics. . . The results are shown belo, . . Time Series Latent ODE Traditional Rnns can only utilize regular time interval signals in its vanilla form. Neural ODE allows us to sample from a continuous dynamic system, . . The result for both the RNN and neural ODE for continuous time series for a spiral synthetic data is shown below. It can be seen that RNNs learn very stiff dynamics and have exploding gradients while ODEs are guaranteed to be smooth. . . Conclusion . Personally, I believe that this is a phenomonal paper which has enabled us to have infinite layers, handle continuous time series and normalizing flow in constant memory by using the state of the art ODE Solvers. It’s impact on learning dynamics for physical and biophysical system would be immense in future. . References . Chen, R. T., Rubanova, Y., Bettencourt, J., &amp; Duvenaud, D. (2018). Neural ordinary differential equations. arXiv preprint arXiv:1806.07366. . .",
            "url": "https://cs598ban.github.io/Fall2021/node/2021/11/02/NODE1_blog.html",
            "relUrl": "/node/2021/11/02/NODE1_blog.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "GAN5 Understanding the Connection Between Privacy and Generalization in Generative Adversarial Networks",
            "content": "Today we will discuss the connection between privacy and generalization as applied towards GANs as was introduced by the paper Generalization in Generative Adversarial Networks: A Novel Perspective from Privacy Protection by Bingzhe Wu, Shiwan Zhao, ChaoChao Chen, Haoyang Xu, Li Wang, Xiaolu Zhang, Guangyu Sun, and Jun Zhou. While relatively simple in both its theoretical and experimental contributions, the paper is nonetheless thought provoking and offers an interesting and important perspective relating these two important ideas. Hopefully this post characterizes those contributions and is able to introduce you to the concepts of privacy and generalization as well as their connection. . Background . GANs were first introduced by Goodfellow et al. and quickly grew in popularity as a powerful generative model. They are defined by a two player min-max game played between the Generator and the Discriminator models, which are typically parameterized as neural networks. The Generator takes random noise and outputs samples mirroring the desired data distribution, typically an image. The Discriminator then takes samples, both real and fake, and attempts to predict whether or not they are real. In competing to be able to fool or correctly adjudicate one another, the models both improve until the Generator is able to output high quality images. . . At the point of time where this paper was released, GANs were already well-studied and achieving impressive state of the art results in image generation among other applications. One main focuses of research in the field at the time was regarding attempts at addressing regularization of GAN training for the purpose of better generalization. This work fell into two main paths addressed by the authors as relevant to their work. The first is Lipschitz regularization which seeks to make the Discriminator Lipschitz continuous through methods such as gradient clipping, gradient penalties, or spectral regularization. The second is the relatively new Bayesian GAN trained using Bayesian learning. Each of these methods had been recently introduced and showed improvements in performance and generalization, and these authors explored why from the novel perspective of privacy. . The Motivation . The paper was motivated by the simple realization that both reducing the generalization gap and protecting the individual’s privacy are achieved in the same way. They both share the same goal of encouraging a neural network to learn the features of the underlying distribution rather than memorizing the features of each individual. So, the paper builds on that connection and explores it both theoretically and empirically. . Theoretical Analysis . The authors perform their theoretical analyses not on a specific architecture or objective but rather the learning algorithms for the Discriminator and Generator, Ad mathcal{A}_dAd​ and Ag mathcal{A}_gAg​, as a whole. Specifically, in the context of GAN training, they analyze Ad mathcal{A}_dAd​ which maximizes U(θd,θg∗)=Ex∼pdata[ϕ(d(x;θd))]+Ez∼pz[ϕ(1−d(g(z;θg∗);θd))]U( theta_d, theta^*_g) = mathbb{E}_{x sim p_{data}}[ phi( mathbf{d}(x; theta_d))] + mathbb{E}_{z sim p_z}[ phi(1- mathbf{d}( mathbf{g}(z; theta^*_g); theta_d))]U(θd​,θg∗​)=Ex∼pdata​​[ϕ(d(x;θd​))]+Ez∼pz​​[ϕ(1−d(g(z;θg∗​);θd​))] and Ag mathcal{A}_gAg​ which minimizes V(θd∗,θg)=Ez∼pz[ϕ(1−d(g(z;θg∗);θd))]V( theta^*_d, theta_g) = mathbb{E}_{z sim p_z}[ phi(1- mathbf{d}( mathbf{g}(z; theta^*_g); theta_d))]V(θd∗​,θg​)=Ez∼pz​​[ϕ(1−d(g(z;θg∗​);θd​))], defining the characteristic min-max game. . However, while Ad mathcal{A}_dAd​ and Ag mathcal{A}_gAg​ nominally optimize over UUU and VVV, the analysis stems from the fact that they in practice do not. The true data distribution pdatap_{data}pdata​ is intractable, so a finite training dataset is used instead. Thus, Ad mathcal{A}_dAd​ and Ag mathcal{A}_gAg​ actually optimize over the empirical losses U^ hat{U}U^ and V^ hat{V}V^. Since V^ hat{V}V^ doesn’t actually touch the training data (it is just the noise passed into the Generator), the analysis then simply considers Ad mathcal{A}_dAd​ optimizing U^ hat{U}U^ given a fixed Generator θg∗ theta^*_gθg∗​ where they seek to understand the generalization gap FU(Ad)=Eθd∼Ad(S)ES∼pdatam[U^(θd,θg∗)−U(θd,θg∗)]F_U( mathcal{A}_d) = mathbb{E}_{ theta_d sim mathcal{A}_d(S)} mathbb{E}_{S sim p_{data}^m}[ hat{U}( theta_d, theta^*_g) - U( theta_d, theta^*_g)]FU​(Ad​)=Eθd​∼Ad​(S)​ES∼pdatam​​[U^(θd​,θg∗​)−U(θd​,θg∗​)] which is how much worse the Discriminator performs due to using the empirical distribution and loss rather than the true underlying values. . With the GAN and generalization definitions formulated, the only remaining piece neccessary before moving into the theoretical analyses are a couple of definitions regarding privacy. The first is differential privacy. Formally, A randomized algorithm A:D→R mathcal{A} : D rarr RA:D→R satisfies ϵ epsilonϵ-differential privacy if for any two adjacent datasets (datasets with a single datum switched) S,S′⊆S, S&#39; subseteqS,S′⊆ D and for any subset of outputs O⊆RO subseteq RO⊆R it holds: Pr[A(S)∈O]≤eϵPr[A(S′)∈O] mathbf{Pr}[ mathcal{A}(S) in O] leq e^ epsilon mathbf{Pr}[ mathcal{A}(S&#39;) in O]Pr[A(S)∈O]≤eϵPr[A(S′)∈O] which basically means that a single data point has negligible effect on the final weights and thus outputs of the model. The second is Uniform RO-Stability. A randomized algorithm A mathcal{A}A is uniform RO-stable with respect to the discriminator loss function if for all adjacent datasets S,S′S, S&#39;S,S′ it holds that: sup⁡x∈S∣Eθd∼A(S)[ϕ(d(x;θd))]−Eθd∼A(S′)[ϕ(d(x;θd))]∣≤ϵstable(m) sup_{x in S}| mathbb{E}_{ theta_d sim mathcal{A}(S)}[ phi( mathbf{d}(x; theta_d))] - mathbb{E}_{ theta_d sim mathcal{A}(S&#39;)}[ phi( mathbf{d}(x; theta_d))]| leq epsilon_{stable}(m)x∈Ssup​∣Eθd​∼A(S)​[ϕ(d(x;θd​))]−Eθd​∼A(S′)​[ϕ(d(x;θd​))]∣≤ϵstable​(m) which basically means that a small change to the input yields a constrained change to the output. . Now we are finally ready to build the theoretical results. The authors use two lemmas from previous literature to build two theorems of their own. The two lemmas state that differential privacy entails Uniform RO-Stability and that Uniform RO-Stability entails a bounded generalization gap (with each of the three concepts coming with their own ϵ epsilonϵ bound within the lemmas). The author’s first theorem then simply connects the two lemmas to state that ϵ epsilonϵ-differential privacy entails a data-independent bounding of the generalization gap. The second theorem states roughly the same idea in the context of GANs, showing that if Ad mathcal{A}_dAd​ is ϵ epsilonϵ-differentially private, then the generalization gap of the discriminator dk mathbf{d}^kdk outputted by the k-th iteration of Ad mathcal{A}_dAd​ can be bounded by a universal constant related to ϵ epsilonϵ. . These theorems are relatively simple in building on existing principles and lemmas (and in fact both have proofs which are given in no more than seven lines), but they are nonetheless important and thought provoking. They demonstrate theoretically that privacy entails generalization, and they furthermore explain how recent techniques such as Lipschitz regularization and Bayesian learning are so successful in producing more generalizable results as they are previously known to be important and effective factors in producing algorithms with strong privacy guarantees. . Quantitative Experiments . Upon arriving at their theoretical results showing that privacy entails generalization, the authors continued to perform a number of experiments showing that generalization generally correlates in the other direction back to privacy. They did so by performing membership attacks based on a variety of differently trained GAN models. . Membership Attacks . A membership attack is an attempt to determine whether a given piece of data is from the original training set that a model is trained. The attacker can either have access to the model in question itself or a series of results that they use to build a proxy network, and the authors tested both such variants. . . Experimental Setup . The authors carried out their membership attack setup on two different datasets, Labeled Faces in the Wild and Invasive Ductal Carcinoma image. They trained a variety of different GAN algorithms (both regularized and normal) within the DCGAN architecture. They then created attack datasets by randomly picking samples mixing the original training and testing datasets. Finally, they created a very simple attacker model which for a given data point outputs that the datapoint is in the training dataset if the discriminator (or proxy) model outputs a greater than average value when given the data point. . Experimental Results . The authors found that GANs which generalized better by containing some form of regularization were also more effective at impeding the performance of the attacker and so offered additional levels of privacy. In their findings there was a clear divide between the original GAN algorithm and all others (which each offered some form of regularization across both the original JS Divergence and the Wasserstein objectives) in terms of the original GAN having both a much larger generalization gap and also much higher metrics of F1 and AUC scores measuring the effectiveness of the membership attack. Specifically, in the experiment on the Labeled Faces in the Wild dataset assuming that the attacker has full access to the Dicriminator model, the attacker achieves an F1 score of 0.565 and AUC of 0.729 for the original GAN while only 0.515 and 0.512 for the next closest models, the Wasserstein GANs with Spectral Normalization and Weight Clipping respectively. Similarly, the generalization gap for the original GAN 0.581, and it is 0.113 for the next closest model, the Weight Clipping model with the original GAN objective. The table below gives the full results for both experiments in which the attacker had access to the full Discriminator model, and similar results were seen in the “black-box” attack setup in which they only had downstream results with which to build a proxy model. So, the experiment backed up the idea that more generalizable GANs are more effective at protecting the privacy of the original dataset. . . Wrapping Up . So, we have explored the new perspective offered by our authors on the topic of GAN generalization through the lens of privacy. We have shown that the connection between privacy is concrete and direct through a series of theorems demonstrating that privacy entails generalization, and we have shown that the connection holds quantitatively as well through the results of a series of membership attack experiments. While both the theoretical and experimental results are fairly simple, the insight and perspective is a valuable one, connecting the two very important topics and directing future work towards solving both problems. . Reference . Wu, B., Zhao, S., Xu, H., Chen, C., Wang, L., Zhang, X., Sun, G., and Zhou, J. Generalization in Generative Adversarial Netwokrs: A Novel Perspective from Privacy Protection. NeurIPS, 2019. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/privacy/2021/10/28/GAN5_blog2.html",
            "relUrl": "/gan/privacy/2021/10/28/GAN5_blog2.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "GAN5 Generalization and Equilibrium in Generative Adversarial Nets (GANs)",
            "content": "Intro . This blog post focuses on the paper Generalization and Equilibrium in Generative Adversarial Nets (GANs) and focuses on the concept of generalization with applications to GANs. The paper introduces the concept of generalization and the fact that the GAN model does not perform good generalization. The paper introduces a new type of distance measure to better calculate the generalization factor. The paper subsequently comes up within a new model called the MIX-GAN which focuses on demonstrating the importance of generalization when it comes to GAN training. The paper aims to help reader’s understand the purpose of the GAN model and how the model can be improved through the implementation of generalization while training the GAN. . ​ ​ . Background . GANs were first introduced by Goodfellow et al. 2014 and the Goodfellow GAN can be described as follows: GANs are type of generative model used to come up with images or other samples to imitate real life samples. In more detail, the objective of the GAN when it comes to training a generator deep net whose input is a standard Gaussian, and whose output is a sample from some real distribution. . GANs work by comparing two distributions, as mentioned above, where the generator net part of the GAN attempts to “fool” the discriminator function into thinking that the generated image is a real image. This dual system is trained through backpropagation of the result from the discriminator that feeds into the generator. Thus the relationship between the generator and the discriminator can be thought of as a game between two players solving a min max scenario. This concept is further explored within the paper. The min max game is critical to understanding GANs and how they are trained. . . (traditional GAN training objective from Goodfellow 2014) ​ In more detail the GAN objective function aims to minimize the output from the generator but the opposite is true for the discriminator which aims to maximize the result. The result is the label assigned to the training sample and then re-evaluated in the long term. ​ . Generalization . The definition of generalization in terms of GANs is related to how well the GAN is able to imitate the real distribution. A lot of the times the discriminator will be fooled but the derived distribution from the GAN model is weak and very ill representative of the actual distribution. Standard metrics are not as useful as measuring whether the distribution is close or not. Often there are times the GAN is able to train successfully to imitate the real distribution but according to standard metrics the trained distribution is deemed extremely different than the actual distribution, even though it might not be. Generalization is important when it comes to describing when the generator has won. At this point you need to consider is the created distribution and the real distribution the same or at least similar? This is tough to do when you have complicated distributions that have many valleys and peaks. Take for example this distribution: . . The distribution as mentioned has many peaks and valleys and when considering this distribution with high dimensionality there could be an exponential number of extrema. Sampling becomes a big issue. How do you sample so that all the extrema are accurately represented? How large would the sample size need to be for this to work? These questions are the underlying issue when it comes to modeling and working high dimensionality and complex distributions. The simple answer boils down to having a sample size of exponential size as well. However, that isn’t feasible and thus needs to be made more realistic. . Generalization Math . Let x1…xm be the training examples and let D^real be the uniform distribution for x1…xm. Also let Gu(h1)…Gu(hr) be a set of r examples from the generated distribution DG. In training, we use: . . to approximate the value of: . . Attempt to minimize the distance (or divergence) between D^real and DG. This can be done to a certain extreme and so the final simplified version of this equation can be represented as: . . The equation above gives a more concrete mathematical equation for Generalization and it can explained as the following: “generalization in GANs means that the population distance between the true and generated distribution is close to the empirical distance between the empirical distributions. Our target is to make the former distance small, whereas the latter one is what we can access and minimize in practice. The definition allows only polynomial number of samples from the generated distribution because the training algorithm should run in polynomial time.” The concept is a little confusing but can be simplified to this: in an ideal world we can draw an infinite number of samples from the true distribution to get the value for Dreal, however; in practicality that is not possible and so we have the empirical distance with a finite number of samples to estimate the expected value of the real distribution as seen above. The equation above takes it a step further and explicates that the distance between the empirical true distribution and empirical generated distribution is as close as it can be to the population distance between the population true distribution and the population generated distribution. The population distance is calculated theoretically and used in the calculation. Thus, the calculation is completed with an error bound that acts as measurement for the difference between the two distances. If the empirical distance is close enough to the population distance within this error bound, we can consider the empirical distance to be the same as the population distance. . Neural Net Distance . The paper brings up a problem with current metrics and that they are a bad method of measuring how well the generated distribution follows and imitates the original distribution. So, the authors behind the paper came up with a new measuring metrics strictly to see how well two distributions match each other. This metric is the neural net distance metric. . Neural Net Distance is derived from the f-divergence concept and follows the derivation of the f-divergence function closely. . . For example when φ(t) is log(t) and F = {all functions from Rd mapped to [0, 1]} then the derived equation above is equal to JS divergence. However, when φ(t) is t and F = {all 1-Lipschitz functions from Rd mapped to [0, 1]} then the equation above is equal to Wasserstein distance. . (Lipschitz functions are mentioned and Lipschitz refers to the training parameters and indicates that changing a parameter by delta changes the output of the deep net by less than constant * delta.) . | . All this is to indicate that the distance metric is variable and changes with what kind of base GAN you are working with / what kind of distribution you’d like to measure. . GAN training uses F to be a class of neural nets with a bound p on the number of parameters. An assumption can be made regarding the measuring function such that it takes in values between [−∆, ∆] and that it is Lφ-Lipschitz. . Even more so, F is a class of discriminators that is L-Lipschitz to the target distribution where p denotes the number of parameters in the target distribution. . . In a general sense you get the equation: . The downside of the Neural Net Distance is that it will return a small value even if u and v are are far from each other. This is due to the capacity being limited by p. . Equilibrium &amp; Mixtures . The idea of equilibrium stems from the GAN min max problem. When trying to train and optimize a GAN the generator must be minimized and the discriminator needs to be maximized. Equilibrium seeks a state where both the discriminator and the generator cannot be optimized further. The thing to keep in mind is that the saddle point reached is not necessarily zero. This paper puts a spin on that and recognizes an “equilibrium” value where generator is always less than or equal to this value. Conversely, the discriminator is always greater or equal to this value as demonstrated below. . . The S value seen in the theorem is used to represent the strategies that the discriminator and generator use respectively. The strategies are unchanged and so there is no changing of strategies once the other player’s strategy is revealed. Each player still performs to the best of their ability keeping in mind their initial strategy. . The paper focuses only on the generator side as opposed to the disciminator. The reasons for the focus on the generator only is as follows: . Payoff is generated by the generator: The payoff is generated by the generator first sample u ∼ Su, h ∼ Dh. This results in an example that is generated x = Gu(h). . | . Mixed Generator Composition vs. Mixed Discriminator: The mixed generator is compromised of a linear mixture of generators. The mixed discriminator is more complicated since the objective function of the discriminator may not be linear. Thus, when accounting for the objective function it escapes a linear space; otherwise the mixed discriminator would also be linear. . | . Output of Discriminator is not important: The output of the discriminator mixture is not important for consideration since the mixture is also unable to differentiate between the generated and actual distribution effectively. . | . All these scenarios are resolved using a mixture generator and approximating the difference between the two distributions to a certain error value (as seen with all the formulas above). . . This is what the strategies come out to be after accounting for a small margin of error. The basic idea here is that it takes O§ (where p is the number of parameters in the sample set) to approximate a quality distribution that is able to imitate the real distribution while accounting for the error. . There is a more complex version of this that takes into account pure strategies. The paper does not go into details about pure strategies but discusses a short overview about them. The idea boils down to the concept of mixtures, there is not much change from what is already written in this blog besides the fact that if a pure strategy is attempted then the complexity increases to O(p2) since each network compromises of O§ size and then those are used to make the mixture generator in a linear fashion. . MIX-GAN . A mix GAN is a GAN that makes use of the idea of a mixture of generators within reason. So they make use of a mixture of T components, where T is constrained by GPU memory (&lt; 5). Maintain T generators and T discriminators. All the respective weights for each generator is maintained and everything is trained through backwards propagation. The specific softmax used for training is listed below. . . The softmax function is critical for defining the weights which in turn are needed for the payoff function for the MIX+GAN. You can think of it as the original min max problem for the GAN as the structure of the equations are similar. . . Empirical Results . . . The examples above show the comparison between a typical DC-GAN and a MIX-DCGAN. The MIX-GAN is represented by the A side and the normal DCGAN is represented by the B side. THe interesting thing to note is the quality of the results of the GANs. The MIX-GAN performed slightly better in terms of more clear output compared to the results of the regular GAN. . . This is further evidenced by the results of the CIFAR-10 test results. The MIX-GAN is able to compete to the same performance level of the GAN that it models. At times it does better than the GAN model that it is imitating. The most curious thing about the MIX-GAN is ability to be small and retain very little parameters yet be able to compete with more complex and larger GANs, as seen with the MIX-DCGAN and DCGAN (5x time) size scores. . Conclusions . The concept of the MIX-GAN leads to more efficient GANs. They are typically smaller and perform the same as the normal version of the particular GAN model that they are attempting to turn into a mixture through the measuring function. The MIX-GAN allows for more quality samples and more diverse samples from the generated distribution because it is able to better reflect the true distribution unlike other GANs. On a theoretical standpoint this paper focused mainly improving the GAN’s ability to come to a good stopping point that properly reflects the true distribution. Many GANs fail to do this hence the team’s research into generalization and the neural net distance metric for a better way to determine if the generated distribution and real distribution are similar or not. . References . 2017 (ICML): S. Arora, R. Ge, Y. Liang, T. Ma, Y. Zhang. Generalization and equilibrium in generative adversarial nets (GANs). ICML, 2017. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/28/GAN5_blog.html",
            "relUrl": "/gan/2021/10/28/GAN5_blog.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "GAN4 Which Training Methods for GANs do actually Converge? （ICML 2018)",
            "content": "Introduction . Generative Adversarial Networks (GANs) are powerful latent variable models that can be used to learn complex real-world distributions. The recent works show the local convergence of GAN training for absolutely continuous data and generator distributions. However, while very powerful, GANs can be hard to train and in practice it is often observed that gradient descent based GAN optimization does not lead to convergence. . In this paper, the author discussed a counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. The paper also showed that how recent techniques for stabilizing GAN training affect local convergence on the example problem, as WGAN, WGAN-GP, and DRAGAN do not converge on this example. And based on this observation, the paper introduced simplified gradient penalties and prove local convergence for the regularized GAN training dynamics. . Problem Definition . We consider the traditional GAN training objective function as L(θ,ψ)=Ep(z)[f(Dψ(Gθ(z)))]+EpD(x)[f(−Dψ(x))] begin{align*} L( theta, psi) = mathbb{E}_{p(z)}[f(D_ psi(G_ theta(z)))] + mathbb{E}_{p_D(x)}[f(-D_ psi(x))] end{align*}L(θ,ψ)=Ep(z)​[f(Dψ​(Gθ​(z)))]+EpD​(x)​[f(−Dψ​(x))]​where the common choice is f(t)=−log⁡(1+exp(−t))f(t) = - log(1 + exp(-t))f(t)=−log(1+exp(−t)) considered in the original GAN paper. For technical reasons we assume that fff is continuously differentiable and satisifes f′(t)≠0f&#39;(t) neq 0f′(t)=0 for all t∈Rt in mathbb{R}t∈R. GANs are usually trained using Simultaneous or Altenating Gradient Descent (SimGD and AltGD), where both of them can be considered as fixed point algorithm that apply some operator Fh(θ,ψ)=(θ,ψ)+hv(θ,ψ)F_h( theta, psi) = ( theta, psi) + hv( theta, psi)Fh​(θ,ψ)=(θ,ψ)+hv(θ,ψ) where v(θ,ψ)v( theta, psi)v(θ,ψ) denotes the gradient vector field v(θ,ψ)=(−∇θL(θ,ψ)∇ψ(θ,ψ))v( theta, psi) = begin{pmatrix} - nabla_ theta L( theta, psi) nabla_{ psi} ( theta, psi) end{pmatrix}v(θ,ψ)=(−∇θ​L(θ,ψ)∇ψ​(θ,ψ)​) Recently, it was shown that local convergence of GAN training near an equilibrium point(θ⋆,ψ⋆)( theta^ star, psi^ star)(θ⋆,ψ⋆) can be analyzed by looking at the spectrum of the Jacobian Fh′(θ⋆,ψ⋆)F&#39;_h( theta^ star, psi^ star)Fh′​(θ⋆,ψ⋆) at the equilibrium: . If Fh′(θ⋆,ψ⋆)F&#39;_h( theta^ star, psi^ star)Fh′​(θ⋆,ψ⋆) has eigenvalues with absolute value bigger than 1, the training algorithm will generally not converge to(θ⋆,ψ⋆)( theta^ star, psi^ star)(θ⋆,ψ⋆). | On the other hand, If Fh′(θ⋆,ψ⋆)F&#39;_h( theta^ star, psi^ star)Fh′​(θ⋆,ψ⋆) has eigenvalues with absolute value smaller than 1, the algorithm will converge in sublinear time. | Dirac-GAN . Equipped with these definitions, we can now have the definition of a simple yet prototypical counterexample which shows that in the general case unregularized GAN training is neither locally nor globally convergent. . Defition 1 The Dirac-GAN consists of a (univariate) generator distribution pθ=γθp_ theta = gamma_ thetapθ​=γθ​ and a linear discriminator Dψ(x)=ψ⋅xD_ psi(x) = psi cdot xDψ​(x)=ψ⋅x. The true data distribution pDp_DpD​ is given by a Dirac-distribution concentrated at 0. . Lemma 2 The unique equilibrium point of the training objective is given by θ=ψ=0 theta = psi = 0θ=ψ=0. And the Jacobian of the gradient vector field has two eigenvalues ±f′(0)i pm f&#39;(0)i±f′(0)i. . Considering the idealized continuous systems in GAN training dynamics, in the previous works, it was assumed that the optimal discriminator parameter vector is a continuous function of the current generator parameters. . Lemma 2.3 The integral curves of the gradient vector field v(θ,ψ)v( theta, psi)v(θ,ψ) do not converge to the Nash-equilibrium. Every integral curve (θ(t),ψ(t))( theta(t), psi(t))(θ(t),ψ(t)) of the gradient vector field v(θ,ψ)v( theta, psi)v(θ,ψ) satisfies θ(t)2+ψ(t)2=const theta(t)^2 + psi(t)^2 = text{const}θ(t)2+ψ(t)2=const for all t∈[0,∞)t in [0, infty)t∈[0,∞) . In this case, unless θ=0 theta = 0θ=0, there is not even an optimal discriminator parameter for the Dirac-GAN. . And the following theorems showed that in two normal training dynamics of GAN: SimGD and AltGD, both encounter such instabilities. But where do these instabilities come from? . Figure 1: (a): In the beginning, the discriminator pushes the generator towards the true data distribution and the slope increases. (b): When the generator reaches the target distribution, the slope of the discriminator is the largest, pushing it away from the target distribution. This results in the oscillating behavior that will never converge. . Another way to look at it is to consider the local behavior of the training algorithm near the Nash-equilibrium, where there is no incentive for the discriminator to move to the equilibrium discriminator. . Figure 2: Converging properties of different GAN training algorithms using alternating gradient descent. We can clearly see that WGAN and WGAN-GP both do not converge on this example. . Regularization Techniques . A common technique to stabilize GANs is to add instance noise, i.e., independent Gaussian noise, to the data points. . Lemma 3.2 For the Dirac-GAN: When using Gaussian instance noise with standard deviation σ sigmaσ, the eigenvalues of the Jacobian of the gradient vector field are given by λ1/2=f′′(0)σ2±f′′(0)2σ4−f′(0)2 lambda_{1/2} = f&#39;&#39;(0) sigma^2 pm sqrt{f&#39;&#39;(0)^2 sigma^4 - f&#39;(0)^2}λ1/2​=f′′(0)σ2±f′′(0)2σ4−f′(0)2 . ​ This theorem also implies that in the case of absolutely continuous distributions, gradient descent based GAN optimization is, under suitable assumptions, locally convergent. . Zero-centered gradient penalties . A penalty on the squared norm of the gradients of the discriminator results in the regularizer R(ψ)=γ2ψ2R( psi) = frac{ gamma}{2} psi^2R(ψ)=2γ​ψ2 Lemma 3.3 The eigenvalues of the Jacobian of the gradient vector field for the gradient-regularized Dirac-GAN at the equilibrium point are given by λ1/2=−γ2±γ24−f′(0)2 lambda_{1/2} = - frac{ gamma}{2} pm sqrt{ frac{ gamma^2}{4} - f&#39;(0)^2}λ1/2​=−2γ​±4γ2​−f′(0)2 . ​ Like instance noise, there is a critical regularization parameter γcritical=2∣f′(0)∣ gamma_{ text{critical}} = 2|f&#39;(0)|γcritical​=2∣f′(0)∣ that results in a locally rotation free vector field. And in this case, simultaneous and alternating gradient descent are both locally convergent. . The analysis suggests that the main effect of the zero-centered gradient penalties on local stability is to penalize the discriminator for deviating from the Nash-equilibrium. Then we can derive the following gradient penalties. R1(ψ)=γ2EpD(x)[∣∣∇Dψ(x)∣∣2] begin{equation} R_1( psi) = frac{ gamma}{2} mathbb{E}_{p_D(x)}[|| nabla D_ psi(x)||^2] end{equation}R1​(ψ)=2γ​EpD​(x)​[∣∣∇Dψ​(x)∣∣2]​​ R2(ψ)=γ2Epθ(x)[∣∣∇Dψ(x)∣∣2] begin{equation} R_2( psi) = frac{ gamma}{2} mathbb{E}_{p_ theta(x)}[|| nabla D_ psi(x)||^2] end{equation}R2​(ψ)=2γ​Epθ​(x)​[∣∣∇Dψ​(x)∣∣2]​​ Figure 3: Measuring convergence for GANs is hard for high dimensional problems, because we lack a metric that can reliably detect non-convergent behavior. So only experiments on 2D Problems were conducted. . Conclusions: In this paper, the authors analyzed the stability of GAN training on a simple yet prototypical example and showed that (unregularized) gradient based GAN optimization is not always locally convergent. And the authors extended the local convergence with simplified zero-centered gradient penalties under suitable assumptions. . The relativistic discriminator: a key element missing from standard GAN (ICLR &#39;18) . In standard generative adversarial network (SGAN), the discriminator D estimatesthe probability that the input data is real. The generator G is trained to increase the probability that fake data is real. In this paper, the authors argue that it should also simultaneously decrease the probability that real data is real because . This would account for a priori knowledge that half of the data in the mini-batch is fake. | This would be observed with divergence minimization. | In optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. | Introduction . Problem Definition GANs can be defined generally in terms of the discriminator in the following way LD=Exr∼P[f~1(D(xr))]+Ez∼Pz[f~2(D(G(z)))] begin{equation} L_D = mathbb{E}_{x_r sim mathbb{P}}[ tilde f_1(D(x_r))] + mathbb{E}_{z sim mathbb{P}_z}[ tilde f_2(D(G(z)))] end{equation}LD​=Exr​∼P​[f~​1​(D(xr​))]+Ez∼Pz​​[f~​2​(D(G(z)))]​​ LG=Exr∼P[g~1(D(xr))]+Ez∼Pz[g~2(D(G(z)))] begin{equation} L_G = mathbb{E}_{x_r sim mathbb{P}}[ tilde g_1(D(x_r))] + mathbb{E}_{z sim mathbb{P}_z}[ tilde g_2(D(G(z)))] end{equation}LG​=Exr​∼P​[g~​1​(D(xr​))]+Ez∼Pz​​[g~​2​(D(G(z)))]​​ where f~1,f~2,g~1,g~2 tilde f_1, tilde f_2, tilde g_1, tilde g_2f~​1​,f~​2​,g~​1​,g~​2​ are scalar-to-sclar functions. P mathbb{P}P is the distribution of the real data. . Integral Probability Metrics (IPM): IPMs are statistical divergences represented mathematically as IPMF(P∣∣Q)=sup⁡C∈FEx∼P[C(x)]−Ex∼Q[C(x)] begin{equation} IPM_F( mathbb{P} || mathbb{Q}) = sup_{C in mathcal{F}} mathbb{E}_{x sim mathbb{P}}[C(x)] - mathbb{E}_{x sim mathbb{Q}}[C(x)] end{equation}IPMF​(P∣∣Q)=C∈Fsup​Ex∼P​[C(x)]−Ex∼Q​[C(x)]​​ IPM-based GANs can be defined using euqation 1 and 2 assuming f~1(D(x))=g~2(D(x))=−D(x) tilde f_1(D(x)) = tilde g_2(D(x)) = -D(x)f~​1​(D(x))=g~​2​(D(x))=−D(x) and f~2(D(x))=g~1(D(x))=D(x) tilde f_2(D(x)) = tilde g_1(D(x)) = D(x)f~​2​(D(x))=g~​1​(D(x))=D(x) and D(x)=C(x)D(x) = C(x)D(x)=C(x). In this paper, the authors argued that the key missing property of SGAN is that the probability of real data being real (D(xr))(D(x_r))(D(xr​)) should decrease as the probability of fake data being real (D(xf))(D(x_f))(D(xf​)) increase. Figure 1: Expected discriminator output of the real and fake data for the direct minimization of the JSD, actual training of the generator to minimize its loss function, and ideal training of the generator to minimize its loss function. . SGAN completely ignores the a priori knowledge that half of the mini-batch samples are fake. And IPM-based GANs implicitly account for the fact that some of the samples must be fake because they compere how realistic real data is compared to fake data. . In SGAN, the discriminator loss function is equal to the Jensen-Shannon divergence. Thus, it can be represented as solving the following maximum problem JSD(P∣∣Q)=12(log⁡(4)+max⁡D:x→[0,1])Exr∼P[log⁡(D(xr))]+Exf∼Q[log⁡(1−D(xf))] begin{equation} JSD( mathbb{P} || mathbb{Q}) = frac{1}{2} ( log (4) + max_{D:x rightarrow [0,1]}) mathbb{E}_{x_r sim mathbb{P}}[ log (D(x_r))] + mathbb{E}_{x_f sim mathbb{Q}}[ log (1 - D(x_f))] end{equation}JSD(P∣∣Q)=21​(log(4)+D:x→[0,1]max​)Exr​∼P​[log(D(xr​))]+Exf​∼Q​[log(1−D(xf​))]​​ In terms of the gradient steps of SGAN and IPM-based GANs, ∇wLDGAN=−Exr∼P[(1−D(xr))∇wC(xr)]+Exf∼Qθ[D(xf)∇wC(xf)] begin{equation} nabla_w L_D^{GAN} = - mathbb{E}_{x_r sim mathbb{P}}[(1 - D(x_r)) nabla_w C(x_r)] + mathbb{E}_{x_f sim mathbb{Q}_ theta}[D(x_f) nabla w C(x_f)] end{equation}∇w​LDGAN​=−Exr​∼P​[(1−D(xr​))∇w​C(xr​)]+Exf​∼Qθ​​[D(xf​)∇wC(xf​)]​​ ∇θLGGAN=−Ez∼Pz[(1−D(G(z)))∇xC(Gz)JθG(z)] begin{equation} nabla_ theta L_G^{GAN} = - mathbb{E}_{z sim mathbb{P}_z}[(1 - D(G(z))) nabla_x C(G_z) J_ theta G(z)] end{equation}∇θ​LGGAN​=−Ez∼Pz​​[(1−D(G(z)))∇x​C(Gz​)Jθ​G(z)]​​ ∇wLDIPM=−Exr∼P[∇wC(xr)]+Exf∼Qθ[∇wC(xf)] begin{equation} nabla_w L_D^{IPM} = - mathbb{E}_{x_r sim mathbb{P}}[ nabla_w C(x_r)] + mathbb{E}_{x_f sim mathbb{Q}_ theta}[ nabla w C(x_f)] end{equation}∇w​LDIPM​=−Exr​∼P​[∇w​C(xr​)]+Exf​∼Qθ​​[∇wC(xf​)]​​ ∇θLGIPM=−Ez∼Pz[∇xC(Gz)JθG(z)] begin{equation} nabla_ theta L_G^{IPM} = - mathbb{E}_{z sim mathbb{P}_z}[ nabla_x C(G_z) J_ theta G(z)] end{equation}∇θ​LGIPM​=−Ez∼Pz​​[∇x​C(Gz​)Jθ​G(z)]​​ In IPMs, oth real and fake data equally contribute to the gradient of the discriminator’s loss function.However, in SGAN, if the discriminator reach optimality, the gradient completely ignores real data, which means if D(xr)D(x_r)D(xr​) does not indirectly change when training the discriminator to reduce D(xf)D(x_f)D(xf​),the discriminator will stop learning what it means for data to be “real” and training will focus entirely on fake data. The discriminator estimates the probability that the given real data is more realistic than a randomly sampled fake data. When the discriminator is defined only on C(x)C(x)C(x). Then we have the discriminator and generator loss functions of the Relativistic Standard GAN LDRSGAN=−E(xr,Xf)∼(P,Q)[log⁡(sigmoid(C(xr)−C(xf)))] begin{equation} L_D^{RSGAN} = - mathbb{E}_{(x_r, X_f) sim ( mathbb{P}, mathbb{Q})}[ log ( text{sigmoid}(C(x_r) - C(x_f)))] end{equation}LDRSGAN​=−E(xr​,Xf​)∼(P,Q)​[log(sigmoid(C(xr​)−C(xf​)))]​​ LGRSGAN=−E(xr,Xf)∼(P,Q)[log⁡(sigmoid(C(xf)−C(xr)))] begin{equation} L_G^{RSGAN} = - mathbb{E}_{(x_r, X_f) sim ( mathbb{P}, mathbb{Q})}[ log ( text{sigmoid}(C(x_f) - C(x_r)))] end{equation}LGRSGAN​=−E(xr​,Xf​)∼(P,Q)​[log(sigmoid(C(xf​)−C(xr​)))]​​ And for discriminator defined as a(C(xr)−C(xf))a(C(x_r) - C(x_f))a(C(xr​)−C(xf​)) LDRGAN=E(xr,Xf)∼(P,Q)[f1(C(xr)−C(xf))]+E(xr,Xf)∼(P,Q)[f2(C(xf)−C(xr))]LGRGAN=E(xr,Xf)∼(P,Q)[g1(C(xr)−C(xf))]+E(xr,Xf)∼(P,Q)[g2(C(xf)−C(xr))] begin{align*} L_D^{RGAN}&amp; = mathbb{E}_{(x_r, X_f) sim ( mathbb{P}, mathbb{Q})}[f_1(C(x_r) - C(x_f))] &amp; + mathbb{E}_{(x_r, X_f) sim ( mathbb{P}, mathbb{Q})}[f_2(C(x_f) - C(x_r))] L_G^{RGAN} &amp; = mathbb{E}_{(x_r, X_f) sim ( mathbb{P}, mathbb{Q})}[g_1(C(x_r) - C(x_f))] &amp; + mathbb{E}_{(x_r, X_f) sim ( mathbb{P}, mathbb{Q})}[g_2(C(x_f) - C(x_r))] end{align*}LDRGAN​LGRGAN​​=E(xr​,Xf​)∼(P,Q)​[f1​(C(xr​)−C(xf​))]+E(xr​,Xf​)∼(P,Q)​[f2​(C(xf​)−C(xr​))]=E(xr​,Xf​)∼(P,Q)​[g1​(C(xr​)−C(xf​))]+E(xr​,Xf​)∼(P,Q)​[g2​(C(xf​)−C(xr​))]​ In RGANs, g1g_1g1​ is influenced by fake data , thus by the generator. This means that in most RGANs, the generator is trained to minimize the full loss function envisioned rather than only half of it. Although the relative discriminator provide the missing property that we want in GANs (i.e. GGG influencing D(xr)D(x_r)D(xr​)), its interpretation is different from the standard discriminator. Rather than measuring “the probability that the input data is real”, it is now measuring “the probability that the input data is more realistic than a randomly sampled data of the opposing type. . So we define that P(xr&nbsp;is&nbsp;real)=Exr∼Q[D(xr,xf)] begin{equation} P(x_r text{ is real}) = mathbb{E}_{x_r sim mathbb{Q}}[D(x_r, x_f)] end{equation}P(xr​&nbsp;is&nbsp;real)=Exr​∼Q​[D(xr​,xf​)]​​ P(xf&nbsp;is&nbsp;real)=Exr∼P[D(xf,xr)] begin{equation} P(x_f text{ is real}) = mathbb{E}_{x_r sim mathbb{P}}[D(x_f, x_r)] end{equation}P(xf​&nbsp;is&nbsp;real)=Exr​∼P​[D(xf​,xr​)]​​ where D(xr,xf)=sigmoid(C(xr)−C(xf))D(x_r, x_f) = text{sigmoid}(C(x_r) - C(x_f))D(xr​,xf​)=sigmoid(C(xr​)−C(xf​)) LDRaGAN=Exr∼P[f1(C(xr)−Exf∼QC(xf))]+Exf∼Q[f2(C(xf)−Exr∼PC(xr))]LGRaGAN=Exr∼P[g1(C(xr)−Exf∼QC(xf))]+Exf∼Q[g2(C(xf)−Exr∼PC(xr))] begin{align*} L_D^{RaGAN} &amp; = mathbb{E}_{x_r sim mathbb{P}}[f_1(C(x_r) - mathbb{E}_{x_f sim mathbb{Q}} C(x_f))] &amp;+ mathbb{E}_{x_f sim mathbb{Q}}[f_2(C(x_f) - mathbb{E}_{x_r sim mathbb{P}} C(x_r))] L_G^{RaGAN} &amp; = mathbb{E}_{x_r sim mathbb{P}}[g_1(C(x_r) - mathbb{E}_{x_f sim mathbb{Q}} C(x_f))] &amp; + mathbb{E}_{x_f sim mathbb{Q}}[g_2(C(x_f) - mathbb{E}_{x_r sim mathbb{P}} C(x_r))] end{align*}LDRaGAN​LGRaGAN​​=Exr​∼P​[f1​(C(xr​)−Exf​∼Q​C(xf​))]+Exf​∼Q​[f2​(C(xf​)−Exr​∼P​C(xr​))]=Exr​∼P​[g1​(C(xr​)−Exf​∼Q​C(xf​))]+Exf​∼Q​[g2​(C(xf​)−Exr​∼P​C(xr​))]​ Figure 2 Case studies on some pictures explaining why we should use relative probability in RGAN. Figure 3 Figure 4: Experimental results of different GAN loss functions on CIFAR-10 datasets. Measured with FID scores. . Conclusions: In this paper, the authors proposed the relativistic discriminator as a way to fix and improve on standard GAN. The authors further generalized this approach to any GAN loss and introduced a generally more stable variant called RaD. The results suggests that relativism significantly improve data quality and stability of GANs at no computational cost. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/26/GAN4_blog2.html",
            "relUrl": "/gan/2021/10/26/GAN4_blog2.html",
            "date": " • Oct 26, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "GAN4 Toward a Better Global Loss Landscape of GANs",
            "content": "Generative Adversarial Nets (GANs) (Goodfellow et al., 2016) are a successful method for various practical applications. Meanwhile, current theoretical studys of GANs are digging into the underlying mechanism in the aspects of statistics and optimization. For statistics, we have Goodfellow et al., (2014) link the minmax formulation and the JS (Jenson-Shannon) distance. Also, Wasserstein Generative Adversarial Nets (WGANs) (Arjovsky et al., 2017) adapted the Wasserstein distance as the loss function. The generalization problems of GANs are also investigated to see how applicable the GANs methods are. On the optimization side, the works of cyclic behavior (Balduzzi et al., 2018) research the issues that the optimization algorithm may cycle around a stable point and converges slowly even diverges. Another challenge for optimization is to avoid the sub-optimal local minima. For GANs, current works (Mescheder et al., 2018) either analyze convex-concave games or perform local analysis without considering the global analysis. Even though some works conduct global analysis, it only works on a simple setting without further generalization. Therefore, to fill in the gaps of theoretical analysis on GANs, the main goal of this work is to perform a global analysis on the GANs landscape for general data distribution. In the paper, the work is put in a table to compare with other theoretical works: Therefore, in specific, this work is defined as a global analysis on GANs landscape by comparing the SepGAN and RpGAN. . Relativistic GANs . Before further discussion, let’s first introduce the work Relativistic GANs (Jolicoeur-Martineau, 2018). As we know, the orginal GANs suffer from the problems of instable training and mode collapse so some works take efforts to solving the problems either by imposing regularizations or changing the loss. In this paper, the concepts of relativity are emphersized to suggest that the discriminator requires the relative probability of real images to help training. The above table is comparing how the probability of real images change for standard GAN and relativistic GAN. Given the images of bread as real images, we have three situations: . The real images are bread and the fake images are dogs. Then, the absolute and relative probability of real images being bread is one. | The real images are bread and the fake images are dogs that look similar to bread. Then, the absolute probability of real images being bread is still one and the relative probability decreases. | The real images are bread in the shape of dogs and fake images are dogs. Then the probability of real images being bread is low while the relative probability increases. | In summary, as the discriminator scores the reality of samples, both real data and fake data should be taken into account so the judgement of absolute real or fake is changed to the probability of being real or fake. This point is illustrated by two aspects: . Utilizing the prior knowledge that the input samples consist of half real and half fake images, the discriminator can score real samples low instead of considering all samples real as fake samples are more realistic than real samples. | As the following figure shows, we often encourage the generator to generate more realistic images so pushing the discriminator to give high scores to fake images but the real samples are ignored. Instead, an ideal training should involve decreasing the score of real samples as the fake samples become more and more real. | Therefore, to make the discriminator judge the samples relatively, the loss objective is modified as following: where pairs of real and fake samples are used for computing loss. . Landscape Analysis of GANs . The paper use a simple case to illustrate the phenomena of bad local minima in GANs. Given real samples x1x_1x1​ and x2x_2x2​, we need to generate fake samples y1y_1y1​ and y2y_2y2​ that maximially match the real samples. As the discriminator update, a boundary that classifies real and fake samples is set between the margins of two sample sets. Then, by updating the generator, the generated samples are forced to approach the real samples. As the process goes on and on, the generated samples will be trapped in a cluster around one real sample, which is so called mode collapse. For the relativistic GAN (RpGAN), since we compare samples in each pair instead of sample sets, each generated sample can be pushed to different real sample, therefore, relieving the mode collapse problem in standard GANs. . Bad Local Minima . To further investigate how pairing influence local minima, the paper sets a two-point case: Given two real samples x1x_1x1​ and x2x_2x2​, and two fake samples y1y_1y1​ and y2y_2y2​, we have four states s0,s1a,s1b,s2s_0, s_{1a}, s_{1b}, s_2s0​,s1a​,s1b​,s2​ that represent ∣{x1,x2}∩{y1,y2}∣=0 left| left {x_{1}, x_{2} right } cap left {y_{1}, y_{2} right } right|=0∣{x1​,x2​}∩{y1​,y2​}∣=0, y1=y2∈{x1,x2}y_1=y_2 in left {x_{1}, x_{2} right }y1​=y2​∈{x1​,x2​}, ∣{x1,x2}∩{y1,y2}∣=1 left| left {x_{1}, x_{2} right } cap left {y_{1}, y_{2} right } right|=1∣{x1​,x2​}∩{y1​,y2​}∣=1, {x1,x2}={y1,y2} left {x_{1}, x_{2} right }= left {y_{1}, y_{2} right }{x1​,x2​}={y1​,y2​}. Representing these states by the divergence function we have the following: ϕJS(Y,X)={−log⁡2≈−0.6931&nbsp;if&nbsp;s2,−log⁡2/2≈−0.3467&nbsp;if&nbsp;s1b,14(2log⁡2−3log⁡3)≈−0.4774&nbsp;if&nbsp;s1a,0&nbsp;if&nbsp;s0 phi_{J S}(Y, X)= left { begin{array}{ll} - log 2 approx-0.6931 &amp; text { if } s_2, - log 2 / 2 approx-0.3467 &amp; text { if } s_{1b}, frac{1}{4}(2 log 2-3 log 3) approx-0.4774 &amp; text { if } s_{1a}, 0 &amp; text { if } s_{0} end{array} right. ϕJS​(Y,X)=⎩ . ⎨ . ⎧​−log2≈−0.6931−log2/2≈−0.346741​(2log2−3log3)≈−0.47740​&nbsp;if&nbsp;s2​,&nbsp;if&nbsp;s1b​,&nbsp;if&nbsp;s1a​,&nbsp;if&nbsp;s0​​ where ϕJS phi_{J S}ϕJS​ is the divergence measurement in JS-GAN. This representation can be transformed to an continuous curve: We can see that the landscape of JS-GAN has a local minima at the state s1as_{1a}s1a​ and a global minima at state S2S_2S2​, meaning that as all fake samples are at the cluster of one real sample, they are trapped in the local minima, which corresponds to the previous illustration. Then, for the RpGANRpGANRpGAN, we have: ϕRS(Y,X)={−log⁡2≈−0.6931&nbsp;if&nbsp;s2,−12log⁡2≈−0.3466&nbsp;if&nbsp;s1a,s1b,0&nbsp;if&nbsp;s0 phi_{ mathrm{RS}}(Y, X)= left { begin{array}{ll} - log 2 approx-0.6931 &amp; text { if } s_2, - frac{1}{2} log 2 approx-0.3466 &amp; text { if } s_{1a}, s_{1b}, 0 &amp; text { if } s_0 end{array} right. ϕRS​(Y,X)=⎩ . ⎨ . ⎧​−log2≈−0.6931−21​log2≈−0.34660​&nbsp;if&nbsp;s2​,&nbsp;if&nbsp;s1a​,s1b​,&nbsp;if&nbsp;s0​​ and Therefore, we can see the RS-GAN only has a global minima at s2s_2s2​. . Landscape Results in Function Space . Theorem 1 . Suppose x1,x2,…,xn∈Rdx_{1}, x_{2}, ldots, x_{n} in mathbb{R}^{d}x1​,x2​,…,xn​∈Rd are distinct. Suppose h1,h2h_{1}, h_{2}h1​,h2​ satisfy Assumptions 4.1, 4.24.24.2 and 4.3. Then for separable-GAN loss gSP(Y)g_{ mathrm{SP}}(Y)gSP​(Y) defined in Eq. (5), we have: (i) The global minimal value is −12sup⁡t∈R(h1(t)+h2(−t))- frac{1}{2} sup _{t in mathbb{R}} left(h_{1}(t)+h_{2}(-t) right)−21​supt∈R​(h1​(t)+h2​(−t)), which is achieved iff {y1,…,yn}={x1,…,xn}. left {y_{1}, ldots, y_{n} right }= left {x_{1}, ldots, x_{n} right } .{y1​,…,yn​}={x1​,…,xn​}. (ii) If yi∈{x1,…,xn},i∈{1,2,…,n}y_{i} in left {x_{1}, ldots, x_{n} right }, i in {1,2, ldots, n }yi​∈{x1​,…,xn​},i∈{1,2,…,n} and yi=yjy_{i}=y_{j}yi​=yj​ for some i≠ji neq ji=j, then YYY is a sub-optimal strict local minimum. Therefore, gSP(Y)g_{ mathrm{SP}}(Y)gSP​(Y) has (nn−n!) left(n^{n}-n ! right)(nn−n!) sub-optimal strict local minima. . This theorem generalizes the results in the case of n=2n=2n=2 to any n and is trying to show two results: first, for standard GAN, global minimal achieves when two sets of points perfectly match; second, local minima always exists as the assumptions and the second restriction in the theorem are satisfied. Since the second part of the theorem is easily to satisfy, the local minima cannot be avoided. . Definition (global-min-reachable) . We say a point www is global-min-reachable for a function F(w)F(w)F(w) if there exists a continuous path from www to one global minimum of FFF along which the value of F(w)F(w)F(w) is non-increasing. . The definition defines any points on an non-increasing curve as global-min-reachable. Then we have the theorem 2. . Theorem 2 . Suppose x1,x2,…,xn∈Rdx_{1}, x_{2}, ldots, x_{n} in mathbb{R}^{d}x1​,x2​,…,xn​∈Rd are distinct. Suppose h satisfies Assumptions 4.44.44.4 and 4.5.4.5 .4.5. Then for RpGAN loss gRg_{ mathrm{R}}gR​ defined in Eq. (6): (i) The global minimal value is h(0)h(0)h(0), which is achieved iff {y1,…,yn}={x1,…,xn}. left {y_{1}, ldots, y_{n} right }= left {x_{1}, ldots, x_{n} right } .{y1​,…,yn​}={x1​,…,xn​}. (ii) Any YYY is global-min-reachable for the function gR(Y).g_{ mathrm{R}}(Y) .gR​(Y). . The main point of this theorm is that any points for the function is on an non-increaseing curve, which means there is no local minima for the loss. . Hence, these two theorems generalize the conclusion of two-point case to all other cases to show the global landscape, which is saying that for standard GANs, local sub-optimal and it’s consequence like mode collapse is hard to avoid, while the RpGANs have none local minimal, which, therefore, avoids the mode collapse and other issues. . Results . This figure is showing how the distribution of generated samples move from the initial state where fake samples are trapped in one real samples cluster. Given that red points are generated samples and blue points are real samples, It’s obvious that for RS-GAN, the generated samples move faster away from the trap of real sample cluster or equally, the local minimal. This results firmly support the theoretical analysis. The above figure is showing how the discriminator loss changes as the training progresses. Compared to the loss of JS-GAN, the loss of RS-GAN converges much faster. Another observation is that the loss of JS-GAN stuck at 0.48 for a while, which is close to the value of ϕJS phi_{JS}ϕJS​ at state s1as_{1a}s1a​ so, again, it support the previous analysis. . Reference . I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NeurIPS, 2014. . M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. In ICML, 2017. . D. Balduzzi, S. Racaniere, J. Martens, J. Foerster, K. Tuyls, and T. Graepel. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018. . L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for gans do actually converge? In ICML, 2018. . A. Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. In ICLR, 2018. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/26/GAN4_blog.html",
            "relUrl": "/gan/2021/10/26/GAN4_blog.html",
            "date": " • Oct 26, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "GAN3 Wasserstein GANs",
            "content": "Introduction . If we view GAN training as a distribution matching process, that is, to train a generated distribution PgP_gPg​ to match the ground truth data distribution PrP_rPr​, then selecting an appropriate distance metric d(Pg,Pr)d(P_g, P_r)d(Pg​,Pr​) becomes the core problem. . The original design of GAN (henceforth called vanilla GAN)1 uses an approimation of the Jensen-Shannon divergence (JSD), while other alternatives include: . The Total Variation (TV) distance | The Kullback-Leibler divergence (KLD) | The Earth-Mover’s distance (EMD), also called Wasserstein-1 | . In this blogpost, we will investigate those different distances and look into Wasserstein GAN (WGAN)2, which uses EMD to replace the vanilla discriminator criterion. After that, we will explore WGAN-GP3, an improved version of WGAN with larger mode capacity and more stable training dynamics. . We shall see that WGAN and WGAN-GP have several advantages over vanilla GAN - reduced mode collapse, less gradient vanishing and more interpretable discriminator losses. . What Is Earth Mover’s Distance? . Firstly we need to formalize each of the distance metrics: . TV distance is calculated as δ(Pr,Pg)=sup⁡A∈Σ∣Pr(A)−Pg(A)∣ delta(P_r, P_g) = sup_{A in Sigma} mid P_r(A) - P_g(A) midδ(Pr​,Pg​)=supA∈Σ​∣Pr​(A)−Pg​(A)∣. . KLD is defined as KL(Pr∥Pg)=∫log⁡(Pr(x)Pg(x))Pr(x)dμ(x)KL(P_r parallel P_g) = int log( frac{P_r(x)}{P_g(x)})P_r(x)d mu(x)KL(Pr​∥Pg​)=∫log(Pg​(x)Pr​(x)​)Pr​(x)dμ(x), with μ muμ a measure function. . JSD is defined as JS(Pr,Pg)=KL(Pr∥Pm)+KL(Pg∥Pm)JS(P_r, P_g) = KL(P_r parallel P_m) + KL(P_g parallel P_m)JS(Pr​,Pg​)=KL(Pr​∥Pm​)+KL(Pg​∥Pm​), where Pm=(Pr+Pg)/2P_m = (P_r + P_g) / 2Pm​=(Pr​+Pg​)/2. . EMD is defined as W(Pr,Pg)=inf⁡γ∈Π(Pr,Pg)E(x,y)∈γ∥x−y∥W(P_r, P_g) = inf_{ gamma in Pi(P_r, P_g)} mathbb{E}_{(x,y) in gamma} parallel x - y parallelW(Pr​,Pg​)=infγ∈Π(Pr​,Pg​)​E(x,y)∈γ​∥x−y∥, where Π(Pr,Pg) Pi(P_r, P_g)Π(Pr​,Pg​) is the set of all joint distributions with marginals PrP_rPr​ and PgP_gPg​. . A simple way to understand EMD is to imagine two piles of earth in the shapes of PrP_rPr​ and PgP_gPg​, and EMD is calculated as the minimal effort it takes to transform one pile to the other. In other words, we need to pick a matching γ gammaγ between PrP_rPr​ and PgP_gPg​ out of the set of all matchings Π(Pr,Pg) Pi(P_r, P_g)Π(Pr​,Pg​), and we want γ gammaγ to minimize the effort to transform from one distribution to the other. . Why Earth Mover’s Distance? . To get a sense of the difference between those distances, here is a simple example. . Example. We want to learn a point x=0x = 0x=0 on the real line. Then if we define PθP_ thetaPθ​ to be the Dirac delta distribution centered at x=θx = thetax=θ, we have the following deductions: . . Note that all metrics except for EMD are discontinuous, and they do not converge to 0 as PθP_ thetaPθ​ approaches P0P_0P0​. The following figure is a visualization of EMD (left) and JSD (right). . . More formally, the WGAN paper proves that: . With mild assumptions on ggg, W(Pr,Pg)W(P_r, P_g)W(Pr​,Pg​) is continuous everywhere, and differentiable almost everywhere | This is not true for all other metrics | . The following figure shows that when PgP_gPg​ and PrP_rPr​ are far apart (or equivalently, when the discriminator is sufficiently well-trained such that it correctly classifies true and fake images), the discriminator outputs are concentrated around 0 and 1 for vanilla GAN, while they fall on a linear function for WGAN. . As a result, the critic’s gradients are almost zero for both true and fake inputs in vanilla GAN, thus hindering further training, while in WGAN such vanishing gradients are avoided. . . What’s more, if we consider a sequence of distributions (Pn)n∈N(P_n)_{n in mathbb{N}}(Pn​)n∈N​ and rank different distance metrics by their convergence conditions, from strong to weak, we get: . KLD | TV and JSD | EMD | For example, we have δ(Pn,P)→0⇒W(Pn,P)→0 delta(P_n, P) to 0 Rightarrow W(P_n, P) to 0δ(Pn​,P)→0⇒W(Pn​,P)→0. This means that it’s easiest to train a model on EMD. . Moreover, the paper proves that W(Pn,P)→0⇔Pn→PW(P_n, P) to 0 Leftrightarrow P_n to PW(Pn​,P)→0⇔Pn​→P, that is, even though EMD has the weakest convergence condition, it can still guarantee that our generated distribution converges to the ground truth PrP_rPr​. . Therefore it is desirable for us to use EMD as the distribution matching criterion. . Wasserstein GAN . Model Design . The original EMD formula is highly intractable. Fortunately, we can transform it to its Kantorovich-Rubinstein duality form: . W(Pg,Pr)=sup⁡∥f∥L≤1Ex∈Pr[f(x)]−Ex∈Pg[f(x)]W(P_g, P_r) = sup_{ parallel f parallel_L le 1} mathbb{E}_{x in P_r} [f(x)] - mathbb{E}_{x in P_g} [f(x)]W(Pg​,Pr​)=sup∥f∥L​≤1​Ex∈Pr​​[f(x)]−Ex∈Pg​​[f(x)]. . Here fff is a function that maps a data sample to a scalar, and ∥f∥L≤1 parallel f parallel_L le 1∥f∥L​≤1 basically means that fff is restricted to be 1-Lipschitz. . This formula naturally blends into the objective function of GAN, and we can straightforwardly write out the objective of WGAN as: . min⁡θmax⁡ωEx∈Pr[Dω(x)]−Ez∈Pz[Dω(Gθ(x))] min_ theta max_ omega mathbb{E}_{x in P_r} [D_ omega(x)] - mathbb{E}_{z in P_z} [D_ omega(G_ theta(x))]minθ​maxω​Ex∈Pr​​[Dω​(x)]−Ez∈Pz​​[Dω​(Gθ​(x))]. . The above expression means that we train the discriminator to approximate EMD as accurate as possible, and the generator to minimize the calculated distance. . However, we still need to ensure that DωD_ omegaDω​ is KKK-Lipschitz with a finite KKK. To this end, the WGAN paper proposes to use weight clipping. Such primitive method will be further improved in WGAN-GP. . Experimental Results . The paper trains WGAN and vanilla GAN on the LSUN-bedroom dataset with two different architectures: a smaller-scale MLP and a larger-scale DCGAN. The following figure shows that WGAN is able to converge with both architectures, and the discriminator criterion decreases smoothly as the image quality gets better. . . On the contrary, vanilla GAN fails to converge with the MLP architecture, and the discriminator loss does not directly reflect sample quality. . . We can conclude from those results that WGAN is more stable, requires less model capacity and provides us with a more interpretable disriminator loss. . WGAN-GP . Weight Clipping: What’s Wrong? . The biggest bottleneck of WGAN is that it uses weight clipping to enforce Lipschitz condition on the discriminator. To show that this design causes trouble, the authors of WGAN-GP make the following proposition: . Proposition. The optimal solution to the Kantorovich-Rubinstein duality form of EMD, denoted as f∗f^ astf∗, has gradient norm 1 almost everywhere under PrP_rPr​ and PgP_gPg​. Furthermore, with any pairing π(x,y) pi(x, y)π(x,y) with π(x=y)=0 pi(x = y) = 0π(x=y)=0, it holds that P(x,y)∼π[∇f∗(xt)=y−xt∥y−xt∥]=1P_{(x, y) sim pi}[ nabla f^ ast(x_t) = frac{y - x_t}{ parallel y - x_t parallel}] = 1P(x,y)∼π​[∇f∗(xt​)=∥y−xt​∥y−xt​​]=1, where xt=tx+(1−t)y,t∈[0,1]x_t = tx + (1 - t)y, t in [0, 1]xt​=tx+(1−t)y,t∈[0,1]. . Therefore, by inspecting whether the discriminator DDD has unit gradient norm, we can know how well WGAN approximates the EMD. . Astonishingly, with different clipping thresholds, WGAN consistently fails to converges to a discriminator with unit gradient norm. Furthermore, the weights of discriminator concentrate around the clipping threshold, largely impairing the discriminator capacity. . . Another observation is made by training WGAN on several toy datasets. It is discovered that WGAN discriminators end up learning very simple functions: . . Gradient Penalty . A smarter way to encourage (instead of enforce) Lipschitz condition on DDD is to regularize it to have unit gradient norm. The additional loss term, called gradient penalty (GP), has the following form: . Ex^∈Px^[(∥∇x^D(x^)∥2−1)2] mathbb{E}_{ hat{x} in P_{ hat{x}}} [( parallel nabla_{ hat{x}} D( hat{x}) parallel_2 - 1)^2]Ex^∈Px^​​[(∥∇x^​D(x^)∥2​−1)2], where x^=tx+(1−t)G(z),x∼Pr,z∼Pz hat{x} = tx + (1 - t)G(z), x sim P_r, z sim P_zx^=tx+(1−t)G(z),x∼Pr​,z∼Pz​. That is, x^ hat{x}x^ is obtained from interpolating real and fake samples. . The GP term is then added to the overall loss function with a weight λ=10 lambda = 10λ=10. . Experimental Results . The paper conducts a large-scale comparison between WGAN-GP and vanilla DCGAN by training them with 200 different architectures. Using Inception Score (IS) as the metric, the following table show that WGAN-GP is consistently better than DCGAN: . . Also by evaluating IS, it is shown that WGAN-GP performs better than WGAN on CIFAR-10: . . Finally, WGAN-GP beats other SOTA GAN-based models under unsupervised setting, and is comparable to the best model under supervised setting: . . Below are some samples (trained on LSUN-bedroom) obtained from the WGAN-GP generator. . . References . . See Generative Adversarial Networks. ↩︎ . | See Wasserstein GAN. ↩︎ . | See Improved Training of Wasserstein GANs. ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/21/GAN3_blog2.html",
            "relUrl": "/gan/2021/10/21/GAN3_blog2.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "GAN3 TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS",
            "content": "Introduction . Background . Generative Adversarial Networks (GANs) have achieved great success at generating realistic and sharp looking images. However, they still remain remarkably diffcult to train and most papers at that time dedicated to heuristically finding stable architecture. And there is little to no theory explaining the unstable behaviour of GAN training. . GAN Formulation . GAN in its original formulation, plays the following game: min⁡Gmax⁡DV(D,G)=Ex∼pdata(x)[log⁡D(x)]+Ez∼pz(z)[log⁡(1−D(G(z)))] min_G max_D V(D,G) = mathbb E_{x sim p_{data}(x)}[ log D(x)]+ mathbb E_{z sim p_z(z)}[ log(1-D(G(z)))]minG​maxD​V(D,G)=Ex∼pdata​(x)​[logD(x)]+Ez∼pz​(z)​[log(1−D(G(z)))] It can be reformulated as: C(G)=max⁡DV(G,D)C(G) = max_D V(G,D)C(G)=maxD​V(G,D) $ = mathbb E_{x sim p_{data}}[ log D^G(x)]+ mathbb E{z sim p_z}[ log(1-D^G(G(z)))]$ $ = mathbb E{x sim p_{data}}[ log D^G(x)]+ mathbb E{x sim p_g}[ log(1-D^G(x))]$ $ = mathbb E{x sim p_{data}}[ log frac{p_{data}(x)}{p_{data}(x)+p_g(x)}]+ mathbb E_{x sim p_g}[ log frac{p_g(x)}{p_{data}(x)+p_g(x)}]$ and this is called the virtual training criterion. . It can be shown that C(G) is equivalent to the following: −log⁡(4)+2⋅JSD(pdata∣∣pg)- log(4)+2 cdot JSD(p_{data}||p_g)−log(4)+2⋅JSD(pdata​∣∣pg​) where JSD is the Jensen-Shannon divergence: JSD(Pr∣∣Pg)=12KL(Pr∣∣PA)+12KL(Pg∣∣PA)JSD( mathbb P_r|| mathbb P_g)= frac{1}{2}KL( mathbb P_r|| mathbb P_A)+ frac{1}{2}KL( mathbb P_g|| mathbb P_A)JSD(Pr​∣∣Pg​)=21​KL(Pr​∣∣PA​)+21​KL(Pg​∣∣PA​) where PA mathbb P_APA​ is the ‘average’ distribution, with density Pr+Pg2 frac{P_r+P_g}{2}2Pr​+Pg​​. . To train a GAN, we first train the discriminator to optimal, which is maximizing: L(D,gθ)=Ex∼Pr[log⁡D(x)]+Ex∼Pg[log⁡(1−D(x))]L(D,g_ theta) = mathbb E_{x sim mathbb P_r}[ log D(x)]+ mathbb E_{x sim mathbb P_g}[ log(1-D(x))]L(D,gθ​)=Ex∼Pr​​[logD(x)]+Ex∼Pg​​[log(1−D(x))] and the optimal discriminator is obatined as: D∗(x)=Pr(x)Pr(x)+Pg(x)D^*(x) = frac{P_r(x)}{P_r(x)+P_g(x)}D∗(x)=Pr​(x)+Pg​(x)Pr​(x)​ . Plug in the optimal discriminator to L(D,gθ)L(D,g_ theta)L(D,gθ​), we get: L(D∗,gθ)=2JSD(Pr∣∣Pg)−2log⁡2L(D^*, g_ theta) = 2JSD( mathbb P_r|| mathbb P_g)-2 log2L(D∗,gθ​)=2JSD(Pr​∣∣Pg​)−2log2 So, minimizing L(D,gθ)L(D,g_ theta)L(D,gθ​) as a function of θ thetaθ gives the minimized Jensen-Shannon divergence when the discriminator is optimal. . In theory, we would first train discriminator as close to optimal as we can, then do gradient steps on θ thetaθ, and alternating these two things to get our generator. color{red} But this doesn’t work. In practice, as the discriminator gets better, the updates to the generator get consistently worse . Main Contribution . Based on the problem regarding GAN training, the authors proposed 4 questions to be answered: . Why do updates get worse as the discriminator gets better? Both in the original and the new cost function. | Why is GAN training massively unstable? | Is the new cost function following a similar divergence to the JSD? If so, what are its properties? | Is there a way to avoid some of these issues? | We first take a look at the source of instabilities . Sources of Instability . Discontinuity Conjecture . From the cost function and the optimal discriminator, we ‘know’ that the trained discriminator will have cost of at most 2log⁡2−2JSD(Pr∣∣Pg)2 log2-2JSD( mathbb P_r|| mathbb P_g)2log2−2JSD(Pr​∣∣Pg​). However, in practice, if we train D till convergence, its error will go to 0, pointing to the fact that the JSD between them is maxed out. The only way this can happen is if the distributions are not continuous, or they have disjoint supports. And one possible cause for the distribution to be discontinuous is if their supports lie on low dimensional manifolds. Previous work on GAN showed strong empirical and theoretical evidence to believe that Pr mathbb P_rPr​ is indeed extremely concentrated on a low dimensional manifold. And the authors proved that this is the case as well for Pg mathbb P_gPg​. . Below regards DCGAN trained for 1,10,25 epochs. Then, with generator fixed, train a discriminator from scratch. . Lemma 1 . Let g:Z→Xg: mathcal Z rightarrow mathcal Xg:Z→X be a function composed by affine transformations and pointwise non-linearities, which can either be rectifiers, leaky rectifiers, or smooth strictly increasing functions (such as the sigmoid, tanh, softplus, etc). Then, g(Z)g( mathcal Z)g(Z) is composed in a countable union of manifolds of dimension at most dim Z mathcal ZZ. Therefore, if the dimension of Z mathcal ZZ is less than the one of X mathcal XX, g(Z)g( mathcal Z)g(Z) will be a set of measure 0 in X mathcal XX. . The intuition behind this lemma is Pg mathbb P_gPg​ is defined via sampling from a simple prior z∼p(z)z sim p(z)z∼p(z), and then applying a function g:Z→Xg: mathcal Z rightarrow mathcal Xg:Z→X, so the support of Pg mathbb P_gPg​ has to be contained in g(Z)g( mathcal Z)g(Z). If the dimensionality of Z mathcal ZZ is less than the dimension of X mathcal XX, then it’s impossible for Pg mathbb P_gPg​ to be continuous. . Perfect Discrimination Theorem . Theorem 2.1 . If two distributions Pr mathbb P_rPr​ and Pg mathbb P_gPg​ have support contained on two disjoint compact subsets M mathcal MM and P mathcal PP respectively, then there is a smooth optimal discriminator D∗:X→[0,1]D^*: mathcal X rightarrow [0,1]D∗:X→[0,1] that has accuracy 1 and ∇xD∗(x)=0∀x∈M∪P nabla_xD^*(x)=0 forall x in mathcal M cup mathcal P∇x​D∗(x)=0∀x∈M∪P. . Definition (Transversal Intersection) . Let M mathcal MM and P mathcal PP be two boundary free regular submanifolds of F mathcal FF, which in our case will simply be F=Rd mathcal F = mathbb R^dF=Rd. Let x∈M∩Px in mathcal M cap mathcal Px∈M∩P be an intersection point of the two manifolds. We say that M mathcal MM and P mathcal PP intersect transversally in xxx if TxM+TxP=TxF mathcal T_x mathcal M+ mathcal T_x mathcal P = mathcal T_x mathcal FTx​M+Tx​P=Tx​F, where TxM mathcal T_x mathcal MTx​M means the tangent space of M mathcal MM around xxx. . Definition (Perfect Alignment) . We say that two manifolds without boundary M mathcal MM and P mathcal PP textbf{perfectly align} if there exists x∈M∩Px in mathcal M cap mathcal Px∈M∩P such that M mathcal MM and P mathcal PP don’t intersect transversally in xxx. And let ∂M partial mathcal M∂M and ∂P partial mathcal P∂P denote the boundary of manifolds M mathcal MM and P mathcal PP, we say M mathcal MM and P mathcal PP are perfectly align if any of the boundary free manifold pairs (M mathcal MM, P mathcal PP), (M mathcal MM, ∂P partial mathcal P∂P), (∂M partial mathcal M∂M, P mathcal PP),(∂M partial mathcal M∂M, ∂P partial mathcal P∂P) perfectly align. . Lemma 2 . Let M mathcal MM and P mathcal PP be two regular submanifolds of Rd mathbb R^dRd that don’t have full dimension. Let η,η′ eta, eta&#39;η,η′ be arbitrary independent continuous random variables. We therefore define the perturbed manifolds as M~=M+η mathcal{ tilde M}= mathcal M+ etaM~=M+η and P~=P+η′ tilde{ mathcal P}= mathcal{P}+ eta&#39;P~=P+η′. Then [ mathbb P_{ eta, eta’}( mathcal{ tilde M} text{ does not perfect align with } mathcal{ tilde P}) = 1] . This implies that, in practice, we can safely assume that any two manifolds never perfectly assign, since arbitrarilly small perturbation on two manifolds will lead them to intersect transversally or don’t intersect at all. . Lemma 3 . Let M mathcal MM and P mathcal PP be two regular submanifolds of Rd mathbb R^dRd that don’t perfectly align and don’t have full dimension. Let L=M∩P mathcal L= mathcal M cap mathcal PL=M∩P. If M mathcal MM and P mathcal PP don’t have boundary, then L mathcal LL is also a manifold, and has strictly lower dimension than both the one of M mathcal MM and the one of P mathcal PP. If they have boundary, L mathcal LL is a union of at most 4 strictly lower dimensional manifolds. In both cases, L mathcal LL has measure 0 in both M mathcal MM and P mathcal PP. . If two manifolds don’t perfectly align, their intersection L=M∩P mathcal L= mathcal M cap mathcal PL=M∩P will be a finite union of manifolds with dimensions strictly lower than both the dimension of M mathcal MM and P mathcal PP. . Theorem 2.2 (The Perfect Discrimination Theorem) . Let Pr mathbb P_rPr​ and Pg mathbb P_gPg​ be two distributions that have support contained in two closed manifolds M mathcal MM and P mathcal PP that don’t perfectly align and don’t have full dimension. We further assume that Pr mathbb P_rPr​ and Pg mathbb P_gPg​ are continuous in their respective manifolds, meaning that if there is a set A with measure 0 in M mathcal MM, then Pr(A)=0 mathbb P_r(A)=0Pr​(A)=0 (and analogously for Pg mathbb P_gPg​). color{red} Then, there exists an optimal discriminator D∗:X→[0,1]D^*: mathcal X rightarrow[0,1]D∗:X→[0,1] that has accuracy 1 and for almost any x in M mathcal MM or P mathcal PP, D∗D^*D∗ is smooth in a neighbourhood of xxx and ∇xD∗(x)=0 nabla_xD^*(x)=0∇x​D∗(x)=0. . Combining the two theorems (2.1 and 2.2) stated together tells us that there are perfect discriminators which are smooth and constant almost everywhere in M mathcal MM and P mathcal PP. And the fact that the discriminator is constant in both manifolds points to the fact that we won’t be able to learn anything by backproping through it. . Theorem 2.3 . Let Pr mathbb P_rPr​ and Pg mathbb P_gPg​ be two distributions whose support lies in two manifolds M mathcal MM and P mathcal PP that don’t have full dimension and don’t perfectly align. We further assume that Pr mathbb P_rPr​ and Pg mathbb P_gPg​ are continuous in their respective manifolds. Then, JSD(Pr∣∣Pg)=log⁡2JSD( mathbb P_r|| mathbb P_g)= log2JSD(Pr​∣∣Pg​)=log2 KL(Pr∣∣Pg)=+∞KL( mathbb P_r|| mathbb P_g)=+ inftyKL(Pr​∣∣Pg​)=+∞ KL(Pg∣∣Pr)=+∞KL( mathbb P_g|| mathbb P_r)=+ inftyKL(Pg​∣∣Pr​)=+∞ . This implies: . divergences will be maxed out even if the two manifolds lie arbitrarilly close to each other (impossible to apply gradient descent) | samples of our generator might look impressively good, yet both KL divergences will be infinity | attempting to use divergences out of the box to test similarities between the distributions we typically consider might be a terrible idea | Vanishing Gradients on the Generator . Denote ∣∣D∣∣=sup⁡x∈X∣D(x)∣+∣∣∇xD(x)∣∣2||D||= sup_{x in mathcal X}|D(x)|+|| nabla_xD(x)||_2∣∣D∣∣=supx∈X​∣D(x)∣+∣∣∇x​D(x)∣∣2​ (Note: The authors stated that this norm is used to make proofs simpler, and can be replaced with Sobolev norm ∣∣⋅∣∣1,p|| cdot||_{1,p}∣∣⋅∣∣1,p​ for p&lt;∞p&lt; inftyp&lt;∞ covered by the universal approximation theorem in the sense that we can guarantee a neural network approximation in this norm) . Theorem 2.4 . Let gθ:Z→Xg_ theta: mathcal Z rightarrow mathcal Xgθ​:Z→X be a differentiable function that induces a distribution Pg mathbb P_gPg​. Let Pr mathbb P_rPr​ be the real data distribution. Let D be a differentiable discriminator. If the conditions of Theorem 2.1 and 2.2 are satisfied, ∣∣D−D∗∣∣&lt;ϵ||D-D^*||&lt; epsilon∣∣D−D∗∣∣&lt;ϵ, and Ez∼p(z)[∣∣Jθgθ(z)∣∣22]≤M2 mathbb E_{z sim p(z)}[||J_ theta g_ theta(z)||^2_2] le M^2Ez∼p(z)​[∣∣Jθ​gθ​(z)∣∣22​]≤M2, then ∣∣∇θEz∼p(z)[log⁡(1−D(gθ(z)))]∣∣2&lt;Mϵ1−ϵ|| nabla_ theta mathbb E_{z sim p(z)}[ log(1-D(g_ theta(z)))]||_2&lt;M frac{ epsilon}{1- epsilon}∣∣∇θ​Ez∼p(z)​[log(1−D(gθ​(z)))]∣∣2​&lt;M1−ϵϵ​ . Theorem 2.4 shows that as our discriminator gets better, the gradient of the generator vanishes. And the figure below shows an experimental verfication of the above statement. First train a DCGAN for 1, 10 and 25 epochs. Then, with the generator fixed train a discriminator from scratch and measure gradients with the original cost function. . The −log⁡D- log D−logD Alternative . In original GAN paper, the authors conjectured that the instable behaviour is because of the choose of loss function so they proposed an alternative to “fix” it, but even with the alternative loss function, the problem remains, and yet little is known about the nature of the alternative loss. . To avoid vanishing gradient when the discrminator is very confident, an alternative gradient step for the generator is used: Δθ=∇θEz∼p(z)[−log⁡D(gθ(z))] Delta theta= nabla_ theta mathbb E_{z sim p(z)}[- log D(g_ theta(z))]Δθ=∇θ​Ez∼p(z)​[−logD(gθ​(z))] . Theorem 2.5 . Let Pr mathbb P_rPr​ and Pgθ mathbb P_{g_ theta}Pgθ​​ be two continuous distributions, with densities Pr,PgθP_r,P_{g_ theta}Pr​,Pgθ​​ respectively. Let D∗(x)=Pr(x)Pr(x)+Pgθ(x)D^*(x) = frac{P_r(x)}{P_r(x)+P_{g_ theta}(x)}D∗(x)=Pr​(x)+Pgθ​​(x)Pr​(x)​ be the optimal discriminator, fixed for a value θ0 theta_0θ0​. Therefore, Ez∼p(z)[−∇θlog⁡D∗(gθ(z))∣θ=θ0]=∇θ[KL(Pgθ∣∣Pr)−2JSD(Pgθ∣∣Pr)]∣θ=θ0 mathbb E_{z sim p(z)}[- nabla_ theta log D^*(g_ theta(z))|_{ theta= theta_0}] = nabla_ theta[KL( mathbb P_{g_ theta}|| mathbb P_r)-2JSD( mathbb P_{g_ theta}|| mathbb P_r)]|_{ theta= theta_0}Ez∼p(z)​[−∇θ​logD∗(gθ​(z))∣θ=θ0​​]=∇θ​[KL(Pgθ​​∣∣Pr​)−2JSD(Pgθ​​∣∣Pr​)]∣θ=θ0​​ . Theorem 2.6 . Let gθ:Z→Xg_ theta: mathcal Z rightarrow mathcal Xgθ​:Z→X be a differentiable function that induces a distribution Pg mathbb P_gPg​. Let Pr mathbb P_rPr​ be the real data distribution, with either conditions of Theorems 2.1 and 2.2 satisfied. Let D be a discriminator such that D∗−D=ϵD^*-D= epsilonD∗−D=ϵ is a centered Gaussian process indexed by xxx and independent for every xxx and ∇xD∗−∇xD=r nabla_xD^*- nabla_xD=r∇x​D∗−∇x​D=r another independent centered Gaussian process indexed by xxx and independent of every xxx. Then, each coordinate of Ez∼p(z)[−∇θlog⁡D(gθ(z))] mathbb E_{z sim p(z)}[- nabla_ theta log D(g_ theta(z))]Ez∼p(z)​[−∇θ​logD(gθ​(z))] is centered Cauchy distribution with infinite expectation and variance. . Theorem 2.6 implies that we would have large variance in gradients, and the instable update would actually lower sample quality. Moreover, the fact that the distribution updates are centered means that, if we bound the updates, the expected update would be 0, providing no feedback to the gradient. . Towards Softer Metrics and Distributions . Break Assumptions . To fix the Instability and vanishing gradients issue, we need to break the assumptions of previous theorems. And the authors choose to add continuous noise to the inputs of the discriminator, therefore, smoothening the distribution of the probability mass. . Theorem 3.1 . If X has distribution PX mathbb {P_X}PX​ with support on M mathcal MM and ϵ epsilonϵ is an absolutely continuous random variable with density PϵP_ epsilonPϵ​, then PX+ϵ mathbb P_{X+ epsilon}PX+ϵ​ is absolutely continuous with density PX+ϵ=Ey∼PX[Pϵ(x−y)]P_{X+ epsilon}= mathbb E_{y sim mathbb P_X}[P_ epsilon(x-y)]PX+ϵ​=Ey∼PX​​[Pϵ​(x−y)] $ = int_{ mathcal M}P_ epsilon(x-y)d mathbb P_X(y)$ . Theorem 3.2 . Let Pr mathbb P_rPr​ and Pg mathbb P_gPg​ be two distributions with support on M mathcal MM and P mathcal PP respectively, with ϵ∼N(0,σ2I) epsilon sim mathcal N(0, sigma^2I)ϵ∼N(0,σ2I). Then, the gradient passed to the generator has the form Ez∼p(z)[∇θlog⁡(1−D∗(gθ(z)))] mathbb E_{z sim p(z)}[ nabla_ theta log(1-D^*(g_ theta(z)))]Ez∼p(z)​[∇θ​log(1−D∗(gθ​(z)))] =Ez∼p(z)[a(z)∫MPϵ(gθ(z)−y)∇θ∣∣gθ(z)−y∣∣2dPr(y)= mathbb E_{z sim p(z)}[a(z) int_{ mathcal M}P_ epsilon(g_ theta(z)-y) nabla_ theta||g_ theta(z)-y||^2d mathbb P_r(y)=Ez∼p(z)​[a(z)∫M​Pϵ​(gθ​(z)−y)∇θ​∣∣gθ​(z)−y∣∣2dPr​(y) −b(z)∫PPϵ(gθ(z)−y)∇θ∣∣gθ(z)−y∣∣2dPg(y)]-b(z) int_{ mathcal P}P_ epsilon(g_ theta(z)-y) nabla_ theta||g_ theta(z)-y||^2d mathbb P_g(y)]−b(z)∫P​Pϵ​(gθ​(z)−y)∇θ​∣∣gθ​(z)−y∣∣2dPg​(y)] This theorem proves that we will drive our samples gθ(z)g_ theta(z)gθ​(z) towards points along the data manifold, weighted by their probability and the distance from our samples. . To protect the discriminator from measure 0 adversarial examples, it is important to backprop through noisy samples in the generator as well. . Corollary . Let ϵ,epsilon′∼N(0,σ2I) epsilon,epsilon&#39; sim mathcal N(0, sigma^2I)ϵ,epsilon′∼N(0,σ2I) and g~θ(z)=gθ(z)+ϵ′ tilde g_ theta(z) = g_ theta(z) + epsilon&#39;g~​θ​(z)=gθ​(z)+ϵ′, then Ez∼p(z)[∇θlog⁡(1−D∗(g~θ(z)))] mathbb E_{z sim p(z)}[ nabla_ theta log(1-D^*( tilde g_ theta(z)))]Ez∼p(z)​[∇θ​log(1−D∗(g~​θ​(z)))] $ = mathbb E_{z sim p(z)}[a(z) int_{ mathcal M}P_ epsilon( tilde g_ theta(z)-y) nabla_ theta|| tilde g_ theta(z)-y||^2d mathbb P_r(y)-b(z) int_{ mathcal P}P_ epsilon( tilde g_ theta(z)-y) nabla_ theta|| tilde g_ theta(z)-y||^2d mathbb P_g(y)]$ =2∇θJSD(Pr+ϵ∣∣Pg+ϵ)= 2 nabla_ theta JSD( mathbb P_{r+ epsilon}|| mathbb P_{g+ epsilon})=2∇θ​JSD(Pr+ϵ​∣∣Pg+ϵ​) . Wasserstein Metric . The other way to go is to use Wasserstein Metric to replace traditional JSD distance. . Definition . [W(P,Q)= inf_{ gamma in Gamma} int_{ mathcal X times mathcal X}||x-y||_2d gamma(x,y)] where Γ GammaΓ is the set of all possible joints on X×X mathcal X times mathcal XX×X that have marginals P and Q. . Wasserstein Distance is also called Earth Mover’s Distance (EMD), it’s the minimum cost of transporting the whole probability mass of P from its support to match the probability mass of Q on Q’s support. . Intuitively, given two distributions, one can be seen as a mass of earth properly spread in space, the other as a collection of holes in that same space. Then, the EMD measures the least amount of work needed to fill the holes with earth. Here, a unit of work corresponds to transporting a unit of earth by a unit of ground distance. . Lemma 4 . Let ϵ epsilonϵ be a random vector with mean 0, then we have W(PX,PX+ϵ)≤V12W( mathbb P_X, mathbb P_{X+ epsilon}) le V^{ frac{1}{2}}W(PX​,PX+ϵ​)≤V21​ where V=E[∣∣ϵ∣∣22]V= mathbb E[|| epsilon||^2_2]V=E[∣∣ϵ∣∣22​] is the variance of ϵ epsilonϵ. . Theorem 3.3 . Let Pr mathbb P_rPr​ and Pg mathbb P_gPg​ be any two distributions, and ϵ epsilonϵ be a random vector with mean 0 and variance V. If Pr+ϵ mathbb P_{r+ epsilon}Pr+ϵ​ and Pg+ϵ mathbb P_{g+ epsilon}Pg+ϵ​ have support contained on a ball of diameter C, then W(Pr,Pg)≤2V12+2CJSD(Pr+ϵ∣∣Pg+ϵ)W( mathbb P_r, mathbb P_g) le 2V^{ frac{1}{2}}+2C sqrt{JSD( mathbb P_{r+ epsilon}|| mathbb P_{g+ epsilon})}W(Pr​,Pg​)≤2V21​+2CJSD(Pr+ϵ​∣∣Pg+ϵ​) . ​ . This theorem implies that Wasserstein Distance can be controlled since both parts of this upper bound can be manually controlled. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/21/GAN3_blog.html",
            "relUrl": "/gan/2021/10/21/GAN3_blog.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "GAN2 Cycle-GAN",
            "content": "Generative Adversarial Network (GAN) (Goodfellow et al., 2016) is a prevalent method that is wildly used to generate realistic images. However, the generating process of GAN is uncontrollable so, to apply GAN in the image-to-image translation tasks, Cycle-GAN (Zhu et al., 2017) proposes to regularize the transformation with a Cycle Consistency Loss. Compared to other methods, Cycle-GAN has following advantages: . Pix2Pix (Isola et al., 2017) network requires paired images, that is, highly-related image xxx and image yyy, to perform translation, which restricts its versatility since the availability of the paired images is limited. Instead, Cycle-GAN is more flexible and be able to leverage unpaired images from different domains. | Unlike some Neural Style Transfer (Huang and Belongie, 2017) methods that take as inputs two specific images to transfer style from style image to target image, Cycle-GAN can capture high-level characteristics of domains and transfer these information between arbitray images from different domains. | The characteristics to be transfered are loosely defined so that we can control the way of transformation by choosing domains. | . Figure 1: Paired and unpaired image datasets . Paired images share most of the features while differing only in a small degree. Producing paired images usually requries hand-craft. . Formulation of Cycle-GAN . Cycle-GAN does transformation among domains so we have transformation associated domains X mathcal XX and Y mathcal YY along with data points {xi}i=1N left {x_{i} right }_{i=1}^{N}{xi​}i=1N​ and {yi}i=1N left {y_{i} right }_{i=1}^{N}{yi​}i=1N​ where xi∈Xx_{i} in mathcal Xxi​∈X and yi∈Yy_{i} in mathcal Yyi​∈Y. To transfer images from X mathcal XX to Y mathcal YY, we have a mapping G:X→YG: X rightarrow YG:X→Y. Similary, from Y mathcal YY to X mathcal XX, we have F:Y→XF: Y rightarrow XF:Y→X. And the main structure of the Cycle-GAN can be illustrated by the following figure: Figure 2: Cycle consistency . Beside the X mathcal XX and Y mathcal YY, we also have DXD_XDX​ and DYD_YDY​ which are discriminators. It’s obvious that a pair of inversed Generative Adversarial Network are integrated into a network in a cycle manner. Therefore, conisdering each half of the cycle, we have a forward GAN from X mathcal XX to Y mathcal YY with the generator GGG and the discriminator DYD_YDY​, which is similar for the backward GAN. . Adversarial Loss . Because the Cycle-GAN is essentially a combination of two GANs, therefore , it naturally borrows the Adversarial Loss as an objective. For the forward network we have: LGAN(G,DY,X,Y)=Ey∼pdata&nbsp;(y)[log⁡DY(y)]+Ex∼pdata&nbsp;(x)[log⁡(1−DY(G(x))] mathcal{L}_{ mathrm{GAN}} left(G, D_{Y}, X, Y right)= mathbb{E}_{y sim p_{ text {data }}(y)} left[ log D_{Y}(y) right] + mathbb{E}_{x sim p_{ text {data }}(x)} left[ log left(1-D_{Y}(G(x)) right] right. LGAN​(G,DY​,X,Y)=Ey∼pdata&nbsp;​(y)​[logDY​(y)]+Ex∼pdata&nbsp;​(x)​[log(1−DY​(G(x))] where maximizing the objective in terms of DYD_YDY​ encourages the discriminator to distinguish the generated images from the sampled real images while minimizing the objective in terms of GGG drives the generator to generate images that can fool the discriminator. And since we also has the backward network, there is a similar Adversarial Loss with the same form. . Consistency Loss . By merely minimizing the Adversarial Loss, the generated images are guaranteed to be mapped to the target domain. However, any subsets of the target domain satisfy the minimization of the Adversarial Loss. To further control the mapping, a Cycle Consistency Loss is introduced: Lcyc&nbsp;(G,F)=Ex∼pdata&nbsp;(x)[∥F(G(x))−x∥1]+Ey∼pdata&nbsp;(y)[∥G(F(y))−y∥1] mathcal{L}_{ text {cyc }}(G, F)= mathbb{E}_{x sim p_{ text {data }}(x)} left[ |F(G(x))-x |_{1} right] + mathbb{E}_{y sim p_{ text {data }}(y)} left[ |G(F(y))-y |_{1} right] Lcyc&nbsp;​(G,F)=Ex∼pdata&nbsp;​(x)​[∥F(G(x))−x∥1​]+Ey∼pdata&nbsp;​(y)​[∥G(F(y))−y∥1​] where the first term and the second term are the loss that measuring the distance between points of X mathcal XX, Y mathcal YY and a corresponding points that first go through either mapping GGG or FFF then go through another mapping. The consistency loss can be demonstrated as following: Figure 3: Intuition behind cycle consistency . The intuition here is to require the distance between an arbitrary point and a corresponding point that goes through the forward and backward network to be minimized, which is the same for both domains. From another angle, we can consider the forward network as an autoencoder with a encoder GGG and a decoder FFF. Additionally, a discriminator DYD_YDY​ is used to regularized the representation of the bottleneck. Therefore, combining the Adversarial Loss with the Cycle Consistency Loss, we have the final objective: L(G,F,DX,DY)=LGAN(G,DY,X,Y)+LGAN(F,DX,Y,X)+λLcyc(G,F) mathcal{L} left(G, F, D_{X}, D_{Y} right)= mathcal{L}_{ mathrm{GAN}} left(G, D_{Y}, X, Y right) + mathcal{L}_{ mathrm{GAN}} left(F, D_{X}, Y, X right) + lambda mathcal{L}_{ mathrm{cyc}}(G, F) L(G,F,DX​,DY​)=LGAN​(G,DY​,X,Y)+LGAN​(F,DX​,Y,X)+λLcyc​(G,F) . Quantatitive Results . The first table shows the performance of different models on the human perceptual tasks, which ask real human to distinguish the real images from the generated images, therefore measuring the quality of the generation. The Cycle-GAN seems to have the best performance. The second table shows the results of generation accuracy that measures pixel-wise difference between the generated images and the groundtruth. The performance of the Cycle-GAN is slightly worse than the Pix2Pix model but according to the third table, it outperforms all models including Pix2Pix in the classification tasks. The ablation study results in above tables show that forward cycle itself enables performance boost in the FCN tests while using both forward and backward cycle allows for maximized performance improvement in the classification tests. . Qualitative Results . Figure 4: Samples of images transformation in different styles . Figure 5: Transform artistic paitinings into realistic images . Figure 6: Images modification . Figure 7: Failure examples . Some failed cases shown above. One problem of the Cycle-GAN model is that it’s incapable of performing geometrical transformation e.g., transfering a cat to a dog. Also, the target transformation cannot be well learned when related datas are missing in the training set. . Reference . I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NeurIPS, 2014. . Jun-Yan Zhu*, Taesung Park*, Phillip Isola, and Alexei A. Efros. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, in IEEE International Conference on Computer Vision (ICCV), 2017. . Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros. Image-to-Image Translation with Conditional Adversarial Networks. CVPR, 2017. . Xun Huang, Serge Belongie. Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization. ICCV, 2017. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/19/GAN2_blog2.html",
            "relUrl": "/gan/2021/10/19/GAN2_blog2.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "GAN2 Understanding Deep Convolutional Generative Adversarial Networks",
            "content": "Today we will discuss the seminal Deep Convolutional Generative Adversarial Network (DCGAN) architecture as well as the paper and experiments by Alec Radford, Luke Metz, and Somit Chantal which introduced the architecture. DCGANs were representative of both the problems and promise of GANs during their early formulation, requiring tons of experimentation and tuning to properly train yet offering incredible and state of the art quality in terms of both their learned representation and image generation. The architecture worked wonderfully and continued to be used as the main GAN architecture for years after as additional innovations were built upon it. Hopefully this post allows you to understand DCGANs in terms of how they work and also the results they offered. . Background . GANs were first introduced by Goodfellow et al. and quickly caught on as an innovative yet powerful generative model. GANs are defined by a two player min-max game played between a pair of different trained models, typically parameterized as neural networks. These models are the Generator and the Discriminator. The Generator takes random noise and outputs a generated sample mirroring the desired data distribution, commonly an image. The Discriminator then takes samples, both real and fake, and attempts to predict whether they are real or not. In competing to be able to fool or correctly adjudicate one another, the models both improve until the Generator is able to output high quality images. . . The DCGAN utilizes this training setup, offering a new architecture for the two models. In its framing during the presentation by Radford et al., the DCGAN lies at the intersection of two major and well studied fields of Artificial Intelligence, and it seeks the build upon both. The first is unsupervised representation learning, which seeks to learn strong representations of samples within a dataset in order to understand and manipulate the relevant traits as well as perform downstream tasks. At the point of time when the DCGAN was introduced, this topic was already considered important, but the leading models and algorithms such as k-means, autoencoders, ladder networks, and deep belief networks were still relatively unrefined compared to modern approaches. Similarly, the second field is that of generating natural images. This too was already considered important and well-researched but was not yet overly impressive as the produced images were wobbly and blurry across models such as VAEs, RNNs with deconvolutions, and even the existing GANs. While the DCGAN did not jump all the way to modern quality in the two fields, it succeeded in taking a large step forward in both. . The Architecture . Up to this point, while GANs were undoubtedly an exciting architecture and idea, they had failed to consistently produce crisp output quality and were falling victim to a phenomenom termed mode collapse in which they produced a limited number of strong samples instead of a diverse representation of the entire distribution. Numerous papers including the introduction of Conditional GANs by Mirza and Osindero, a laplacian pyramid extension to GANs by Denton et al., a reccurent approach by Gregor et al., and a deconvolutional approach by Dosovitskiy et al. all were promising but struggled with either blurry or homogenous generation of natural images. A specific problem believed to be underlying these issues was that GANs were yet to successfully use the network most commonly used for image related tasks, CNNs. DCGANs fixed many of the downstream issues by addressing that underlying issue and utilizing CNNs as the core of their architecture, and they achieved that via a ton of experimentation and variation before settling on the following 4 innovations. . Eliminating Pooling Layers: Most CNN-based architectures at the time utilized maxpooling or simple repetition to downscaling and upscaling their representations to different sizes throughout the architecture. However, the DCGAN architecture utilized only strided convolutions and fractionally-strided convolutions for the same purpose in order to allow the network to learn its own upscaling and downscaling algorithms. | Removing Fully Connected Layers: Most CNN-based architectures at the time also typically utilized fully connected layers as a head on top of their CNN model or otherwise within the model mixed into other blocks. However, the DCGAN eliminated as many such layers as possible, leaving only a single matrix multiplication at the start of the Generator to reshape the noise vector as well as a single sigmoid layer at the end of the Discriminator. | Using Batch Normalization: The DCGAN also utilized the recently introduced batch normalization layer throughout their architecture. BatchNorm layers normalize the input to each layer which comes after them to be centered at zero and have unit variance in order to stabilize training and aid in gradient flow. They found that this helped with the problem of mode collapse and so applied it to all but the last Generator layer and first Discriminator layer. | Adjusting the Activation Functions: The final augmentation or novelty offered by the DCGAN was finely tuned activation functions. They used Tanh for the Generator output, ReLU for the other Generator layers, and LeakyReLU throughout the Discriminator. This contrasted with the previously used Maxout for other GANs. | . These simple innovations were the result of a large amount of experimentation and combined to form the “all-CNN” DCGAN architecture which was able to stabilize training and improve both the generative capacity and also the learned representations. . . Experiments . Upon arriving at their final DCGAN architecture, Goodfellow et al. continued to explore its power and effectiveness via a series of innovative and often fun experiments. The highlights of those experiments are described below. . Analysis of Possible Memorization . First, the authors were concerned that given the power of their DCGAN architecture, their trained models might just be memorizing the training data. So, they performed a number of experiments and analyses to ensure and argue that they were not. First, they trained for a single epoch with a small learning rate before qualitatively looking at the already strong generative samples and making the argument that the model could not be memorizing data yet and so must instead be learning. Similarly, they built a simple hashing model in order to match images to more quantitatively show that their generated images do not match images in the training dataset. So, the authors concluded that their model was learning rather than memorizing. . Using Learned Feature for Supervised Learning . They then continued to explore what exactly it was that their model was learning. They set up an experiment on both the CIFAR-10 and Street View House Numbers datasets in which they fully trained a DCGAN, extracted each of the Discriminator features maxpooled into a 4x4 representation, flattened, and concatenated, and finally trained a linear model on top of it for supervised classification. They then compared the results of the classification task as a proxy for the quality of the underlying representation, achieving near state of the art results on CIFAR-10 (and beating what they characterized as a strong k-means benchmark) and setting the new state of the art for the SVHN dataset. . . Exploring the Latent Space . Next, they explored the latent space by picking 10 pairs of points for the original noise vectors and generating outputs at a series of points along the line connecting each pair. They found that generations shifted steadily from one image to another while remaining semantically sound, demonstrating a strong underlying representation as well as no signs of memorization (which would likely yield sharp jumps from one image to another). . . Removing Features in Generations . Next, using manual analysis and a logistic regression model, the authors identified the features which corresponded to windows in the LSUN bedroom dataset. Then, during forward passes they dropped all positive values from these features and replaced them with noise, seeing how it affected the final generations. They impressively found that while the generations did suffer and get blurrier, they remained semantically sound and crucially did not contain any windows, instead typically replacing them with walls or mirrors. . . Performing Vector Arithmetic . Finally, the authors also performed vector arithmetic within the latent space. Instead of using a single point, they averaged points three or four points which shared a desired characteristic and then operated over that as the representation of the characteristic. In the model of the famous Word2Vec example in which the vector representation for King minus the representation for Man plus that for Woman is extremely close to Queen’s representation, they were able to similarly perform constructions to create a smiling man, woman with glasses, and a vector which represented faces turned at a variety of angles. . . Wrapping Up . So, we have described the field and landscape that the DCGAN entered into, the core features and novelties of the architecture, and the series of experiments which explored and characterized its abilities. We have seen that it was a fairly simple architecture that did not introduce any complex or even novel innovations but rather iterated on many different innovations present in the literature of the day until finding an extremely effective combination and architecture. In doing so, they arrived at the DCGAN which was able to stabilize training and produce a wide array of crisp, realistic images on a variety of datasets as well as learn a strong underlying image representation. . Reference . Radford, A., Metz, L., and Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2016. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/19/GAN2_blog.html",
            "relUrl": "/gan/2021/10/19/GAN2_blog.html",
            "date": " • Oct 19, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "GAN1 f-GAN-Training Generative Neural Samplers using Variational Divergence Minimization",
            "content": "Intro . This blog post focuses on the paper f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization and focuses on the GAN model and improving the model through making it more versatile and applicable to many different scenarios. Traditionally generative models are very much focused on computing samples or derivations but fail to compute likelihoods and marginalization. To combat the inability to do so, the adversial network was added to the generative model making the GAN model (Generative Adverisial Model). The paper goes into more detail about expanding the adverisal neural network concept and making it more generic so it is applicable to all needs. ​ ​ . Background . The paper has two focuses and discusses two model types. The paper aims to improve generative models as a whole, a GAN is a subset of a generative model. . Generative Models . Generative models are more diverse and on a macro level are made to imitate some kind of distribution. The technical definition is that a a generative distribution aims to describe a probability distribution over a domain. Generative models are able to perform sampling, estimation, and point-wise likelihood estimation tasks. Going more into detail: . Sampling: inspecting samples or calculating a function on a set of samples -&gt; able to get ideas about the distribution / resolve decisions from class Q. . | . Estimation: Given samples {x1, x2, . . . , xn} from an unknown true distribution, get a Q that is able to describe the distribution. (*true distributions only however) . | . Point-wise Likelihood Eval: Given sample x, able to evaluate likelihood Q(x). . | . GANS . GANs were first introduced by Goodfellow et al. 2014 and the Goodfellow GAN can be described as follows: GANs are type of generative model used to come up with images or other samples to imitate real life samples. In more detail, the objective of the GAN when it comes to training a generator deep net whose input is a standard Gaussian, and whose output is a sample from some real distribution. . GANs have all the features of a normal generative model except they are able to perform additional tasks. GANs are able to perform exact sampling and approximate estimation on top of everything a generative model can do. They are able to do this because of the way the model is designed with the key separator being the adversial neural network. The GAN possesses a feed-forward neural network framework which allows it to produce an output from a random sample known as the generative neural sampler. . The goal of the authors of the paper is to take the functionality of the adversial network within GANs and expand their application to all generative models through something called f-divergence. Understanding GAN Math . GANs comprise of a generator and discriminator working in sync where the generator aims to create an output that will fool the discriminator into thinking that the generator’s output is the same as a real output. The discriminator will give a result which will pass back to the generator for it to improve and once again it will create an output to pass to the discriminator. This cycle is how the GAN is trained. The key component to look at here is the discriminator function. The f-divergence concept is heavily tied to the discriminator and so that is what we will be looking at. . The original GAN paper (Goodfellow et al 2014) described the model to include two divergences for the discriminator. The discriminator mainly compromises of the Jennson-Shannon Divergence (JS) and the Kulback-Leibler Divergence (KL). GANs are different since they use two divergences for training that are optimized simultaneously. . . Looking at the image of the Jennson-Shannon Divergence we are able to see how the internal Kulback-Leibler Divergence makes use of both distributions P and Q for comparison. . The critical thing to keep note of here is that DJS (P||Q) is a proper divergence measure between distributions this implies that the true distribution P can be approximated well in case there are sufficient training samples and the model class Q is rich enough to represent P. . The paper expands this concept from the GAN training objective and aims to generalize it for arbitrary f-divergences (so no longer just the Jennson-Shannon or Kulback-Leibler). . Overview . The core contributions of the paper as follows: . Derive the GAN training objectives for all f-divergences and provide as example additional divergence functions. . | . Dimplify the saddle-point optimization procedure of Goodfellow. . | . Provide experimental insight into which divergence function is suitable for estimating. . | . This blog post will cover in detail each section for better understanding. . Methodology (f-divergence) . The idea of f-divergence originates from a different paper (Nguyen et al.). Nguyen came up with a divergence estimation framework based off f-divergence. . Statistical divergences such as Kullback-Leibler measure the difference between two probability distributions. F-divergences essentially is synonymous as a large class of different divergences known as the Ai-Silvey distances. So, given two distributions P and Q that possess, respectively, an absolutely continuous density function p and q we can define a f-divergence as the following: . The key thing to keep in mind is that f-divergence aims to to find the difference between two distributions and so the generator function f must be convex, lower-semicontinuous function satisfying f(1) = 0. This is critical for the f-divergence to work correctly. This is specific for the function above and would be different for different functions. f(1) = 0, is important so Df (P||P) = 0. . Variational Estimation of F-Divergences . Nguyen et al. derived a general variational method to estimate f-divergences given only samples from P and Q. However, the authors of the paper decided to expand upon that and extend their method from estimating divergences for fixed models to estimating model parameters. Hence, the variational divergence minimization (VDM). The VDM framework was invented to show that GANs fall in this class of framework and is a highly specialized version of tha VDM framework. . The shortened proof is as follows: . Continuing off Nguyen et al’s divergence estimation procedure, every convex lower-semicontinuous function (f) has a convex conjugate function (f*) that is known as the “Fenchel conjugate”. The function is defined below: . A key characteristic of this function (f*) is that it is also convex and lower-semicontinuous. The creates a duality with f and (f*) where f** = f. Thus, the function (f) can be rewritten as . As it can be seen in the proof, the importance of the structure of the generator goes beyond the first derivative and is needed for the rest of the calculations to occur. The suprema equation gets used below in the next section of the shortened derivation. Which is as follows: . Continuing from the previous equation and taking from Nguyen et al. a new equation can be created. The equation defined above is a variational representation of f and can be substituted into the definition of f-divergence from Nguyen et al. Doing so will create a lower bound on the divergence as follows: . . In the equation above there are two types of Ts referenced and so for the sake of this blog post T shall indicate the function T(x) and T~ shall indicate the class that T is contained within. . The T~ is introduced as an arbitrary class of functions but is primarily used to determine which f is the best suited for the situation. The derivation results in a lower bound because of Jensen’s inequality when swapping the integral for a suprema. Another contributing component is that T~ contains only a subset of all possible functions, thus there are only a certain number of possible functions to choose from that fit the requirements. . Thus, we can determine the T that is the best fit taking the variation of the lower bound from the above T and find that the bound is tight for: . Defining T* is critical as it is the main component in discovering the optimal function (f) for our divergence. The conditional statement helps in choosing f and designing the class of functions T~. Since every divergence will have it’s own unique T* function we need the condition above to construct a consistent function that can accurately portray f. . The generated table of relevant divergences and their functions is listed in this table and provides an example of how all these functions differ and their implementation could make profound changes on the applied scenario. . . Variational Divergence Minimization (VDM) . All the previous work and definitions were made to fully explain the VDM. The variational lower bound (T) served to figure out the P and Q for Df(P||Q). P is the true distribution and Q is the generative model that is estimated based off P. . To accomplish this goal the GAN approach is considered. By making use of two neural networks Q and T where Q is the generative model and T is the variational function. Q takes in a random input vector and outputs a sample. T takes in an input and returns a scalar. Q uses the variable θ and T uses w. In terms of creating a parametric equation you get: . The generative model (Q) can be learned by finding a saddle point for the following f-GAN objective where the goal is to minimize θ and maximize w. . The terms in the equation above can be calculated through the following: Ex∼P [·] can be approximated by sampling from the training set without replacement; Ex∼Qθ [·] can be approximated through sampling from the current model. It is important to note to sample the same number from each location. . Running Single-Step Gradient we are able to find a saddle point where Theta is strongly convex and W is strongly concave. There can be many satisfying saddle points and so it comes down to setting a custom metric as to what you want to accept. . . VDM vs GAN Construction . The GAN model and VDM model look similar and operate similarly too with how they aim to find a saddle point for maximizing and minimizing certain sections. However, there are some key distinctions between the two. . FGAN: GAN: . The equations are very similar in appearance but there are some minor discrepancies. The key differences that the D function for GANs is a discriminant function that is more complex than the T lower bound designator function. Also in terms of training speed, maximizing the second term in the F-Gan model is easier than minimizing the second term in the GAN model. . With this you get to see the improvements made and how the VDM expands the capability of a GAN and improves not only its performance in terms of time to compute, but also widens the number of applications that the GAN is able to be used in. . Experiments &amp; Empirical Results . The paper is interesting since it begins talking about f-divergence and expanding on the purpose of f-divergence and their applications. However, the paper ends up with a new model that makes use of f-divergence and variational divergence estimators to make the VDM. This section will consider the success of the VDM. The VDM differs from the traditional GAN and as such is tested differently. The VDM model mainly depends on the divergences that make up the VDM and as such are necessary to test all the various divergences as opposed to just testing the model itself. . Different f-divergences . When testing for different functions to see how well they perform, the researchers came up with this experiment: . Have models learn a Gaussian distribution and find the most optimal parameters to describe the Gaussian distribution. When Q, our model, is turned into a linear function that receives: z ~ N(0,1). Q outputs: Gθ(z) = µ + σz where θ = (µ, σ). . The results of the experiment led to the following table: . . The results of the table affirm the qualities of the VDM. The left side of the table discusses the VDM’s ability to determine the model parameters. This is seen with the µ and σ variables that are meant to show how well the model was able to predict the characteristics as opposed to the best fit / the actual parameters. The right side is a bit less intuitive, but it shows that the models were best fit for the divergence they were trained for. The interesting thing is the dynamic between divergences and well they score when crossfitted with other divergences. This is useful to determine what combinations of divergences are viable options together like for instance using JS and KL for the GAN objective function. . MNIST Digits . Two components to the VDM: Generative Model and Variational Function similar to the GAN. This experiment essentially compares how well the VDM is able to generate MNIST digits using different divergences. The results are compared to a control group. The design of the models that were used for generating were very simple. They made use of a very simple generative model with batch normalization, ReLU, and sigmoid activation functions. The variational function is also simple with a few linear layers and exponential in between. The VDM is trained as described by sampling without replacement from batches of size 4096. The model is then trained on that information for one hour. The model was compared against variational autoencoders with at least 20 latent dimensions. The results were compiled by doing kernal density estimation and the average log-likelihood was measured for performance. . . The table and the created outputs for some divergences are displayed. The results for each divergence is given and some basic conclusions can be made. Overall, the performance is all over the place in comparison to the control group, with some divergences performing better than others. The point of the experiment still stands and shows that different divergences and combinations of divergences are better for certain implementations and it comes down to the researchers to figure that out. . Conclusions &amp; Summary . The focus of the paper was more on the GAN feed forward network and how that generative neural sampler could be applied to all generative models. The paper expanded on the concept of f-divergence borrowing from Nguyen et al and drew elements from the traditional GAN to make a new type of GAN, hence the name of the paper f-GAN. Utilizing the proper divergence function leads to the optimal model and better performance and this point was further emphasized with the experiments. The paper contributed the VDM model and the general idea behind the usage of f-Divergence with an application towards GNS / GANs. Thus, GANs can be generalized to an arbitrary divergence function and the algorithm behind GANs is simplified as well. . References . 2016 (NeurIPS): S. Nowozin, B. Cseke, R. Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. NeurIPS, 2016. .",
            "url": "https://cs598ban.github.io/Fall2021/gan/2021/10/14/GAN1_blog.html",
            "relUrl": "/gan/2021/10/14/GAN1_blog.html",
            "date": " • Oct 14, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "NF4 Discrete Flows - Invertible Generative Models of Discrete Data",
            "content": "Introduction . In recent years, normalizing flows have become a promising method for both generative tasks and density estimation. This is because, unlike variational autoencoders, it is tractable to compute direct log-likelihoods, which can then be used to optimize the model. While normalizing flows have shown strong results in modeling continuous domains, they have not been explored in a discrete setting. This may be because the idea of a flow in a discrete setting is difficult to intuit. However, it can be considered as “relabeling” the data instead. The key idea of this paper is: Can flows be used on discrete distributions? . In this paper, two new models are introduced: . Discrete Autoregressive Flows | Discrete Bipartite Flows | . Background . Continuous normalizing flows . Normalizing flows generally assume that there is a function, f−1f^{-1}f−1, usually parameterized by a neural network, which is invertible. The goal is to map the distribution of the data to a more easily interpretable base distribution. Generally it is desirable for the base distribution to be factorizable (independent components), and spherical Gaussians are often used. Additionally, much work enforces a constraint on the Jacobian of f−1f^{-1}f−1 to make it computationally tractable to compute. To optimize the maximum likelihood of the data, a change of variables formulation is used to instead calculate the maximum likelihood of the base, which is tractable. . Given a discrete random variable xxx from the base distribution and y=f(x)y = f(x)y=f(x), we can write the change of variables as: p(y)=p(f−1(y))det∣dxdy∣ p(y) = p(f^{-1}(y)) text{det}| frac{dx}{dy}| p(y)=p(f−1(y))det∣dydx​∣ Note that prior work often writes this the other direction, but it is equivalent. . Generally, normalizing flows can be divided into two classes: . Autoregressive Flows, such as MAF [2] or IAF [3]. | Bipartite flows, such as RealNVP [4]. | . Autoregressive Flows . These are models that are both autoregressive and flows. Autoregressive models take the previous inputs and use it to create an output. The output is then passed back in to the function to calculate the next output. Given a base distribution x∼p(x)x sim p(x)x∼p(x) in DDD dimensions, Transform xxx into yyy: yd=μd+σd⋅xd y_d = mu_d + sigma_d cdot x_d yd​=μd​+σd​⋅xd​ where σd=f(y1,...,yd−1) sigma_d = f(y_1, ..., y_{d-1})σd​=f(y1​,...,yd−1​) for d∈[D]d in [D]d∈[D]. For the inverse, xd=σd−1(yd−μd) x_d = sigma^{-1}_d (y_d - mu_d) xd​=σd−1​(yd​−μd​) . Bipartite flows . These methods use a bipartite factorization method. Essentially, some variables are held constant while the others are transformed. For total dimensions DDD and 0&lt;d&lt;D0 &lt; d &lt; D0&lt;d&lt;D, y1:d=x1:d y_{1:d} = x_{1:d} y1:d​=x1:d​ yd+1:D=μ+σx(d+1):D y_{d+1:D} = mu + sigma x_{(d+1):D} yd+1:D​=μ+σx(d+1):D​ Here, note that μ muμ is a location transform and σ sigmaσ is a scale transform. Both forward (inference) and inverse (generation) computations are fast, but these models are not as expressive as autoregressive flows. . Normalizing flows have the downside that they cannot be used for dimensionality reduction. . Discrete Distribution Modelling . There has not been comparable work to normalizing flows for discrete distributions. Existing work either focuses on: . latent variable models, such as generating sentences from a continuous space | models that assume a fixed-ordering of the data, such as RNNs, Transformers, etc. | . Older work has also used bidirectional models such as Markov random fields, but these require approximate inference or sampling. However, note that models such as BERT show improvements from bidirectionality, which this paper has in the form of discrete bipartite flows. There has also been work on non-autoregressive discrete models, but they do not maintain an exact density like this work. . Building Blocks . Discrete Change of Variables . Suppose xxx is a discrete random variables and y=f(x)y = f(x)y=f(x) Then the discrete change of variables is: p(y=y)=∑x∈f−1(y)p(x=x)p( textbf{y} = y)= sum_{x in f^{-1}(y)} p( textbf{x}=x) p(y=y)=x∈f−1(y)∑​p(x=x) where f−1f^{-1}f−1 is the pre-image of fff. If fff is invertible, this simplifies to: p(y=y)=p(x=f−1(y)) p( textbf{y} = y)=p( textbf{x}=f^{-1}(y)) p(y=y)=p(x=f−1(y)) Compare this to the continuous change of variables: p(y)=p(f−1(y))det∣dxdy∣ p(y) = p(f^{-1}(y)) text{det}| frac{dx}{dy}| p(y)=p(f−1(y))det∣dydx​∣ Essentially, the discrete formulation is just lacking the Jacobian determinant. This makes intuitive sense: discrete distributions have no volume, so there is no need to correct for the change in volume (which is what the determinant does). . Discrete Flow Transformations - XOR Example . To create discrete flows, we need a discrete, invertible analog to the functions used for transformation in the continuous case. To this end, we will first consider a binary case: XOR. Given a binary vector xxx, we can compute yd=μd⊕xd y_d = mu_d oplus x_d yd​=μd​⊕xd​ for ddd in 1,...,D1, ..., D1,...,D This is of course invertible: xd=μd⊕yd x_d = mu_d oplus y_d xd​=μd​⊕yd​ Now we will consider an example to build intuition. Assume D=2D=2D=2 and p(x)p(x)p(x) defined as follows: . XOR example table [1] It is clear that we cannot factorize this distribution (i.e., we cannot write the distribution as a product p(x1)p(x2)p(x_1)p(x_2)p(x1​)p(x2​), which would be an independence assumption) Now, we will show that flows can model this. To do so, consider the flow f(x1,x2)=(x1,x1⊕x2)=(y1,y2) f(x_1, x_2) = (x_1, x_1 oplus x_2) = (y_1, y_2)f(x1​,x2​)=(x1​,x1​⊕x2​)=(y1​,y2​) Here, y2=x1⊕x2y_2 = x_1 oplus x_2y2​=x1​⊕x2​ is the invertible transformation, while y1=x1y_1 = x_1y1​=x1​ is the linear partition. . Thus, after applying one layer of flow, p(x1)=[0.7,0.3],p(x2)=[0.9,0.1]p(x_1) = [0.7, 0.3], p(x_2) = [0.9, 0.1]p(x1​)=[0.7,0.3],p(x2​)=[0.9,0.1]. Here, the notation used is confusing, so we will instead call p(x1)=p(y1)p(x_1) = p(y_1)p(x1​)=p(y1​) and p(x2)=p(y2)p(x_2) = p(y_2)p(x2​)=p(y2​). Thus, p(y1)=[0.7,0.3],p(y2)=[0.9,0.1]p(y_1) = [0.7, 0.3], p(y_2) = [0.9, 0.1]p(y1​)=[0.7,0.3],p(y2​)=[0.9,0.1] . p(y1)p(y_1)p(y1​) is the marginal of the table above. However, how do we get p(y2)p(y_2)p(y2​)? Plugging this into fff, p(y2=0)=1−p(x1⊕x2)=p(x1=x2)=0.63+0.27=0.9p(y_2=0) = 1-p(x_1 oplus x_2) = p(x_1 = x_2) = 0.63 + 0.27 = 0.9 p(y2​=0)=1−p(x1​⊕x2​)=p(x1​=x2​)=0.63+0.27=0.9 Essentially, the flow “relabels” the data so it is better modeled by the base. . Discrete Flow Transformations - Extension to Categorical Data . We now have flows for binary variables, but we want to extend them to categorical discrete data. To do so, we take a page from number theory and generalize the XOR function. The authors call this “modulo location-scale transform”. . Given a DDD-dimensional vector xxx where each element has KKK values, yd=(μd+σd⋅xd)mod&nbsp;K y_d = ( mu_d + sigma_d cdot x_d) text{mod } K yd​=(μd​+σd​⋅xd​)mod&nbsp;K Here, μd mu_dμd​ and σd sigma_dσd​ autoregressive functions of yyy. Note that σ sigmaσ cannot be zero (just 1,…,K−11,…,K-11,…,K−1) just like in continuous case. . An important condition of flows is the invertible function. To invert the above function, it is necessary that σ sigmaσ and KKK are coprime. This means they only share the divisor 1. Hence, there are three possible constraints that can be used to make this happen: . Set KKK to be prime | Mask noninvertible (non-coprime to KKK) values of σ sigmaσ | Set σ=1 sigma=1σ=1 | . Additionally, note what happens when K=2K=2K=2 and σ=1 sigma = 1σ=1. It’s the XOR function! . We can see how the modulo location-scale transform works in the following figure. . Example of a single flow using the modulo location-scale transform. (a) is the data modelled (which is discretized into bins), (b) is an attempt to factorize the base distribution, and (c) is 1 discrete flow. Note that even 1 flow is much better at modelling the data. [1] Model . Discrete Autoregressive Flows . . This figure shows 4 inputs (and outputs). Note multiple levels of autoregression can be stacked. The solid lines show receptive fields of the red block, while the dashed lines show other connections. Note how the order can be switched . Discrete Bipartite Flows . . This figure shows two bipartite flows. The receptive field of the 2nd output is only x1:3x_{1:3}x1:3​, unlike autoregressive flows above. Note that Blue and green are binary masks indicating variables that don’t transform. The other blocks are transformed. This can quickly be confirmed by looking at the number of outgoing lines from a block. . Training . Like other normalizing flows, the model can be trained by directly optimizing the maximum likelihood. Again, note that we have change of variables: p(y=y)=p(x=f−1(y)) p( textbf{y} = y)=p( textbf{x}=f^{-1}(y)) p(y=y)=p(x=f−1(y)) In this case, we can optimize both parameters of fff and base distribution ppp. Note that this paper doesn’t distinguish ppp from the left and the right, but we are optimizing ppp on the right. . . Unfortunately, using discrete functions means we cannot calculate a direct gradient. Specifically, μ muμ and σ sigmaσ both produce discrete values but need to be backpropagated through. To address this issue, the authors use the straight-through gradient estimator. This essentially means that on the backward pass of the network they pretend the discrete function is the identity function and just pass the gradients through. . We run into another issue with differentiation. To compute μ muμ and σ sigmaσ, first produce KKK logits for each, where KKK is chosen using an argmax. μd=one_hot(argmax(θd)) mu_d = text{one _hot} ( text{argmax} ( theta_d) ) μd​=one_hot(argmax(θd​)) However, this also isn’t differentiable! Instead, we use a temperature-softmax dμddθd≈ddθdsoftmax(θdτ) frac{d mu_d}{d theta_d} approx frac{d}{d theta_d} text{softmax} ( frac{ theta_d}{ tau}) dθd​dμd​​≈dθd​d​softmax(τθd​​) Now, we have a differentiable function. Further, this function approaches the argmax as τ tauτ approaches 0. The authors pick τ=0.1 tau=0.1τ=0.1 for their experiments. . Results . Alright, we’ve looked at how the model works. Now, let’s see the results! . Experimental Settings . For discrete autoregressive flows, we will use an autoregressive Categorical base distribution. For discrete bipartite flows, use a factorized Categorial distribution. Use σ=1 sigma = 1σ=1 for all experiments except character-level language modeling. . Full-rank Discrete Distribution . First, to test the flexibility of the model, the authors test the model on how it fits a full-rank discrete distribution. They sample KKK classes in DDD dimensions from a Dirichlet distribution with α=1 alpha=1α=1. Additionally, they use a transformer with 64 hidden units is used as a base model and for flow parameters. They compute “nats” for negative log likelihood, indicating natural logarithm is used. . . Negative log-likelihoods for the full-rank distribution fitting task. $D$ and $K$ are varied. Note that bipartite flows achieve nearly the same values as autoregressive flows without the associated disadvantages. [1] ### Addition Next, the authors consider a more challenging toy example: adding base-10 numbers, which they do with $D=10$ and $D=20$ digits. Addition is a right-to-left task, which disadvantages the base autoregressive model. They use an LSTM with 256 hidden units for D=10 and 512 for D=20 as a base. . For D=10, the autoregressive base (left to right) achieves 4.0 nats (negative log likelihood). The autoregressive flow achieves 0.2 nats. | The bipartite model achieves 4.0, 3.17, and 2.58 nats for 1, 2, and 4 flows. | For D=20, the autoregressive base achieves 12.2 nats (negative log likelihood). The autoregressive flow achieves 4.8 nats. | The bipartite model achieves 12.2, 8.8, 7.6, and 5.08 nats for 1, 2, 4, and 8 flows. | . Potts Model . Next, the authors wonder: Can the discrete flows be applied to models with intractable sampling and likelihood? To test this, they sample from the Potts model, which is a 2D Markov random field. Samples are a DDD x DDD matrix, where the coupling between elements is JJJ. Essentially, it is a grid of discrete values where neighbors are correlated according to JJJ. Additionally, since sampling is intractable, they use 500 steps of the Metropolis-Hastings (MCMC). . . An example of Potts models from [5]. Note that β = J here. The following figure shows their tests on this model. Note that the authors use this task to examine the different between an autoregressive base model and their autoregressive flows. They do not test bipartite flows. The following table shows how the base autoregressive model cannot capture the undirected Potts model as well. . . Additionally, the authors sample from these models and find that they cannot distinguish the samples from the true distribution. . . Character Level Tasks . Finally, the authors test the models on real-world tasks: modelling natural language. First, use the Penn treebank. It has K=51K = 51K=51 characters. To preprocess, data is split into sentences. In this work, sequence length is restricted to 288 (which is not explained). The authors compare to only one other non-autoregressive language model [6], a VAE-based generative model which learns a normalizing flow in the latent space. However, that model isn’t directly comparable because the sequence length was not restricted for it. . . Next, the authors tested on the text8 dataset. This was originally intended for testing text compression algorithms, and it has 100M characters as opposed to just 5M, which provides more data to improve the models. The results are shown in the following table: . . Note, the important takeaway is how much faster these models can generate compared to the large Transformer and RNN models. . . This figure from Papers With Code [7] shows how the bipartite flow results compare to all models tested on text8. Final Takeaway . This paper is an initial step in the direction to extend the power of normalizing flows to discrete distributions. It can be summarized in the following points: . Motivation: Normalizing flows generally are only used for continuous distributions . This can be extended to discrete distributions using a different change of variables formulation (without a Jacobian determinant!) | . | Discrete autoregressive flows enable bidirectionality . | Discrete bipartite flows enable quick generation . | Future work: . Can inverse autoregressive flows be made discrete? . | How can this method be scaled to many more classes? The straight-through estimator might not work on word sequences with 1000s of vocabulary tokens. . (This is also why they test on character modelling instead, which has significantly fewer values) | . | Can other invertible discrete functions be applied, such as from cryptography or random number generation? . | . | . References . [1] Tran, Dustin, et al. “Discrete flows: Invertible generative models of discrete data.” Advances in Neural Information Processing Systems 32 (2019): 14719-14728. [2] Papamakarios, George, Theo Pavlakou, and Iain Murray. “Masked autoregressive flow for density estimation.” arXiv preprint arXiv:1705.07057 (2017). [3] Kingma, Durk P., et al. “Improved variational inference with inverse autoregressive flow.” Advances in neural information processing systems 29 (2016): 4743-4751. [4] Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. “Density estimation using real nvp.” arXiv preprint arXiv:1605.08803 (2016). [5] ACM Youtube: “Session 6B - Efficient sampling and counting algorithms for the Potts model” [6] Ziegler, Zachary, and Alexander Rush. “Latent normalizing flows for discrete sequences.” International Conference on Machine Learning. PMLR, 2019. [7] https://paperswithcode.com/sota/language-modelling-on-text8 .",
            "url": "https://cs598ban.github.io/Fall2021/normalizing%20flows/2021/10/12/NF4_blog.html",
            "relUrl": "/normalizing%20flows/2021/10/12/NF4_blog.html",
            "date": " • Oct 12, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "VAE4 Can VAE learn concepts from data unsupervised?",
            "content": "Background on VAEs . The Variaional Autoencoders (VAEs) are a method of modeling data distribution p(x)p( mathbf{x})p(x) by introducing latent random variables. Intuitively, VAE encodes the input into some compressed representation form in the latent space, and by forcing a correct reconstruction, hopefully, the model captures some insights in the data distribution. . To be formal, we propose pθ(x)=∫zpθ(x∣z)pθ(z) dzp_ theta( mathbf{x})= int_{ mathbf{z}} p_ theta( mathbf{x}| mathbf{z})p_ theta( mathbf{z}) ,d mathbf{z}pθ​(x)=∫z​pθ​(x∣z)pθ​(z)dz . where pθ(z)p_ theta( mathbf{z})pθ​(z) is the prior distribution, usually assumed to be standard normal distribution, pθ(x∣z)p_ theta( mathbf{x}| mathbf{z})pθ​(x∣z) is the likelihood (probabilistic encoder), and pθ(z∣x)p_ theta( mathbf{z}| mathbf{x})pθ​(z∣x) is the posterior (probablistic decoder). . This posterior, however, need to calculate this integral, which is intractable if z mathbf{z}z is high dimensional. So, we introduce another approximator qϕ(z∣x)q_ phi( mathbf{z}| mathbf{x})qϕ​(z∣x). . The architecture can be summarized as follows: . . Image Credits to Wikipedia on Variational Autoencoder__;Kg!!DZ3fjg!t6Ws-NJYmBcOfbpLUXAjo8DEPtCG30oxgsJHir59ycjXJtqwfs7MNQ-7N8ZuWMjx9w$ | . Naturally, in order to train the model, we want to maximize the probability on the dataset, pθ(x)p_ theta( mathbf{x})pθ​(x). We achieve this through maximizing a lower bound of it. . Evidence Lower Bound (ELBO) . log⁡pθ(x)≥log⁡pθ(x)−DKL(qϕ(z∣x)∥pθ(z∣x))undefined≥0=Ez∼qϕ(z∣x)[log⁡pθ(x∣z)]undefinedReconstruction−DKL(qϕ(z∣x)∥pθ(z))undefinedRegularizationundefinedELBO begin{align*} log p_ theta( mathbf{x}) &amp; geq log p_ theta( mathbf{x})- overbrace{D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}| mathbf{x}) right)}^{ geq 0} &amp;= underbrace{ underbrace{ mathbb{E}_{ mathbf{z} sim q_ phi( mathbf{z}| mathbf{x})} left[ log p_ theta( mathbf{x}| mathbf{z}) right]}_ text{Reconstruction} - underbrace{D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}) right)}_ text{Regularization}}_ text{ELBO} end{align*}logpθ​(x)​≥logpθ​(x)−DKL​(qϕ​(z∣x)∥pθ​(z∣x)) . ​≥0​=ELBO . Reconstruction . Ez∼qϕ​(z∣x)​[logpθ​(x∣z)]​​−Regularization . DKL​(qϕ​(z∣x)∥pθ​(z))​​​​​ . In practice, we use gradient descent to minimize the negative ELBO, called VAE Loss. . Lθ,ϕ(x)=−Ez∼qϕ(z∣x)[log⁡pθ(x∣z)]+DKL(qϕ(z∣x)∥pθ(z)) mathcal{L}_{ theta, phi}( mathbf{x})=- mathbb{E}_{ mathbf{z} sim q_ phi( mathbf{z}| mathbf{x})} left[ log p_ theta( mathbf{x}| mathbf{z}) right]+D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}) right)Lθ,ϕ​(x)=−Ez∼qϕ​(z∣x)​[logpθ​(x∣z)]+DKL​(qϕ​(z∣x)∥pθ​(z)) . What is Desired? Disentanglement! . Disentanglement = Independence + Semantics . We are hoping that unsupervised learning could produce some results that have special meanings to human begins. One specific factor is whether each dimension in the latent space has a atomic meaning capturing some concept from the dataset. . Unsupervised learning of a disentangled posterior distribution over the underlying generative factors of sensory data is a major challenge in AI research 1 2. | Motivations include discovering independent components, controllable sample generation, and generalization/robustness. | Facilitates interpretable decision making and controlled transfer. | . The following graph from Ricky Chen’s talk demontrates what we want clearly. For different sample points in the latent space, they are of different genders, ages, and etcs. However, along the axis pointed by the arrow, it means whether the generated images wear sunglasses. Such disentanglement in the space means we can reliably predict how the generated images would change. . . Axis-aligned traversal in the representation space and Global interpretability in data space. Image Credits to Ricky Chen’s talk at NIPS 2018 | . On the other hand, vanilla VAE’s objective is focusing only on reconstruction, if we look at the right examples, traversing along an axis does not produce a smooth changing trend. . . Traversal of the rotationallatent dimension 3. | . Datasets for Disentanglement . Most of these datasets are specifically constructed so that the intended disentanglement factors are clear. Take dSprites as an example, factors include posXposXposX, posYposYposY, rotationrotationrotation, shapeshapeshape, and scalescalescale. . . Common datasets used in the disentanglement task4. | . Related Works . DC-IQN . An obvious attempt is to attach meanings to latent spaces by designers. Deep Convolutional Inverse Graphics Network (DC-IGN) 5 is a model similar to a VAE with special designed training procedure to enforce a designed latent space. . . DC-IQN architecture 5. | . . DC-IQN latent structure 5 | . In short, to enforce the structure, they use a modified training proceduring. First select a dimension corresponding to a factor, then form a minibatch where only that factor changes. They masked the output of other dimensions by averaging them so that the gradient signal is mixed and mingled, which is going to force the network to capture the changes in the specifie dimension. . . DC-IQN training 5 | . InfoGAN . The GAN formulation uses a simple factored continuous input noise vector z mathbf{z}z, but imposing no restrictions on how the generator may use it. So the generator may use it in a highly entangled way. . However, in InfoGAN6, . Uses a set of structured latent variables c=(c1,…,cL) mathbf{c}=(c_1, dots,c_L)c=(c1​,…,cL​), and assuming p(c)=∏i=1Lp(ci)p( mathbf{c})= prod_{i=1}^L p(c_i)p(c)=∏i=1L​p(ci​). | The generator becomes G(z,c)G( mathbf{z}, mathbf{c})G(z,c). | With no constraints, the generator could ignore c mathbf{c}c, pG(x∣c)=pG(x)p_G( mathbf{x}| mathbf{c})=p_G( mathbf{x})pG​(x∣c)=pG​(x). | There should be high mutual information between latent code c mathbf{c}c and the generator distribution, meaning I(c;G(z,c))I( mathbf{c};G( mathbf{z}, mathbf{c}))I(c;G(z,c)) should be high. | . An Attempt: β betaβ-VAE . ELBO from Another Perspective . Quick Mention on Karush-Kuhn-Tucker (KKT) Conditions . If we have a non-linear programming problem. Optimize&nbsp;f(x)subject&nbsp;to&nbsp;gi(x)≤0,i=1,…,mhj(x)=0,j=1,…,r begin{align*} &amp; text{Optimize } &amp;&amp;f( mathbf{x}) &amp; text{subject to } &amp;&amp;g_i( mathbf{x}) leq 0, i=1, dots,m &amp; &amp;&amp;h_j( mathbf{x})=0, j=1, dots,r end{align*} ​Optimize&nbsp;subject&nbsp;to&nbsp;​​f(x)gi​(x)≤0,i=1,…,mhj​(x)=0,j=1,…,r​ . Then, we can form the Lagrangian function: L(x,μ,λ)=f(x)+μT[g1(x),…,gm(x)]T+λT[h1(x,…,hl(x)]TL( mathbf{x}, mathbf{ mu}, mathbf{ lambda})=f( mathbf{x})+ mathbf{ mu}^T left[g_1( mathbf{x}), dots,g_m( mathbf{x}) right]^T+ mathbf{ lambda}^T left[h_1( mathbf{x}, dots,h_l( mathbf{x}) right]^TL(x,μ,λ)=f(x)+μT[g1​(x),…,gm​(x)]T+λT[h1​(x,…,hl​(x)]T . If (x∗,μ∗,λ∗)( mathbf{x}^*, mathbf{ mu}^*, mathbf{ lambda}^*)(x∗,μ∗,λ∗) solves the problem, then Karush-Kuhn-Tucker Conditions holds: . Stationarity: ∇f(x∗)+∑i=1mμi∇gi(x∗)+∑j=1rλj∇hj(x∗)=0 nabla f( mathbf{x}^*)+ sum_{i=1}^m mu_i nabla g_i( mathbf{x}^*)+ sum_{j=1}^r lambda_j nabla h_j( mathbf{x}^*)=0∇f(x∗)+∑i=1m​μi​∇gi​(x∗)+∑j=1r​λj​∇hj​(x∗)=0 for minimization. | Primal Feasibility: gi(x∗)≥0,i=1,…,mg_i( mathbf{x}^*) geq 0,i=1, dots,mgi​(x∗)≥0,i=1,…,m and hj(x∗)=0,j=1,…,rh_j( mathbf{x}^*)=0,j=1, dots,rhj​(x∗)=0,j=1,…,r. | Dual Feasibility: μi≥0,i=1,…,m mu_i geq 0, i=1, dots,mμi​≥0,i=1,…,m. | Complementary Slackness: ∑i=1mμigi(x∗)=0 sum_{i=1}^m mu_i g_i( mathbf{x}^*)=0∑i=1m​μi​gi​(x∗)=0. | . If we take a look at the VAE loss again θ,ϕ=arg⁡min⁡θ,ϕ{−Ez∼qϕ(z∣x)[log⁡pθ(x∣z)]+DKL(qϕ(z∣x)∥pθ(z))]} theta, phi= underset{ theta, phi}{ arg min} left {- mathbb{E}_{ mathbf{z} sim q_ phi( mathbf{z}| mathbf{x})} left[ log p_ theta( mathbf{x}| mathbf{z}) right]+D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}) right)] right }θ,ϕ=θ,ϕargmin​{−Ez∼qϕ​(z∣x)​[logpθ​(x∣z)]+DKL​(qϕ​(z∣x)∥pθ​(z))]} . We can formulate it as a constrained optimization problem: . Optimization Problem from ELBO . min⁡θ,ϕ−Ez∼qϕ(z∣x)[log⁡pθ(x∣z)]&nbsp;subject&nbsp;to&nbsp;DKL(qϕ(z∣x)∥pθ(z))]&lt;ϵ min_{ theta, phi}- mathbb{E}_{ mathbf{z} sim q_ phi( mathbf{z}| mathbf{x})} left[ log p_ theta( mathbf{x}| mathbf{z}) right] text{ subject to }D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}) right)]&lt; epsilonθ,ϕmin​−Ez∼qϕ​(z∣x)​[logpθ​(x∣z)]&nbsp;subject&nbsp;to&nbsp;DKL​(qϕ​(z∣x)∥pθ​(z))]&lt;ϵ . Rewriting it as a Lagrangian under KKT conditions, we have F(θ,ϕ,β;x,z)=−Ez∼qϕ(z∣x)[log⁡pθ(x∣z)]+β(DKL(qϕ(z∣x)∥pθ(z))]−ϵ) mathcal{F}( theta, phi, beta; mathbf{x}, mathbf{z})=- mathbb{E}_{ mathbf{z} sim q_ phi( mathbf{z}| mathbf{x})} left[ log p_ theta( mathbf{x}| mathbf{z}) right]+ beta left(D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}) right)]- epsilon right)F(θ,ϕ,β;x,z)=−Ez∼qϕ​(z∣x)​[logpθ​(x∣z)]+β(DKL​(qϕ​(z∣x)∥pθ​(z))]−ϵ) . Since β,ϵ≥0 beta, epsilon geq 0β,ϵ≥0 according to the complementary slackness, we have the β betaβ-VAE Loss: F(θ,ϕ,β;x,z)≥L(θ,ϕ,β;x,z)=−Ez∼qϕ(z∣x)[log⁡pθ(x∣z)]+βDKL(qϕ(z∣x)∥pθ(z))] mathcal{F}( theta, phi, beta; mathbf{x}, mathbf{z}) geq mathcal{L}( theta, phi, beta; mathbf{x}, mathbf{z})=- mathbb{E}_{ mathbf{z} sim q_ phi( mathbf{z}| mathbf{x})} left[ log p_ theta( mathbf{x}| mathbf{z}) right]+ beta D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}) right)]F(θ,ϕ,β;x,z)≥L(θ,ϕ,β;x,z)=−Ez∼qϕ​(z∣x)​[logpθ​(x∣z)]+βDKL​(qϕ​(z∣x)∥pθ​(z))] . β betaβ-VAE Loss . L(θ,ϕ,β;x,z)=−Ez∼qϕ(z∣x)[log⁡pθ(x∣z)]+βDKL(qϕ(z∣x)∥pθ(z))] mathcal{L}( theta, phi, beta; mathbf{x}, mathbf{z})=- mathbb{E}_{ mathbf{z} sim q_ phi( mathbf{z}| mathbf{x})} left[ log p_ theta( mathbf{x}| mathbf{z}) right]+ beta D_ text{KL} left(q_ phi( mathbf{z}| mathbf{x}) |p_ theta( mathbf{z}) right)]L(θ,ϕ,β;x,z)=−Ez∼qϕ​(z∣x)​[logpθ​(x∣z)]+βDKL​(qϕ​(z∣x)∥pθ​(z))] . Observations . Setting β=1 beta=1β=1 corresponds to the original VAE formulation. . | Setting β&gt;1 beta&gt;1β&gt;1 puts a stronger constraint on the latent bottleneck . Limiting the capacity of z mathbf{z}z while trying to maximize the log-likelihood should encourage the model to learn a more efficient representation. | Higher value of β betaβ should encourage the conditional independence in qϕ(z∣x)q_ phi( mathbf{z}| mathbf{x})qϕ​(z∣x) because more weights are put on the DKLD_ text{KL}DKL​ term. | . | Disentangled representation emerge when the right balance is found between reconstruction and latent capacity restriction. . Create a trade-off between reconstruction fidelity and the quality of the disentanglement. | . | Note: In real implementations, β betaβ is usually a training-step dependent variable, from 0 to the set value. The intuition behind this warm-up is to first get the network to be able to learn reconstruction. . | . Measuring Disentanglement - Higgins’ Metric . The basic idea to measure the quality of disentanglement is to have a pair of data points where one factor is fixed while others are sampled randomly. Then, we could let a classifier acts on the difference between their latent representations and see whether the fixed factor could be singled out, and report the classifier accuracy as the disentanglement score. . . Image from 3. | . Results . . Results from β betaβ-VAE3. | . . Results from β betaβ-VAE3. | . . Results from β betaβ-VAE3. | . . Results from β betaβ-VAE3. | . . Results from β betaβ-VAE3. | . The Effect of Tuning β betaβ . β betaβ is a mixing coefficient that weighs the gradients magnitudes between reconstruction and the prior-matching. So it is natural to consider normalized β betaβ in analysis by the latent space dimension MMM and input data dimension NNN, βnorm=βMN beta_ text{norm}= frac{ beta M}{N}βnorm​=NβM​. | β betaβ being too low or too high, the model would learn a entangled representation due to either too much or too little capacity in the latent z mathbf{z}z bottleneck. | Good disentanglement representations often lead to blurry reconstructions. However, in general, β&gt;1 beta&gt;1β&gt;1 is necessary to achieve good disentanglement. | . . Positive correlation is present between the size of z mathbf{z}z and the optimal normalised values of β betaβ for disentangled factor learning for a fixed β betaβ-VAE architecture. Orange approximately corresponds to unnormalized β=1 beta=1β=13. | . How Does It Work? . Shortly, it is not clear from β betaβ-VAE. Thus, we need to investigate why have a large β betaβ penalizing the KL term has such effect. . Decomposing the ELBO More . Quick Mention on Mutual Information (MI) . Let (X,Y)(X,Y)(X,Y) be a pair of r.v.s over the space X×Y mathcal{X} times mathcal{Y}X×Y. Then their mutual information is . I(X;Y)=DKL(p(X,Y)∥p(X)p(Y))I(X;Y)=D_ text{KL}(p(X,Y) |p(X)p(Y))I(X;Y)=DKL​(p(X,Y)∥p(X)p(Y)) | KaTeX parse error: Undefined control sequence: E at position 8: I(X;Y)= ̲E̲_X left[D_ text… | I(X;Y)I(X;Y)I(X;Y) intuitively measures how much could you infer about the other random variable if you are given knowledge about one of them. I(X;Y)=0I(X;Y)=0I(X;Y)=0 means independence because nothing can be inferred (not related at all). . TC-Decomposition . Define a uniform random variable on {1,2,…,N} {1,2, dots,N }{1,2,…,N} with which each data point relates. Denote q(z∣n)=q(z∣xn)q( mathbf{z}|n)=q( mathbf{z}|x_n)q(z∣n)=q(z∣xn​) and q(z,n)=q(z∣n)p(n)=q(z∣n)1Nq( mathbf{z}, n)=q( mathbf{z}|n)p(n)=q( mathbf{z}|n) frac{1}{N}q(z,n)=q(z∣n)p(n)=q(z∣n)N1​. q(z)=∑n=1Nq(z∣n)p(n)q( mathbf{z})= sum_{n=1}^N q( mathbf{z}|n)p(n)q(z)=∑n=1N​q(z∣n)p(n) is the emph{aggregated posterior}. Then, we can decompose the regularization term in the ELBO as . 1N∑n=1NDKL(q(z∣xn)∥p(z))=Ep(n)[DKL(q(z∣n)p(z))]=DKL(q(z∣n)∥p(z))undefinedIndex-Code&nbsp;MI+DKL(q(z)∥∏jq(zj))undefinedTotal&nbsp;Correlation+∑jDKL(q(zj∥p(zj)))undefinedDimension-wise&nbsp;KL begin{align*} &amp; dfrac{1}{N} sum_{n=1}^N D_ text{KL} left(q( mathbf{z}|x_n) |p( mathbf{z}) right) = mathbb{E}_{p(n)} left[D_ text{KL} left(q( mathbf{z}|n)p( mathbf{z}) right) right] &amp;= underbrace{D_ text{KL}(q( mathbf{z}|n) |p( mathbf{z}))}_ text{Index-Code MI} + underbrace{D_ text{KL}(q( mathbf{z}) | prod_j q(z_j))}_ text{Total Correlation} + underbrace{ sum_j D_ text{KL} left(q(z_j |p(z_j) right))}_ text{Dimension-wise KL} end{align*} ​N1​n=1∑N​DKL​(q(z∣xn​)∥p(z))=Ep(n)​[DKL​(q(z∣n)p(z))]=Index-Code&nbsp;MI . DKL​(q(z∣n)∥p(z))​​+Total&nbsp;Correlation . DKL​(q(z)∥j∏​q(zj​))​​+Dimension-wise&nbsp;KL . j∑​DKL​(q(zj​∥p(zj​)))​​​ . The index-code MI is the mutual information Iq(z;n)I_q( mathbf{z};n)Iq​(z;n). It is argued that higher mutual information can lead to better disentanglement, but recent investigations also claim that a penalized one encourages compact and disentangled representations. | The total correlation is one of many generalization of mutual information. It is a measure of dependency between the variables. This is claimed to be the main source of disentanglement. | The dimension-wise KL divergence mainly prevents individual latent dimensions from deviating too far from priors. It acts like a complexity penalty. | . β betaβ-TCVAE Loss . L=−Eq(z∣n)p(n)[log⁡p(n∣z)]+αIq(z;n)+βDKL(q(z)∥∏jq(zj))+γ∑jDKL(q(zj∥p(zj))) mathcal{L}=- mathbb{E}_{q( mathbf{z}|n)p(n)} left[ log p(n| mathbf{z}) right]+ alpha I_q( mathbf{z};n) + beta D_ text{KL}(q( mathbf{z}) | prod_j q(z_j)) + gamma sum_j D_ text{KL} left(q(z_j |p(z_j) right))L=−Eq(z∣n)p(n)​[logp(n∣z)]+αIq​(z;n)+βDKL​(q(z)∥j∏​q(zj​))+γj∑​DKL​(q(zj​∥p(zj​))) . With ablation studies, tuning β betaβ leads to the best results. The proposed model uses α=γ=1 alpha= gamma=1α=γ=1, which is the same object as in FactorVAE7. | Provides better trade-off between density estimation and disentanglement. Different from β betaβ-VAE, higher value of β betaβ would not penalize the mutual information term too much. | . . Ablation study shows that setting α alphaα to zero gives no clear improvement4. | . Estimate Density from Minibatch . Decomposition expression requires the evaluation of the density q(z)=Ep(n)[q(z∣n)]q( mathbf{z})= mathbb{E}_{p(n)} left[q( mathbf{z}|n) right]q(z)=Ep(n)​[q(z∣n)], which depends on the entire dataset. Simple Monte Carlo approximation is not likely to work, so we need weighted sampling. Given a minibach of samples {n1,…,nm} {n_1, dots,n_m }{n1​,…,nm​}, we use the estimator . Eq(z)[log⁡q(z)]≈1M∑i=1M[log⁡1NM∑j=1Mq(z(ni)∣nj)] mathbb{E}_{q( mathbf{z})} left[ log q( mathbf{z}) right] approx dfrac{1}{M} sum_{i=1}^M left[ log dfrac{1}{NM} sum_{j=1}^M q( mathbf{z}(n_i)|n_j) right]Eq(z)​[logq(z)]≈M1​i=1∑M​[logNM1​j=1∑M​q(z(ni​)∣nj​)] . where z(ni)∼q(z∣ni) mathbf{z}(n_i) sim q( mathbf{z}|n_i)z(ni​)∼q(z∣ni​). . Measuring Disentanglement - Mutual Information Gap (MIG) . Higgins’ metric uses an extra classifier, which introduced hyperparameters and more training time. In addition, it cannot meausre axis alignment. Is there a metric based only on the distribution of factors and latent variables? . Mutual Information Gap (MIG) is introduced to solve these problems. Estimate the mutual information between a latent variable ziz_izi​ and a ground truth factor vkv_kvk​ by q(zj,vk)=∑n=1Np(vk)p(n∣vk)q(zj∣n)q(z_j,v_k)= sum_{n=1}^N p(v_k)p(n|v_k)q(z_j|n)q(zj​,vk​)=∑n=1N​p(vk​)p(n∣vk​)q(zj​∣n), and use it in some way. A higher mutual information implies that zjz_jzj​ contains a lot of information about vkv_kvk​. MI is maximal if there exists a deterministic, invertible relationship between zjz_jzj​ and vkv_kvk​. . For each vkv_kvk​, take zj,zlz_j,z_lzj​,zl​ that has the highest and the second highest mutual information with vkv_kvk​. | MIG=1K∑k=1K1H(vk)(I(zj;vk)−I(zl;vk)) text{MIG}= frac{1}{K} sum_{k=1}^K frac{1}{H(v_k)} left(I(z_j;v_k)-I(z_l;v_k) right)MIG=K1​∑k=1K​H(vk​)1​(I(zj​;vk​)−I(zl​;vk​)) | Averaging by KKK and normalizing by the entropy H(vk)H(v_k)H(vk​) provides a value between 0 and 1. MIG→1 text{MIG} rightarrow 1MIG→1 implies good disentanglement. . . Joint distribution between latent variables and ground truth factors. Image Credits to Ricky Chen’s talk at NIPS 2018 | . . Mutual information between latent variables and ground truth factors. Image Credits to Ricky Chen’s talk at NIPS 2018 | . Results . . Results from β betaβ-TCVAE4. | . . Results from β betaβ-TCVAE4. | . . Results from β betaβ-TCVAE4. | . Conclusion . There have been many efforts in different machine learning communities to produce interpretable artificial intelligence systems. Unsupervised learning is a particularly hard task to enforce the interpretability and independence between representations. However, through the exploration and attempt, we have gained more understanding towards its objective (ELBO) and optimization process, and we have many amazing results where the underlying factors are disentangled. . . Yoshua Bengio, Aaron Courville, and Pascal Vincent.Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828, 2013. ↩︎ . | Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017. ↩︎ . | Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, XavierGlorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Tejas D Kulkarni, Will Whitney, Pushmeet Kohli, and Joshua BTenenbaum. Deep convolutional inverse graphics network. arXiv preprint arXiv:1503.03167, 2015. ↩︎ ↩︎ ↩︎ ↩︎ . | Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Proceedings of the 30th International Conference on Neural Information Processing Systems, pages 2180–2188, 2016. ↩︎ . | Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on Machine Learning, pages 2649–2658. PMLR, 2018. ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/28/VAE4_blog2.html",
            "relUrl": "/variational%20autoencoder/2021/09/28/VAE4_blog2.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "VAE4 Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
            "content": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations . This is the best paper [2] in ICML 2019, which incurred huge controversy at that time. It heavily criticizes the previous works on disentanglement, but some claims of it are regarded to be too strong. I will introduce those assumptions challenged by this paper. Although I find some arguments not well supported, most of the conclusions from this paper are actually valuable and inspiring for the later works on the disentanglement. . Introduction to disentanglement . There’s actually no formal definition of disentanglement right now. Intuitively, disentangled representation should be compact and interpretable, where each dimension of the representation is informative and independent. Consider two independent random variables aaa and bbb, then x=[a+b,a−b] mathbf{x}=[a+b,a-b]x=[a+b,a−b] is an entangled representation while x=[a,b] mathbf{x}=[a,b]x=[a,b] is a disentangled representation. These two representations actually contain the same information about aaa and bbb but the disentangled representation is expected to be more interpretable and more useful for downstream tasks, such as controllable sample generation and robot manipulation. . For a long period, many VAE-based methods like β betaβ-VAE [1], with additional tricks to encourage the dimension independence of the latent representation, have been proposed for disentanglement. But all these methods are based on some common assumptions and they are not carefully verified. . Disentanglement is impossible without inductive bias . This paper claims that, for an arbitrary generative model, the disentanglement is actually impossible. For each disentanglement representation zzz, there exists an inifinite family of bijective functions f(z)f(z)f(z), where f(z)f(z)f(z) is entangled but it shares the same marginal distribution with zzz. In other words, there are infinitely many generative models which have the same marginal distribution for the observation xxx, and without inductive bias, there’s no guarantee the one we obtain gives the disentangled representation. This theorem is also similar to the well-known “No free lunch theorem” [9]. Therefore, it’s necessary for each disentanglement method to clearly define its inductive bias. . Challenging the common assumptions behind disentanglement learning . This paper investigates several assumptions behind the disentanglement learning. It considers 6 distanglement methods, including β betaβ-VAE [1], AnnealedVAE [6], FactorVAE [5], β betaβ-TCVAE [3], DIP-VAE-I and DIP-AVE-II [4]. It also uses 6 metrics for measuring disentanglement, including BetaVAE metric [1], FactorVAE metric [5], Mutual Information GAP (MIG) [3], Modularity [7], DCI Disentanglement gap (named as “disentanglement metric” originally) [8], and SAP score [4]. The experiments are conducted on datasets dSprites, Cars3D, SmallNORB, Shapes3D, Color-dSprites, Noisy-dSprites and Scream-dSprites. . Mean representation of the latent variables are correlated . It’s a common practice to use the mean vector of the Gaussian encoder as the representation of the latent variable for evaluation. However, it turns out that although the samples from the Gaussian encoder have uncorrelated dimensions, the mean vector doesn’t internally have this property. Constrained by a stronger regularization, as shown in Fig 1, the total correlation, which measures the correlation among dimensions, of the sampled representation indeed goes down (left) but the total correlation of the mean representation increases (right) instead, except for DIP-VAE-I which directly optimizes the covariance matrix of the mean representation to be diagonal. . Fig 1. Total correlation among dimensions of latent representations, mean representation (left) and sampled representation (right). Source: Locatello et al. [2] Disentanglement metrics are correlated . The second question is whether all these metrics measuring the disentanglement are correlated. And the results give the positive answer. All metrics except Modularity are mildly correlated. . Fig 2. Correlation among metrics. Source: Locatello et al. [2] Importance of models and hyperparameters . All these methods claim that they get a better disentangled representation, but whether the improvement in their metrics is from more disentanglement remains unknown. In the experiment, each model is run over different random seeds, but it turns out that these methods have large overlappings (left in Fig 3) in their performances. In other words, a good random seed is more meaningful than a good objective. The same conclusion holds for the hyperparameter (right in Fig 3). . Fig 3. Violin plots of disentanglement scores over random seeds for different models (left) and different hyperparameters (right). Source: Locatello et al. [2] Recipes for hyperparameter selection . The paper now considers the strategy to select a good hyperparameter for a model. However, all these metrics require a substantial amount of labels or a full generative model, so we need to consider the hyperparameter selection in an unsupervised manner. Unfortunately, no model could dominate others all the time and there does not exist a hyperparameter selection strategy that works consistently well as shown in Fig 4. Additionally, there’s also no strategy to identify a good and a bad run for different random seeds. . Fig 4. Model performances under different hyperparameters on different datasets. Source: Locatello et al. [2] Specifically, the paper investigates the unsupervised losses and transfer performances, which can also serve as a strategy to select the hyperparamter without supervision on the target dataset. For the unsupervised losses, including the reconstruction error, KL divergence between the prior and the approximate posterior, evidence lower bound (ELBO), and the estimated total correlation of the sampled representation, none of them are actually correlated with the disentanglement metrics (Fig 5). . Fig 5. Correlation between disentanglement scores and unsupervised losses. Source: Locatello et al. [2] The transferring fails as well. When the model is transferred across the same metric and same dataset (different random seeds), there&#39;s 80.7% chance the model performance is not worse than the random model selection. However, this result drops to 59.3% for different datasets and further drops to 54.9% when metrics are also different. One example is shown in Fig 6. Fig 6. Model performance after transferring. Source: Locatello et al. [2] Benefits of disentanglement . Finally, this paper explores the benefits of the disentanglement. The disentangled representation is intuitively believed to be more useful for downstream tasks, and able to reduce the sample complexity of learning. In the experiments, the downstream performances show high correlation with the disentanglement scores (Fig 7), but the authors are careful with the conclusion and doubts the source of the correlation, which could be either the disentanglement or the relevant information embedded in the representation. I think the experiments here are incomplete, where authors can actually build entangled representations from the disentangled ones and evaluate the performance of the entangled representations. This comparison could give the idea where the correlation comes from. . Fig 7. Correlation between downstream performances and disentanglement scores. Source: Locatello et al. [2] Besides, the experimental results show no clear correlation between the disentanglement scores and sample efficiencies (Fig 8). Fig 8. Correlation between sample efficiencies and disentanglement scores. Source: Locatello et al. [2] Future directions . This paper proposes three principles for the future work on the disentanglement based on the experiments before. . Inductive biases and implicit and explicit supervision. As proved by this paper, the inductive bias is necessary for the disentangled methods, which should be made clear in the later works. Besides, it’s demonstrated by the experimental results that it’s impossible for the hyperparameter selection under no supervision, the supervision parts should also be explicitly specified. . | Concrete practical benefits of disentangled representations. Previous works take it for grant that disentangled representation is better, however, this paper points out its benefits is not clear yet and quite data dependent. Therefore, the concrete benefits of disentangled representations should be specified under each context. . | Experimental setup and diversity of data sets. It’s shown that no model can consistently outperform others on all datasets, so it’s questionable whether these models really improve the disentanglement. A sound, robust, and reproducible experimental setup on a diverse set of data sets is needed to demonstrate the advantage of a disentangled method. . | References . [1] 2017 (ICLR): I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, A. Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017. | [2] 2019 (ICML): F. Locatello, S. Bauer, M. Lucic, G. RÃ¤tsch, S. Gelly, B. Scholkopf, O. Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. ICML, 2019. | [3] 2018 (NeurIPS): T. Chen, X. Li, R. Grosse, D. Duvenaud. Isolating sources of disentanglement in variational autoencoders. NeurIPS, 2018. | [4] 2018 (ICLR): A. Kumar, P. Sattigeri, A. Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. ICLR, 2018. | [5] 2018 (ICML): H. Kim, A. Mnih. Disentangling by factorising. NIPS, 2017. | [6] 2017 (NIPS): C. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, A. Lerchner. Understanding disentangling in β betaβ-VAE. NIPS, 2017. | [7] 2018 (NIPS): K. Ridgeway, M. Mozer. Learning deep disentangled embeddings with the f-statistic loss. NIPS, 2018. | [8] 2018 (ICLR): C. Eastwood, C. Williams. A framework for the quantitative evaluation of disentangled representations. ICLR, 2018. | [9] 1997 (IEEE): D. Wolpert, W. Macready. No Free Lunch Theorems for Optimization. IEEE Transactions on Evolutionary Computation, 1997. | .",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/28/VAE4_blog.html",
            "relUrl": "/variational%20autoencoder/2021/09/28/VAE4_blog.html",
            "date": " • Sep 28, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "VAE3 Deep Hierarchical",
            "content": ". Introduction . This blogpost talks about deep hierarchical VAEs - why do we need deep (instead of shallow) hierarchy, what are the main challenges, and how do to design them. . It is commonly known that VAE variants generates much less realistic images than GANs. One major hypothesis is that the latent space sturcure is too limited: there is no hierarchy and we try to fit a simple prior (e.g. a standard Gaussian) to it. When we want to increase its capacity while keeping the divergence loss tractable, the idea of hierarchy naturally comes to mind. . Ladder VAE (LVAE) is an early example of hierarchical VAE. In addition to dividing the latent space into multiple layers, it also enables bidirectional (bottom up and top-down) flow of information from layer to layer. The models we will discuss below are based heavily on its design. . . The latter structure alone doesn’t bridge the gap bbetween GANs and VAEs, but we will see below that models based on its design will finally reach a performance comparable to SOTA generative models. . NVAE . Nouveau VAE (NVAE) focuses on improving performance through architecture refinement. The hierarchical architecture is based on LVAE, but with difference scale per layer of latent variables. . . There are four main improvements: . Residual Cells in Encoder and Decoder . Adding residual connection to VAE is easy, but to realize long-range correlation in data is tricky. One straightforward way is to increase the kernel size of convolution layers in the decoder, but then we face the problem of large parameter sizes. To solve this problem, NVAE uses depthwise separable convolution from MobileNetV2 1. . Other components of the residual cells include: . batch normalization (BN) to replace weight normalization (WN) | Swish activation f(u)=u1+exp⁡(−u)f(u) = frac{u}{1 + exp(-u)}f(u)=1+exp(−u)u​ to replace ELU | final Squeeze and Excitation (SE）layer inspired by SENet 2 | . Residual Normal Distribution . Deep hierarchical VAE suffers from the problem of instable KL divergence. Unlike the vanilla VAE which assumes a standard Gaussian prior p(z)=N(0,I)p(z) = mathcal{N}(0, I)p(z)=N(0,I), in each layer of LVAE both the posterior q(zl∣x,z&lt;l)q(z_l mid x, z_{&lt;l})q(zl​∣x,z&lt;l​) and the prior p(zl∣z&lt;l)p(z_l mid z_{&lt;l})p(zl​∣z&lt;l​) are generated from previous layers, making it very challenging to match the two distributions. . This problem can be solved via reparametrizing the posterior using residual distribution Δμ Delta muΔμ and Δσ Delta sigmaΔσ. That is, q(zl∣z&lt;l,x)=N(μ(z&lt;l)+Δμ(z&lt;l,x),σ(z&lt;l)⋅Δσ(z&lt;l,x))q(z_l mid z_{&lt;l}, x) = mathcal{N}( mu(z_{&lt;l}) + Delta mu(z_{&lt;l}, x), sigma(z_{&lt;l}) cdot Delta sigma(z_{&lt;l},x))q(zl​∣z&lt;l​,x)=N(μ(z&lt;l​)+Δμ(z&lt;l​,x),σ(z&lt;l​)⋅Δσ(z&lt;l​,x)). . Under this parametrization, when the prior p(zl∣z&lt;l)p(z_l mid z_{&lt;l})p(zl​∣z&lt;l​) changes, the posterior q(zl∣z&lt;l,x)q(z_l mid z_{&lt;l}, x)q(zl​∣z&lt;l​,x) moves accordingly. . Another way to interpret this change is to derive the expression for KL divergence: DKL(q(z∣x),p(z))=12(Δμ2σ2+Δσ2−log⁡Δσ2−1)D_{KL}(q(z mid x), p(z)) = frac{1}{2}( frac{ Delta mu^2}{ sigma^2} + Delta sigma^2 - log Delta sigma^2 - 1)DKL​(q(z∣x),p(z))=21​(σ2Δμ2​+Δσ2−logΔσ2−1). When σ2 sigma^2σ2 is bounded from below, the term is mainly dependent on the encoder’s output, and therefore it’s easier to minimize KL divergence compared to the standard parametrization, where KLD is dependent on the outputs of both the encoder and the decoder. . Spectral Regularization (SR) . To ensure that the encoder doesn’t produce drastically different latent codes when the input slightly changes, it would be nice if it is made Lipschitz. To achieve this end LSR=λ∑is(i) mathcal{L}_{SR} = lambda sum_i s^{(i)}LSR​=λ∑i​s(i) is added as a regularization loss, where s(i)s^{(i)}s(i) is the largest singular value of the iii-th convolution layer. Spectral regularization 3 is shown to minimize the Lipschitz constant for each layer. . Normalization Flows (NFs) for Generating Posterior . Finally, n order to further increase expressivity of the posterior distributions, a few IAF 4 layers are appended to the encoder. . Very Deep Hierarchical VAE . In the other paper 5, the authors claim that deep hierarchical VAEs generalize autoregressive models such as PixelRNN 6. The claims is proven in two theorems: . Proposition 1. N-layer VAEs generalize autoregressive models when N is the data dimension. Proposition 2. N-layer VAEs are universal approximators of N-dimensional latent densities. . The first proposition can be intuitively understood through the following figure, while the second proposition trivially follows it if we admit that image domains usually have lower rank than their resolutions. . . Therefore, a hierarchical VAE that is deep enough should be able to reach the same level of sample quality as autoregressive models. The paper uses a slightly different variant of LVAE (but deeper), but also with multi-scale layers and residual blocks. . . Experiments and Conclusion . The very deep hierarchical VAE paper provides a thorough investigation of different VAE as well as autoregressive models: . | Dataset | Model | Model Type | NLL | | CIFAR-10 | Sparse Transformer | AR | 2.80 | | CIFAR-10 | Flow++ | Flow | ≤ le≤ 3.08 | | CIFAR-10 | NVAE | VAE | ≤ le≤ 2.91 | | CIFAR-10 | Very Deep VAE | VAE | ≤ le≤ 2.87 | | ImageNet-32 | Image Transformer | AR | 3.77 | | ImageNet-32 | Flow++ | Flow | ≤ le≤ 3.86 | | ImageNet-32 | NVAE | VAE | ≤ le≤ 3.92 | | ImageNet-32 | Very Deep VAE | VAE | ≤ le≤ 3.80 | | ImageNet-64 | Sparse Transformer | AR | 3.44 | | ImageNet-64 | Flow++ | Flow | ≤ le≤ 3.69 | | ImageNet-64 | Very Deep VAE | VAE | ≤ le≤ 3.52 | | FFHQ-256 | NVAE | VAE | ≤ le≤ 0.68 | | FFHQ-256 | Very Deep VAE | VAE | ≤ le≤ 0.61 | | FFHQ-1024 | Very Deep VAE | VAE | ≤ le≤ 2.42 | . In conclusion, Very Deep VAE is able to reach the same level of sample quality as SOTA autoregressive and NF methods. Also it can be trained on high-resolution datasets, which no previous VAE has attempted. . Although it seems trivial to expand LVAE architecture to a larger scale, successfully doing so actually requires paying attention to various details. The proof that VAEs have the capacity to generate images as good as autoregressive models or NFs do is also meaningful and inspiring for future research. . Footnotes . . See MobileNetV2: Inverted Residuals and Linear Bottlenecks. ↩︎ . | See Squeeze-and-Excitation Networks. ↩︎ . | See Spectral Norm Regularization for Improving the Generalizability of Deep Learning. ↩︎ . | See Improved Variational Inference with Inverse Autoregressive Flow。 ↩︎ . | See Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. ↩︎ . | See Pixel Recurrent Neural Networks. ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/23/VAE3_blog.html",
            "relUrl": "/variational%20autoencoder/2021/09/23/VAE3_blog.html",
            "date": " • Sep 23, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "VAE2 Improved Inference, Representation",
            "content": "Ladder VAE . Introduction . Ladder VAE was introduced in 2016, just after the introduction of VAE. And the purpose of LVAE was to explore how we can change the variational inference part of VAE to improve the performance without changing the generative model. It recursively corrects the generative distribution by a data dependent approximate likelihood. . Review of VAE . VAE models are made by two parts: the inference part and the generative part. The inference part takes an observation X and learns a latent representation of the input X and outputs a posterior distribution, usually Gaussian. Variational here means that the posterior learnt is approximate since the actual posterior distribution is not observable. The generative part takes a sample z from the latent posterior distribution learnt by the inference part and then learns to reconstruct the original observation X corresponding to this latent representation. And this is called the encoder-decoder structure. . The Problem . VAE model has some nice properties, for example, VAE is highly expressive, which means that they can learn a pretty good latent representation and generate vivid samples. VAE is also flexible and computationally efficient in most cases. . But due to the hierarchies of conditional stochastic variables, it is difficult to optimize when the model gets deep. . The paper found that purely bottom-up inference normally used in VAEs and gradient ascent optimization are only to a limited degree able to utilize the two layers of stochastic latent variables, e.g. If you train a vanilla VAE with 5 layers, you will see that only the first two layers learn something, and the other layers are all inactive. . Thus, previous work on VAEs have been restricted to shallow models with one or two layers of stochastic latent variables. The performance of such models is constrained by the restrictive mean field approximation to the intractable posterior distribution. There are evidences suggesting that a more complex model often means better performance, and the research conducted here is following that direction which eventually becomes a cornerstone for further researches on deep VAE models like β betaβ-VAE framework. . Main Contribution . The paper’s main contribution is on: . Investigated into the inability of oridnary VAE to train deep hierachical stochastic layers. . | Proposed Ladder VAE architecture to support deep hierarchical encoder, proposed LVAE which changes a bit of the inference model. . | Verified the importance of BatchNormalization(BN) and Warm-Up(WU) to VAE. Made comparisons on VAE models with/without BN and WU to see the influence of BN and WU on the models. And turns out these techniques are essential for good performance. . | Model Architecture . Ladder VAE model combines the approximate Gaussian likelihood with the generative model. According to the authors, ordinary VAE has no information sharing between encoder and decoder and this might be a bottleneck for learning a consistent posterior distribution. Thus, the proposed Ladder VAE model adds information sharing between the inference part and generative part. As the illustration below, Ladder VAE model (right) added deterministic upward nodes. Then during the stochastic downward pass, parameters are shared between the inference part and the generative part, while ordinary VAE model (left) has no information sharing between the two latent models. . To perform a forward pass for the encoder, there would first be a deterministic upward pass computes the approximate likelihood contribution, followed by a stochastic downward pass recursively computing both the approximate posterior and generative distributions. . The approximate posterior distribution can be viewed as merging information from a bottom up computed approximate likelihood with top-down prior information from the generative distribution, . The sharing of information (and parameters) with the generative model gives the inference model knowledge of the current state of the generative model in each layer and the top down-pass recursively corrects the generative distribution with the data dependent approximate log-likelihood using a simple precision-weighted addition. . Objective Function . Ladder VAE model also uses Evidence Lower Bound (ELBO) as objective function: log⁡p(x)≥Eqϕ(Z∣X)[log⁡pθ(x,z)qϕ(Z∣X)]=L(θ,ϕ;x) log p(x) ge E_{q_ phi(Z|X)}[ log frac{p_ theta(x,z)}{q_ phi(Z|X)}]=L( theta, phi;x)logp(x)≥Eqϕ​(Z∣X)​[logqϕ​(Z∣X)pθ​(x,z)​]=L(θ,ϕ;x) =−βKL(qϕ(z∣x)∣∣pθ(z))+Eqϕ(Z∣X)(log⁡pθ(x∣z))=- beta KL(q_ phi(z|x)||p_ theta(z))+E_{q_ phi(Z|X)}( log p_ theta(x|z))=−βKL(qϕ​(z∣x)∣∣pθ​(z))+Eqϕ​(Z∣X)​(logpθ​(x∣z)) where KLKLKL here is the KL-diveregence . Notice that there is an extra β betaβ term in front of the KL-divergence term. This is what so called Warm-Up, which increases from 0 to 1 gradually during training time. The purpose of such “slow start” is to prevent high order layers of Ladder VAE from overfitting in early stage of training.By gradually introduce the KL-divergence, which is the variational regularization term used for regularizing the approximate posterior for each unit towards its own prior, the modified ELBO would start with reconstruction error term only and give high order layers sometime to learn useful information instead of ignoring them all before they can learn anything useful. . Generative Architecture . The generative part is the same for both VAE and Ladder VAE pθ(z)=pθ(zL)∏i=1L−1pθ(zi∣zi+1)p_ theta(z)=p_ theta(z_L) prod_{i=1}^{L-1}p_ theta(z_i|z_{i+1})pθ​(z)=pθ​(zL​)∏i=1L−1​pθ​(zi​∣zi+1​) pθ(zi∣zi+1)=N(zi∣μp,i(zi+1),σi+12(zi+1))p_ theta(z_i|z_{i+1})=N(z_i| mu_{p,i}(z_{i+1}), sigma^2_{i+1}(z_{i+1}))pθ​(zi​∣zi+1​)=N(zi​∣μp,i​(zi+1​),σi+12​(zi+1​)) pθ(zL)=N(zL∣0,I)p_ theta(z_L)=N(z_L|0,I)pθ​(zL​)=N(zL​∣0,I) pθ(x∣z1)=N(x∣μp,0(z1),σp,02(z1))p_ theta(x|z_1)=N(x| mu_{p,0}(z_1), sigma^2_{p,0}(z_1))pθ​(x∣z1​)=N(x∣μp,0​(z1​),σp,02​(z1​)) . Inference Architecture . For VAE model KaTeX parse error: Unexpected character: &#39;�&#39; at position 1: �̲�(𝑦)=MLP(𝑦) KaTeX parse error: Unexpected character: &#39;�&#39; at position 1: �̲�(𝑦)=Linear(𝑑… KaTeX parse error: Unexpected character: &#39;�&#39; at position 11: sigma^2 (�̲�)=Softplus(Lin… . qϕ(z∣x)=qϕ(z1∣x)∏i=2Lqϕ(zi∣zi−1)q_ phi(z|x)=q_ phi(z_1|x) prod_{i=2}^Lq_ phi(z_i|z_{i-1})qϕ​(z∣x)=qϕ​(z1​∣x)∏i=2L​qϕ​(zi​∣zi−1​) qϕ(z1∣x)=N(z1∣μq,1(x),σq,12(x))q_ phi(z_1|x)=N(z_1| mu_{q,1}(x), sigma^2_{q,1}(x))qϕ​(z1​∣x)=N(z1​∣μq,1​(x),σq,12​(x)) qϕ(zi∣zi−1)=N(zi∣μq,i(zi−1),σq,i2(zi−1)),i=2…Lq_ phi(z_i|z_{i-1})=N(z_i| mu_{q,i}(z_{i-1}), sigma^2_{q,i}(z_{i-1})),i=2 ldots Lqϕ​(zi​∣zi−1​)=N(zi​∣μq,i​(zi−1​),σq,i2​(zi−1​)),i=2…L . For Ladder VAE model dn=MLP(dn−1),d0=xd_n=MLP(d_{n-1}),d_0=xdn​=MLP(dn−1​),d0​=x μ^q,i=Linear(di),i=1…L hat mu_{q,i}=Linear(d_i),i=1 ldots Lμ^​q,i​=Linear(di​),i=1…L σ^q,i2=Softplus(Linear(di)),i=1…L hat sigma^2_{q,i}=Softplus(Linear(d_i)),i=1 ldots Lσ^q,i2​=Softplus(Linear(di​)),i=1…L σq,i=1σ^q,i−2+σp,i−2 sigma_{q,i}= frac{1}{ hat sigma^{-2}_{q,i}+ sigma^{-2}_{p,i}}σq,i​=σ^q,i−2​+σp,i−2​1​ μq,i=μ^q,iσ^q,i−2+μp,iσp,i−2σ^q,i−2+σp,i−2 mu_{q,i}= frac{ hat mu_{q,i} hat sigma^{-2}_{q,i}+ mu_{p,i} sigma^{-2}_{p,i}}{ hat sigma^{-2}_{q,i}+ sigma^{-2}_{p,i}}μq,i​=σ^q,i−2​+σp,i−2​μ^​q,i​σ^q,i−2​+μp,i​σp,i−2​​ σq,L=σ^q,L,μq,L=μ^q,L sigma_{q,L}= hat sigma_{q,L}, mu_{q,L}= hat mu_{q,L}σq,L​=σ^q,L​,μq,L​=μ^​q,L​ qϕ(Zi∣⋅)=N(zi∣μq,i,σq,i2)q_ phi(Z_i| cdot)=N(z_i| mu_{q,i}, sigma^2_{q,i})qϕ​(Zi​∣⋅)=N(zi​∣μq,i​,σq,i2​) . Experimental Results . The paper conducted experiments on both MNIST dataset and OMNIGLOT dataset and here I show the main results. . Results on MNIST dataset Results on OMNIGLOT dataset Samples from both datasets. The left part of the image is a illustration of sample reconstruction for Ladder VAE, where the left most image is ground truth and the middle image is the reconstructed image. On the right, the top part are samples drawn from MNIST dataset and the bottom part are samples drawn from OMNIGLOT dataset. . They also recorded the log-likelihood for each layer in throughout the training to compare the number of active units in each layer at each timestep. And from the plot one can see that ordinary VAE can’t train layers above 2 while Ladder VAE model and VAE model+BN+WU have significantly more active units in high order layers. Layer-wise PCA analysis has shown that Ladder VAE model is able to learn much more useful information in high order layers than ordinary VAE model. . Vector-Quantized VAE . Vector Quantized VAE (VQ-VAE) aims to train an expressive VAE model using discrete latent space. . Motivation and Approach . VQ-VAE borrowed ideas from Vector Quantization in compression algorithms, which is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms. . Thus, the posterior now becomes: where zez_eze​ is the ordinary continuous latent code given by encoder, eje_jej​ is a discrete latent embedding. So that VQ-VAE is essentially adding an extra quantization layer between ordinary VAE encoder and decoder. . Main Contribution . The main contribution of VQ-VAE is they adapted a discrete latent space (their encoder generates discrete latent codes), so that their model achieves extrodinary dimension reduction while maintaining a good performance. By using discrete latent codes, VQ-VAE has a smaller variance and they also managed to circumvent the problem of posterior collapse which happens when decoder ignores samples from posterior when it’s too weak or too noisy. . Objective . The objective function of VQ-VAE is made up by three parts: L=log⁡p(x∣zq(x))+∣∣sg[ze(x)]−e∣∣22+β∣∣ze(x)−sg[e]∣∣22L= log p(x|z_q(x))+||sg[z_e(x)]-e||^2_2+ beta||z_e(x)-sg[e]||^2_2L=logp(x∣zq​(x))+∣∣sg[ze​(x)]−e∣∣22​+β∣∣ze​(x)−sg[e]∣∣22​ where sgsgsg stands for stop gradient operator which is defined as identity during forward pass and has zero partial derivative, thus constraining its operand to be a non-updated constant. . Here, the first term is reconstruction Loss which optimizes encoder-decoder. The second term is VQ Objective which Learns the latent embedding. And the last term is commitment loss, used to make sure the encoder commits to an embedding and its output does not grow. Since the volume of the embedding space is dimensionless, it can grow arbitrarily if the embeddings eje_jej​ do not train as fast as the encoder parameters . Experiments . The paper conducted experiments on image, audio, and video stream and here I show some results for images. . Below is an example of image reconstruction, the top is ground truth and the bottom is reconstructed image. . Below are exmaples of sampled images from VQ-VAE. . Reference . Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. Ladder Variational Autoencoders. Neural Information Processing Systems, 2016 | | .",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/21/VAE2_blog2.html",
            "relUrl": "/variational%20autoencoder/2021/09/21/VAE2_blog2.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "VAE2 Diagnosing and Enhancing VAE Models",
            "content": "Diagnosing and Enhancing VAE Models (ICLR &#39;19)1 . Introduction . Even though variational autoencoders (VAEs)2 have a wide variety applications in deep generative models, many aspects of the underlying energy function remain poorly understod. It is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. . In this paper, the authors rigorously analyzed that reaching the global optimum does not guarantee that if VAE model can learn the true distribution of data, i.e., there could exist alternative solutions that both reach the global optimum and yet do not assign the same probability measure as ground-truth probability distribution. And it also proposed a two-stage remedy model, i.e., a two-stage VAE model to address the above issues and enhance the original VAE so that any gloablly minimizing solution is uniquely matched to the ground-truth distribution. . Problem Definition: . The starting point is the desire to learn a probabilistic generative model of observable variables x∈Xx in mathcal Xx∈Xwhere X mathcal XX is a r-dimensional manifold embedded in Rd mathbb R ^dRd | Denote a ground-truth probability measure on X mathcal{X}X as μgt mu_{gt}μgt​ where ∫Xμgtdx=1 int_{ mathcal{X}} mu_{gt} d mathbf{x} = 1∫X​μgt​dx=1 | The canonical VAE attempts to approximate this ground-truth measure using parameterized density pθ(x)p_{ theta}( mathbf{x})pθ​(x) where pθ(x)=∫pθ(x∣z)p(z)dzp_{ theta}(x) = int p_{ theta}(x | z) p(z) dzpθ​(x)=∫pθ​(x∣z)p(z)dz, z∈Rκz in mathbb{R}^ kappaz∈Rκ with κ≈r kappa approx rκ≈r and p(z)=N(z∣0,I)p(z) = mathcal{N}(z | 0, mathbf{I})p(z)=N(z∣0,I) | . We will consider two situations where r&lt;dr &lt; dr&lt;d and r=dr = dr=d to illustrate the aforementioned non-uniqueness issues. . VAE Objective . In the vanilla VAE model, we normally write the objective function to be optimized as evidence lower bound (ELBO): Lθ,ϕ(x)=−Eqϕ(z∣x)[log⁡pθ(x,z)−log⁡qϕ(z∣x)]=KL(qϕ(z∣x)∣∣pθ(z))+Eqϕ(z∣x)[−log⁡pθ(x,z)] begin{align*} mathcal{L}_{ theta, phi}(x) &amp; = - mathbb{E}_{q_{ phi}(z|x)}[ log p_{ theta}(x, z) - log q_{ phi}(z|x)] &amp; = mathbb{KL}(q_{ phi}(z|x) || p_{ theta}(z)) + mathbb{E}_{q_{ phi}(z|x)}[- log p_{ theta}(x, z)] end{align*}Lθ,ϕ​(x)​=−Eqϕ​(z∣x)​[logpθ​(x,z)−logqϕ​(z∣x)]=KL(qϕ​(z∣x)∣∣pθ​(z))+Eqϕ​(z∣x)​[−logpθ​(x,z)]​ | In this case, based on the ground-truth probability measure μgt mu_{gt}μgt​, we can rewrite it into: Lθ,ϕ(x)=∫X{−log⁡pθ(x)+KL[qϕ(z∣x)∣∣pθ(z∣x)]}μgtdx≥∫X−log⁡pθ(x)μgtdxLθ,ϕ(x)=∫X{−Eqϕ(z∣x)[log⁡pθ(z∣x)]+KL[qϕ(z∣x)∣∣p(z)]}μgtdx begin{align*} mathcal{L}_{ theta, phi}(x) &amp; = int_{ mathcal{X}} {- log p_{ theta}(x) + mathbb{KL}[q_{ phi}(z|x) || p_{ theta}(z|x)] } mu_{gt} dx geq int_{ mathcal{X}} - log p_{ theta}(x) mu_{gt} dx mathcal{L}_{ theta, phi}(x) &amp; = int_{ mathcal{X}} {- mathbb{E}_{q_{ phi}(z|x)} [ log p_{ theta}(z|x)] + mathbb{KL}[q_{ phi}(z|x) || p(z)] } mu_{gt} dx end{align*}Lθ,ϕ​(x)Lθ,ϕ​(x)​=∫X​{−logpθ​(x)+KL[qϕ​(z∣x)∣∣pθ​(z∣x)]}μgt​dx≥∫X​−logpθ​(x)μgt​dx=∫X​{−Eqϕ​(z∣x)​[logpθ​(z∣x)]+KL[qϕ​(z∣x)∣∣p(z)]}μgt​dx​ | In principle, qϕ(z∣x)q_{ phi}(z|x)qϕ​(z∣x) and pθ(x∣z)p_{ theta}(x|z)pθ​(x∣z) can be arbitrary distributions. In the practical implementation, a commonly adopted distributional assumption is that both distribution are Gaussian, which was previously considered as a limitation of VAE. | . Diagnosing the Non-uniqueness . Ideas: Even with the stated Gaussian distributions, there exist parameters θ,ϕ theta, phiθ,ϕ that can simultaneously: . Globally optimize the VAE object | Recover the ground-truth probability measure in a certain sense | Definition 1: A κ kappaκ-simple VAE is defined as a VAE model with dim[z mathbf{z}z] = κ kappaκ latent dimensions, the Gaussian encoder qϕ(z∣X)=N(z∣μz,Σz)q_{ phi}(z|X) = mathcal{N}(z | mu_z, Sigma_z)qϕ​(z∣X)=N(z∣μz​,Σz​) and the Gaussian decoder pθ(x∣z)=N(x∣μx,Σx)p_{ theta}(x|z) = mathcal{N}(x | mu_x, Sigma_x)pθ​(x∣z)=N(x∣μx​,Σx​) With these definitions, we can now move to the discussion of κ kappaκ-simple VAE with κ≥r kappa geq rκ≥r can achieve the above optimality criteria from the simpler case where r=dr = dr=d followed by the extended scenario with r&lt;dr &lt; dr&lt;d. . When r=d . Assuming pgt(x)=μgt(dx)/dxp_{gt}(x) = mu_{gt}(dx) / dxpgt​(x)=μgt​(dx)/dx exists everywhere in Rd mathbb{R}^dRd, the minimal possible value of negative log-likelihood will necessarily occur if KL[qϕ(z∣x)∣∣pθ(z∣x)]=0&nbsp;and&nbsp;pθ(x)=pgt(x)&nbsp;almost&nbsp;everywhere mathbb{KL}[q_{ phi}(z|x) || p_{ theta}(z|x)] = 0 text{ and } p_{ theta}(x) = p_{gt}(x) text{ almost everywhere}KL[qϕ​(z∣x)∣∣pθ​(z∣x)]=0&nbsp;and&nbsp;pθ​(x)=pgt​(x)&nbsp;almost&nbsp;everywhere Naturally we will conclude that . Theorem 2: Suppose that r=dr=dr=d and there exists a density pgt(x)p_{gt}(x)pgt​(x) associated with the ground-truth measure μgt mu_{gt}μgt​ that is nonzero everywhere on Rd mathbb{R}^dRd. Then for any κ≥r kappa geq rκ≥r, there is a sequence of κ kappaκ-simple VAE model parameters {θt⋆,ϕt⋆} { theta_t^ star, phi_t^ star }{θt⋆​,ϕt⋆​} such that lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&nbsp;and&nbsp;lim⁡t→∞pθt⋆(x)=pgt(x)&nbsp;almost&nbsp;everywhere lim_{t to infty} mathbb{KL}[q_{ phi_t^ star}(z|x) || p_{ theta_t^ star}(z|x)] = 0 text{ and } lim_{t to infty} p_{ theta_t^ star}(x) = p_{gt}(x) text{ almost everywhere} t→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&nbsp;and&nbsp;t→∞lim​pθt⋆​​(x)=pgt​(x)&nbsp;almost&nbsp;everywhere The theorem implies that as long as latent dimension is sufficiently large (i.e., κ≥r kappa geq rκ≥r), the optimal ground-truth probability measure can be recovered, whether the encoder and decder has Gaussian assumptions or not, since the ground-truth probability measure being recovered almost everywhere is the necessary conditions for optimized objective value. . When r &lt; d . When both qϕ(z∣x)q_ phi(z|x)qϕ​(z∣x) and pθ(x∣z)p_{ theta}(x|z)pθ​(x∣z) are arbitrary/unconstrained, i.e., without Gaussian assumptions, then inf⁡ϕ,θL(θ,ϕ)=−∞ inf_{ phi, theta} mathcal{L}( theta, phi) = - inftyinfϕ,θ​L(θ,ϕ)=−∞ by forcing qϕ(z∣x)=pθ(z∣x)q_{ phi}(z|x) = p_{ theta}(z|x)qϕ​(z∣x)=pθ​(z∣x). | To show that this does not need to happen, define a manifold density p~gt(x) tilde p_{gt}(x)p~​gt​(x) as the probability density of μgt mu_{gt}μgt​ with respect to the volume measure of the manifold X mathcal{X}X. If d=rd = rd=r then this volume is the standard Lebesgue measure in Rd mathbb{R}^dRd and p~gt(x)=pgt(x) tilde p_{gt}(x) = p_{gt}(x)p~​gt​(x)=pgt​(x) since when r&lt;dr &lt; dr&lt;d, pgt(x)p_{gt}(x)pgt​(x) may not exist everywhere in the ambient space. | . Theorem 3: Assume r&lt;dr &lt; dr&lt;d and that there exists a manifold density p~gt(x) tilde p_{gt}(x)p~​gt​(x) associated with the ground-truth measure μgt mu_{gt}μgt​ that is nonzero everywhere on X mathcal{X}X. Then for any κ≥r kappa geq rκ≥r, there is a sequence of κ kappaκ-simple VAE model parameters {θt⋆,ϕt⋆} { theta_t^ star, phi_t^ star }{θt⋆​,ϕt⋆​} such that . lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&nbsp;and&nbsp;lim⁡t→∞∫X−log⁡pθt⋆(x)μgtdx=−∞ lim_{t to infty} mathbb{KL}[q_{ phi_t^ star}(z|x) || p_{ theta_t^ star}(z|x)] = 0 text{ and } lim_{t to infty} int_{ mathcal{X}} - log p_{ theta_t^ star}(x) mu_{gt} dx = - inftyt→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&nbsp;and&nbsp;t→∞lim​∫X​−logpθt⋆​​(x)μgt​dx=−∞ | lim⁡t→∞∫X∈Apθt⋆(x)dx=μgt(A∪X) lim_{t to infty} int_{ mathcal{X} in A} p_{ theta_t^ star} (x) dx = mu_{gt} (A cup mathcal{X})t→∞lim​∫X∈A​pθt⋆​​(x)dx=μgt​(A∪X) for all measurable sets A⊆RdA subseteq mathbb{R}^dA⊆Rd with μgt(∂A∪X)=0 mu_{gt}( partial A cup mathcal{X}) = 0μgt​(∂A∪X)=0 where ∂A partial A∂A is the boundary of AAA. | . Implications of this theorem: . From (1), the VAE Gaussian assumptions do not prevent minimization of L(θ,ϕ) mathcal{L}( theta, phi)L(θ,ϕ) from converging to minus infinity. | From (2), there exists solutions that assign a probability mass to most all measurable subsets of Rd mathbb{R}^dRd that is distinguishable from the ground-truth measure. | In r=dr = dr=d situation, the theorem necessitates that the ground-truth probability measure has been recovered almost everywhere. | In r&lt;dr &lt; dr&lt;d situation, we have not ruled out the possibility that a different set of parameters {θ,ϕ} { theta, phi }{θ,ϕ} can push the lost to −∞- infty−∞ and not achieve (2), i.e., the VAE can reach the lower bound of negative log-likelihood but fail to closely approximate μgt mu_{gt}μgt​. | . Optimal Solutions . The necessary conditions for VAE optimal value would be induced from the following theorems. . Theorem 4: Let {θγ⋆,ϕγ⋆} { theta^ star_ gamma, phi_ gamma^ star }{θγ⋆​,ϕγ⋆​} denote an optimal κ kappaκ-simple VAE solution (with κ≥r kappa geq rκ≥r) where the decoder variance γ gammaγ is fixed. Moreover, we assume that μgt mu_{gt}μgt​ is not a Gaussian distribution when d=rd = rd=r. Then for any γ&gt;0 gamma &gt; 0γ&gt;0, there exists a γ′&lt;γ gamma&#39; &lt; gammaγ′&lt;γ such that L(θγ′⋆,ϕγ′⋆)&lt;L(θγ⋆,ϕγ⋆) mathcal{L}( theta_{ gamma&#39;}^ star, phi_{ gamma&#39;}^ star) &lt; mathcal{L}( theta_{ gamma}^ star, phi_{ gamma}^ star)L(θγ′⋆​,ϕγ′⋆​)&lt;L(θγ⋆​,ϕγ⋆​) . The theorem implies that if γ gammaγ is not constrained, it must be that γ→0 gamma to 0γ→0 if we wish to minimize the VAE objective. While in existing practical VAE applications, it is standard to fix γ≈1 gamma approx 1γ≈1 with the standard Gaussian assumptions during training. . Theorem 5: Applying the same conditions and definitions in Theorem 4, then for all xxx drawn from μgt mu_{gt}μgt​, we also have that lim⁡γ→0fμx[fμz(x;ϕγ⋆)+fSz(x;θγ⋆)ϵ;ϕγ⋆]=lim⁡γ→0fμx[fμz(x;ϕγ⋆);θγ⋆]=x,∀ϵ∈Rκ lim_{ gamma to 0} f_{ mu_x} [f_{ mu_z}(x; phi_{ gamma}^ star) + f_{S_z}(x; theta{ gamma}^ star) epsilon; phi_ gamma^ star] = lim_{ gamma to 0} f_{ mu_x}[f_{ mu_z}(x; phi_ gamma^ star); theta_ gamma^ star] = x, forall epsilon in mathbb{R}^ kappaγ→0lim​fμx​​[fμz​​(x;ϕγ⋆​)+fSz​​(x;θγ⋆)ϵ;ϕγ⋆​]=γ→0lim​fμx​​[fμz​​(x;ϕγ⋆​);θγ⋆​]=x,∀ϵ∈Rκ . With this theorem, it indicates that any x∈X mathbf{x} in mathcal{X}x∈X will be perfectly reconstructed by the VAE model at globally optimal solutions. | Adding dimensions to latent dimension cannot improve the value of the VAE data term in meaningful way. In the training process, there are likely to be rrr eigenvalues of the decoder covariance converging to 0 and κ−r kappa - rκ−r converging to one. This demonstrats that VAE has the ability to detect the manifold dimension and select the proper number of latent dimensionsin practical environments. | If VAE model parameters have learned a near optimal mapping onto X mathcal{X}X using γ≈0 gamma approx 0γ≈0, then the VAE cost will scale as (d−r)log⁡γ(d - r) log gamma(d−r)logγ regardless of μgt mu_{gt}μgt​. | . Two-Stage VAE Model . The above analysis suggests the following two-stage remedy: . Given nnn observed samples {x(i)}i=1n {x^{(i)} }^n_{i=1}{x(i)}i=1n​, train a κ kappaκ-simple VAE, with κ≥r kappa geq rκ≥r, to estimate the unknown rrr-dimensional ground-truth manifold X mathcal{X}X embedded in Rd mathcal{R}^dRd using a minimal number of active latent dimensions. Generate latent samples {z(i)}i=1n {z^{(i)} }^n_{i=1}{z(i)}i=1n​ via z(i)∼qϕ(z∣x(i))z_{(i)} sim q_{ phi}(z|x^{(i)})z(i)​∼qϕ​(z∣x(i)). | Train a second κ kappaκ-simple VAE, with independent parameters {θ′,ϕ′} { theta&#39;, phi&#39; }{θ′,ϕ′} and latent representation uuu, to learn the unknown distribution qϕ(z)q_ phi(z)qϕ​(z) as a new ground-truth distribution and use samples {z(i)}i=1n {z^{(i)} }^n_{i=1}{z(i)}i=1n​ to learn it. | Samples approximating the original ground-truth μgt mu_{gt}μgt​ can then be formed via the extended ancestral process u∼N(u∣0,I),z∼pθ′(z∣u),x∼pθ(x∣z)u sim mathcal{N}(u | 0, mathbf{I}), z sim p_{ theta&#39;}(z | u), x sim p_{ theta}(x|z)u∼N(u∣0,I),z∼pθ′​(z∣u),x∼pθ​(x∣z) | The structure of the first-stage of the Two-Stage VAE Model . Analysis: . If the first stage was successful, then even though they will not generally resemble N(z∣0,I) mathcal{N}(z|0, mathbf{I})N(z∣0,I), samples from qϕ(z)q_ phi(z)qϕ​(z) will have nonzero measure across the full ambient space Rκ mathbb{R}^ kappaRκ. | If κ&gt;r kappa &gt; rκ&gt;r, then the extra latent dimensions will be naturally filled in via randomness. | Consequently, as long as we set κ≥r kappa geq rκ≥r, the operational regime of the second-stage VAE is effectively equivalent to the situation that the manifold dimension is equal to the ambient dimension, and reaching global optimum solutions would recover the ground-truth probability measure almost everywhere. | . Experiment Results . The following table indicates the performance evaluation results of the experiments conducted on four significantly different datasets: MNIST, Fash-ion MNIST, CIFAR-10 and CelebA. The evaluation metrics used Frchet Inception Distance (FID)3 Score: used to assess the quality of images created by a generative model, comparing the generated images with the distribution of real images. Note: The training of two stages need to be separate. Concatenating two stages and jointly training does not improve the performance. . Another set of experiments were conducted on the same datasets with different evaluation metrics. Kernel Inception Distance (KID)4 applies a polynomial-kernel Maximum Mean Discrepancy (MMD) measure to estimate the inception distance, as FID score is believed to exhibit bias in certain circumstances. . Analysis of the Results: . The second stage of Two-Stage VAE model can reduce the gap between q(z)q(z)q(z) and p(z)p(z)p(z), resulting in better manifold reconstruction. | γ gammaγ will converge to zero at any global minimum of the VAE objective, allowing for tighter image reconstructions with better manifold fit. | . . Contributions and Conclusions . This paper rigorously proved that VAE global optimum can in fact uniquely learn a mapping to the correct ground-truth manifold when r&lt;dr &lt; dr&lt;d, but not necessarily the correct probability measure within this manifold. | The proposed Two-Stage VAE model can resolve this issue and better recover the ground-truth manifold and reduce the gap between pθ(z∣x)p_ theta(z|x)pθ​(z∣x) and qϕ(z∣x)q_ phi(z|x)qϕ​(z∣x). And this is the first demonstration of a VAE pipeline that can produce stable FID scores that are comparable to at least some popular GAN models under neutral testing conditions. | The two-stage mechanism can improve the reconstruction of original distribution so that it has comparable performance with GAN models. This work narrows the gap between VAE and GAN models in terms of the realism of generated samples so that VAEs are worth considering in a broader range of applications. | No need Gaussian assumption in the canonical VAE model to achieve the optimal solutions. | References . . Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In 7th International Conferenceon Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. ↩︎ . | Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. ↩︎ . | Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and SeppHochreiter. GANs trained by a two time-scale update rule converge to a local Nashequilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637,2017. ↩︎ . | Miko laj Bi ́nkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. arXiv:1801.01401, 2018 ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/21/VAE2_blog.html",
            "relUrl": "/variational%20autoencoder/2021/09/21/VAE2_blog.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "VAE1 Importance Weighted Autoencoders",
            "content": "Importance Weighted Autoencoders: what makes a good ELBO in VAE? . Variational AutoEncoders (VAE) [1] is a powerful generative model which combines the variational inference and autoencoders together. It approximates the posterior distribution with a simple and tractable one, and optimize the lower bound of the true data distribution, which is called evidence lower bound (ELBO). Althoug optimizing ELBO is effective in practice, this estimation is actually biased, and it’s shown that this bias actually cannot be eliminated in vanilla VAE. Here we introduce a work that tries to minimize this bias called Importance Weighted Autoencoders (IWAE) [2], along with its variants which combines the objective in VAE and IWAE. . Introduction to VAE and ELBO . VAE consists of the encoder qϕq_{ phi}qϕ​ and the decoder pθp_{ theta}pθ​. It first encodes each sample xxx into a distribution of the latent variables qϕ(⋅∣x)q_{ phi}( cdot|x)qϕ​(⋅∣x). Then the latent variables are sampled from the distibution as z∼qϕ(z∣x)z sim q_{ phi}(z|x)z∼qϕ​(z∣x). The latent variables serve as the input to the decoder where the reconstructed output is x^∼pθ(x∣z) hat{x} sim p_{ theta}(x|z)x^∼pθ​(x∣z). The overview of VAE is shown in Fig 1. . Fig 1. Overview of VAE (source from [1]) The training objective of VAE is to maximize ELBO. There are multiple ways to derivate ELBO, and one way is through the Bayesian theory. log⁡pθ(x) log{p_{ theta}(x)}logpθ​(x) can be rewritten as . log⁡pθ(x)=Eqϕ(z∣x)[log⁡pθ(x)]=Eqϕ(z∣x)[log⁡pθ(x,z)pθ(z∣x)]=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)qϕ(z∣x)pθ(z∣x)]=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)]+Eqϕ(z∣x)[qϕ(z∣x)pθ(z∣x)]. begin{align} log{p_{ theta}(x)}&amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{p_{ theta}(x)}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{p_{ theta}(z|x)}}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}} frac{q_{ phi}(z|x)}{p_{ theta}(z|x)}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}}]+ mathbb{E}_{q_{ phi}(z|x)}[ frac{q_{ phi}(z|x)}{p_{ theta}(z|x)}]. end{align}logpθ​(x)​=Eqϕ​(z∣x)​[logpθ​(x)]=Eqϕ​(z∣x)​[logpθ​(z∣x)pθ​(x,z)​]=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​pθ​(z∣x)qϕ​(z∣x)​]=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​]+Eqϕ​(z∣x)​[pθ​(z∣x)qϕ​(z∣x)​].​​ . Here the second term in Equation (4) is actually the the KL divergence DKL(qϕ(z∣x)∥pθ(z∣x))D_{KL}(q_{ phi}(z|x) |p_{ theta}(z|x))DKL​(qϕ​(z∣x)∥pθ​(z∣x)) that is always non-negative. Therefore, the first term Lθ,ϕ(x)=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)] mathcal{L}_{ theta, phi}(x)= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}}]Lθ,ϕ​(x)=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​] actually serve as an lower-bound of log⁡pθ(x) log{p_{ theta}(x)}logpθ​(x), which is exactly the ELBO. Furthermore, ELBO can be written in the regularized reconstruction form as . Lθ,ϕ(x)=−DKL(qϕ(z∣x)∥pθ(z))+Eqϕ(z∣x)[−log⁡pθ(x∣z)], begin{align} mathcal{L}_{ theta, phi}(x)=-D_{KL}(q_{ phi}(z|x) |p_{ theta}(z)) + mathbb{E}_{q_{ phi}(z|x)}[- log{p_{ theta}(x|z)}], end{align}Lθ,ϕ​(x)=−DKL​(qϕ​(z∣x)∥pθ​(z))+Eqϕ​(z∣x)​[−logpθ​(x∣z)],​​ . where the first term regularizes the posterior distribution towards the prior which is usually set as a standard normal distribution, and the second term corresponds to the reconstruction. . Nonetheloss, the regularization term actually has a conflict with the second term in Equation (4). When the regularization term is perfectly optimized, qϕ(z∣x)q_{ phi}(z|x)qϕ​(z∣x) will stay close to the prior p(z)p(z)p(z), meanwhile, it makes hard for qϕ(z∣x)q_{ phi}(z|x)qϕ​(z∣x) to be close enough to the true posterior distribution pθ(z∣x)p_{ theta}(z|x)pθ​(z∣x). Therefore, the gap between ELBO and the true data distribution, namely DKL(qϕ(z∣x)∥pθ(z∣x))D_{KL}(q_{ phi}(z|x) |p_{ theta}(z|x))DKL​(qϕ​(z∣x)∥pθ​(z∣x)), will always exists, which prevents ELBO from being a tighter lower bound. . Fig 2. Example of a heavy penalization in VAE We can understand this in the other view. When a latent variable is sampled from the low-probability region of a latent distribution, it would inevitably lead to a bad reconstruction. . For the example in Fig 2, if we unfortunately sample a latent variable from the distribution of digit “5” (red) in the orange point, it turns out that this latent variable actually lies in the high probability region of the latent distribution generated by digit “3” (black) and it’s highly possible that we get a final reconstruction more similar to “3” rather than “5”. To make the posterior distribution close to the normal distribution, the regularizer will penalize this sample heavily by decreasing the variance, leading to a small spearout of the latent distribution. This drawback motivates the work of Importance Weight Autoencoders (IWAE) to introduce the importance weights into VAE, where a sampled latent variable which is far away from the mean will get assigned a lower weight during updates since it is known to give a bad reconstruction with high probability. . Importance Weighted Autoencoders . Another way to derivate ELBO is through the Jensen’s Inequality. Since log⁡(⋅) log{( cdot)}log(⋅) is a concave function, we have . log⁡pθ(x)=log⁡Eqϕ(z∣x)[pθ(x,z)qϕ(z∣x)]≥Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)]=Lθ,ϕ(x). begin{align} log{p_{ theta}(x)}&amp;= log{ mathbb{E}_{q_{ phi}(z|x)}[ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}]} &amp; geq mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}}] &amp;= mathcal{L}_{ theta, phi}(x). end{align}logpθ​(x)​=logEqϕ​(z∣x)​[qϕ​(z∣x)pθ​(x,z)​]≥Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​]=Lθ,ϕ​(x).​​ . A simple example is shown in Fig 3. Consider a random variable XXX taking value from {x1,x2} {x_1, x_2 }{x1​,x2​}, and we want to estimate log⁡E[X] log{ mathbb{E}[X]}logE[X]. If we use E[log⁡X] mathbb{E}[ log{X}]E[logX] to estimate it, then the estimation will converge at log⁡x1+log⁡x22 frac{ log{x_1}+ log{x_2}}{2}2logx1​+logx2​​, and the bias term cannot be eliminated by simply increasing the sampling times. . Fig 3. Bias in log expectation estimation If we instead use E[1k∑i=1klog⁡Xi] mathbb{E}[ frac{1}{k} sum_{i=1}^k{ log{X_i}}]E[k1​∑i=1k​logXi​] for estimation, when we gradually increase the sampling times kkk, the bias will become smaller. And when k→+∞k rightarrow+ inftyk→+∞, the term inside the expectation actually becomes a constant which is exactly log⁡E[X] log{ mathbb{E}[X]}logE[X], as shown in Fig 4. . Fig 4. Reducing the bias in the log expectation If we apply this property on ELBO estimation, let wi=pθ(x,zi)qϕ(zi∣x)w_i= frac{p_{ theta}(x,z_i)}{q_{ phi}(z_i|x)}wi​=qϕ​(zi​∣x)pθ​(x,zi​)​ and Lk=Eqϕ(z∣x)[log⁡1k∑i=1kwi] mathcal{L}_k= mathbb{E}_{q_{ phi}(z|x)}[ log{ frac{1}{k} sum_{i=1}^kw_i}]Lk​=Eqϕ​(z∣x)​[logk1​∑i=1k​wi​], we actually have the theorem . log⁡pθ(x)≥Lk+1≥Lk. begin{align} log{p_{ theta}(x)} geq mathcal{L}_{k+1} geq mathcal{L}_k. end{align}logpθ​(x)≥Lk+1​≥Lk​.​​ . And Lk mathcal{L}_kLk​ will converge to log⁡pθ(x) log{p_{ theta}(x)}logpθ​(x) when pθ(x,z)qϕ(z∣x) frac{p_{ theta}(x,z)}{q_{ phi}(z|x)}qϕ​(z∣x)pθ​(x,z)​ is bounded. . Equipped with this theorem, IWAE simply replace sthe objective in VAE with Lk mathcal{L}_kLk​, where k&gt;1k&gt;1k&gt;1. This gives a tighter lower bound compared with ELBO. And when k=1k=1k=1, IWAE is reduced to VAE. . In the backward pass, the gradient of Lk mathcal{L}_kLk​ can be written as . ∇θ,ϕLk=Eqϕ(z∣x)[∇θ,ϕlog⁡1k∑i=1kwi]=Eqϕ(z∣x)[∑i=1kwi~log⁡∇θ,ϕwi], begin{align} nabla_{ theta, phi} mathcal{L}_k&amp;= mathbb{E}_{q_{ phi}(z|x)}[ nabla_{ theta, phi} log{ frac{1}{k} sum_{i=1}^kw_i}] &amp;= mathbb{E}_{q_{ phi}(z|x)}[ sum_{i=1}^k tilde{w_i} log{ nabla_{ theta, phi}w_i}], end{align}∇θ,ϕ​Lk​​=Eqϕ​(z∣x)​[∇θ,ϕ​logk1​i=1∑k​wi​]=Eqϕ​(z∣x)​[i=1∑k​wi​~​log∇θ,ϕ​wi​],​​ . where wj~=wj∑i=1kwi tilde{w_j}= frac{w_j}{ sum_{i=1}^kw_i}wj​~​=∑i=1k​wi​wj​​ is the normalized importance weights, which makes the model name “Importance Weighted” Autoencoders. In VAE, wj~ tilde{w_j}wj​~​ takes the value 111. . The meaning of the importance weights could be interpreted as this: if a latent sample itself has low probability in the latent distribution, then it should get assigned a lower weight in the gradient update since it’s known to cause a bad reconstruction with high probability. Introducing importance weights can effectively lower the risk shown in Fig 2. . Variants of IWAE . To make a straightforward comparison with ELBO in VAE, we fix the sampling times for both VAE and IWAE as kkk. The ELBO for IWAE and VAE become . ELBOIWAE=log⁡1k∑i=1kwiELBOVAE=1k∑i=1klog⁡wi. begin{align} text{ELBO}_{ text{IWAE}}&amp;= log{ frac{1}{k} sum_{i=1}^kw_i} text{ELBO}_{ text{VAE}}&amp;= frac{1}{k} sum_{i=1}^k log{w_i}. end{align} ELBOIWAE​ELBOVAE​​=logk1​i=1∑k​wi​=k1​i=1∑k​logwi​.​​ . The main difference here is the position of the average operation, either insider or outside log⁡(⋅) log{( cdot)}log(⋅). IWAE regards the sampling outside log⁡(⋅) log{( cdot)}log(⋅) as the variance reduction, and it’s shown that IWAE actually doesn’t suffer from the large variance, so IWAE puts all sampling inside log⁡(⋅) log{( cdot)}log(⋅) to reduce the bias as much as possible. . However, a follow-up work [3] of IWAE theoretically proves that the sampling outside log⁡(⋅) log{( cdot)}log(⋅) is crucial to the training of the encoder. A tighter bound used by IWAE helps the generative network (decoder) but hurts the inference network (encoder). . Based on this discovery, three new models combining ELBOIWAE text{ELBO}_{ text{IWAE}}ELBOIWAE​ and ELBOVAE text{ELBO}_{ text{VAE}}ELBOVAE​ are proposed. For the following text, we fix the total sampling times to be MKMKMK, where MMM is the sampling times outside log⁡(⋅) log{( cdot)}log(⋅) and KKK is the sampling times inside log⁡(⋅) log{( cdot)}log(⋅). . MIWAE. MIWAE simply uses an ELBO objective with both M&gt;1M&gt;1M&gt;1 and K&gt;1K&gt;1K&gt;1, i.e., ELBOMIWAE=1M∑m=1Mlog⁡1K∑k=1Kwm,k. begin{align} text{ELBO}_{ text{MIWAE}}= frac{1}{M} sum_{m=1}^M log{ frac{1}{K} sum_{k=1}^Kw_{m,k}}. end{align} ELBOMIWAE​=M1​m=1∑M​logK1​k=1∑K​wm,k​.​​ | CIWAE. CIWAE uses a convex combination of two ELBOs, i.e., ELBOCIWAE=βELBOVAE+(1−β)ELBOIWAE. begin{align} text{ELBO}_{ text{CIWAE}}= beta text{ELBO}_{ text{VAE}}+(1- beta) text{ELBO}_{ text{IWAE}}. end{align} ELBOCIWAE​=βELBOVAE​+(1−β)ELBOIWAE​.​​ | PIWAE. PIWAE uses different objectives for the inference network and generative network. For the generative network, it keeps the objective of IWAE, ELBOIWAE text{ELBO}_{ text{IWAE}}ELBOIWAE​. While for the inference network, it switches to the objective ELBOMIWAE text{ELBO}_{ text{MIWAE}}ELBOMIWAE​. | . Experimental Results . The experimental results demonstrate the advantage of IWAE against VAE as Table 1 shows. IWAE achieves lower negative log-likelihood (NLL) and more active units (active units captures data infomation) on all datasets and model architectures. And as kkk increases, the performance is better since the lower bound is tighter. . Table 1. IWAE results For the qualitative analysis in Fig 5, it’s worth noting that for IWAE, it has a larger spredout of the latent distribution and sometimes different output digits, e.g., “6” for “0”. This demonstrates the relaxation of the heavy panelization on the outliers, contrary to the example in Fig 2. . Fig 5. Ouput samples from VAE and IWAE In a grid search of different combinations of (M,K)(M,K)(M,K) with MKMKMK fixed as 646464, we can see neither M=1M=1M=1 or K=1K=1K=1 makes the optimal solution in Fig 6. In other words, the ELBO objective should consider both the sampling inside and outside log⁡(⋅) log{( cdot)}log(⋅), which are beneficial to the generative network and inference network respectively. . Fig 6. Ouput samples from VAE and IWAE Conclusion . IWAE uses a simple technique, moving the average operation inside log⁡(⋅) log{( cdot)}log(⋅), to achieve a tighter lower bound. The importance weights relaxes the heavy penalization on the posterior samples which fail to explain the observation. Although IWAE effectively reduces the bias, it’s shown that the sampling inside log⁡(⋅) log{( cdot)}log(⋅) is only beneficial to the generative network, but hurts the inference network. Therefore, to combine the ELBO in VAE and IWAE, the IWAE variants, MIWAE, CIWAE and PIWAE are proposed. The final results demonstrate that the optimal objective needs the sampling inside and outside log⁡(⋅) log{( cdot)}log(⋅) to be both greater than one. These works takes a deep look into the ELBO objective in VAE and reveal its role in the learning process. . References . [1] 2014 (ICLR): D. Kingma, M. Welling, Auto-Encoding Variational Bayes, ICLR, 2014. | [2] 2016 (ICLR): Y. Burda, R. Grosse, R. Salakhutdinov. Importance Weighted Autoencoders. ICLR, 2016. | [3] 2018 (ICML): T. Rainforth, A. Kosiorek, T. Le, C. Maddison, M. Igl, F. Wood, Y. Teh, Tighter Variational Bounds are Not Necessarily Better. ICML, 2018. | .",
            "url": "https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/16/VAE1_blog.html",
            "relUrl": "/variational%20autoencoder/2021/09/16/VAE1_blog.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "AR3 Attention Is All You Need",
            "content": "Attention Is All You Need . Introduction . In the last three years, the Transformer architecture has become an influential paradigm within deep learning. It has been applied prolifically within natural language processing (NLP), is beginning to see promising applications in computer vision (CV), and is also used within many other modalities and fields of deep learning. The paper which introduced the Transformer is “Attention is All You Need” [1] by Vaswani et al. Attention is All You Need (from here, AAYN) uses the Transformer architecture to perform machine translation. . Historically, the work in AAYN was done when recurrent neural networks (RNN) were the dominant force in NLP. Common modifications of these included the Long short-term memory (LSTM) [5] and gated recurrent unit (GRU) [6]. However, these models have a big problem—they compute along the length of a sequence, so they cannot be parallelized easily. Additionally, RNNs struggle to learn long-term dependencies. In order to rectify this issue, the authors propose the key idea (and title): attention is all you need. Although there had been previous work on using attention, most of those papers combined it with RNNs, so it still had the drawbacks from that method. . Task – Machine Translation . In AAYN, the primary goal of the model is translation, making this a sequence-to-sequence generation problem. In particular, they focus on English to German and English to French tasks from WMT2014. Machine translation is trained on bitext – data where each sample consists of the same sentence in the source and the target language. Then, the model is evaluated on a test set where it has to translate sentences. The results are compared to several human reference translations which are used to compute the BLEU score. It is defined as follows [2,7] . . Definition of BLEU from [2] Here, the key things to note are the brevity and n-gram overlap. Note that if the model outputs something very short, then it has a high probability of completely overlapping with n-grams in a reference translation. To penalize this, the brevity penalty is added, so when the output translation is shorter than the reference translation, then the exponent will be to the power of a negative number making a smaller brevity term. If the reverse is true then the brevity will be 1 due to the minimum and be ignored. The other important term in BLEU is the n-gram overlap. This essentially measures how well an output matches the references. The different lengths of n-grams measure different things; unigrams measure adequacy and the longer n-grams measure fluency. Note that this definition allows the candidate output to combine parts from different reference translations and have a good score. . . BLEU score interpretation from [2] Preliminaries . The Transformer model uses an encoder-decoder sequence-to-sequence architecture. It can be described as mathematically as follows: . Input: Length nnn sequence of symbolic representations: (x1,...,xn)(x_1,...,x_n)(x1​,...,xn​) | Encoder: produces latent representations: z=(z1,...,zn)z=(z_1,...,z_n)z=(z1​,...,zn​) | Decoder: uses zzz to produce length mmm output sequence: (y1,...,ym)(y_1,...,y_m)(y1​,...,ym​) | . Model . The Transformer is the following model: . . In the model, the encoder is on the left, and it feeds into the decoder on the right. In AAYN, the encoder and decoder are each stacked six times. We will examine the construction of both the encoder and decoder, so first let’s look at how the pieces of each layer are built. . Scaled Dot-Product Attention . Attention is at the key of the Transformer architecture. The intuition behind this approach is that it allows the model to decide what other symbols in the sequence are most important to look at for solving whatever problem. In AAYN, the attention mechanism is implemented using multiplicative attention (dot product). In addition, the major modification from the paper is to scale the dot products—this is done by dividing by the square root of the number of dimensions. This is due to the author’s observation that the the dot product grows too large in magnitude for a high number of dimensions, which would limit the model’s efficacy. . . . Dot-product attention works by learning a query, key, and value projection from some input. The query and key values are used to compute the attention—how much weight the model gives each token in a sequence. The multiplication between QQQ and KTK^TKT produces a sequence length by sequence length array of logits. Then Softmax is applied, which essentially turns the logits into probability distributions (one for each symbol in the sequence). These probabilities are used to compute a weighted average of the values VVV. VVV contains a representation for each symbol in the sequence, so the attention distribution decides how much one symbol iii should pay to any other symbol jjj. A visual of this will be shown later in the results section. . Multi-Head Attention . The authors notice that the weighted average in attention prevents the model from looking at different representation subspaces—it can’t consider multiple different parts of the sequence without averaging them. To fix this, the authors propose multi-head attention. . . Multi-head attention essentially allows the model to look at multiple things at the same time. Each attention head can learn to look for different things, such as connecting adjectives to nouns or connecting verbs and objects. Naively using multi-head attention, however, would increase the computational costs of the model. . . To address this issue, the authors decrease the representation dimension of each head by hhh, where hhh is the number of heads. This results in the same total number of parameters in the attention mechanism. The following dimensions are used for each head: . . The output representation from each head is concatenated together to create a vector of the original length, dmodeld_{model}dmodel​. This is projected again. . Positional Embeddings . One key issue of only using attention is that, according to attention, one symbol in a sequence is the same distance away from any other symbol. To allow the model to determine distance, the authors introduce positional embeddings. . . Visualization of sinusoidal positional embeddings from [3]. Each column is a positional embedding. . Equations for the sinusoidal position embeddings The authors selected a sinusoidal embedding function because the offset between embeddings can be represented as a linear function. However, many positional embeddings are possible and the authors also experiment with learned positional embeddings (achieving similar results). The sinusoidal embeddings are used in the paper because the authors hypothesize that they will allow the model to extrapolate to sequence lengths longer than those the model was trained on. Note that later work shows that position is not necessarily as important as intuition suggests [8]. . Types of Attention . In AAYN, three types of attention are used: . Encoder-decoder attention: Queries QQQ come from the last layer of the decoder, keys KKK and values VVV come from encoder. This allows the decoder to look at the input source language in order to translate it. . | Encoder self-attention layer: Each position can attend to every other position in the previous layer of the encoder. . | Decoder self-attention layer: Same as encoder self-attention but also mask out all connections in the Softmax that cannot have been seen. . This maintains the autoregressive property of the model by preventing the model from looking at words it hasn’t seen yet. | . | . Why Self-Attention? . Self-attention allows the model to learn dependencies between different symbols in the sequence. This is shown in the following table: . . Note that self-attention achieves the best complexity in terms of both maximum path length and sequential operations (which indicates parallelizability). Additionally, the complexity per layer is low if nnn is significantly smaller than ddd, which often occurs in practice (although many researchers are working on models where this assumption no longer holds). . Putting It All Together . Alright, we’ve looked at all the pieces. Now, let’s put everything together! . The Encoder . . The encoder combines all the parts we talked about. Then, it is stacked NNN times (6 in the base Transformer model). A couple notable things that we didn’t mention yet: . The model uses either byte-pair or word-piece tokenization to create token sequences from raw character strings. . | The model uses residual connections by adding an earlier representation. It follows this up with layer normalization. . | The model uses a simple feed-forward network with two layers after attention. . | . The Decoder . . The decoder is mostly the same as the encoder. However, it uses masking to ignore symbols that the model shouldn’t have seen yet from the input. For example, if I say “The cat sat” then the model would need to generate the next word. This is called autoregression. If this model generates “on”, then “The cat sat on” would be fed in the model to generate the next word (maybe “the”). When we’re training the model, we have the full sentence that the model is learning to generate, so we can’t let it know what’s coming. . In addition, the decoder also has an attention mechanism that looks at what it’s trying to translate. This allows it to see what it should be doing, and the decoder can use attention to connect the source input to its translated output. For example, “gato”, or cat in Spanish, might be connected to “cat” in the example above using attention. . Training . The training details for the model are as follows: . Sentences are encoded (convert a string of characters to a sequence of symbols): . English-German uses BytePair encoding for 37,000 tokens on 4.5M sentence pairs. | English-French uses WordPiece encoding for 32,000 tokens on 36M sentence pairs. | . | Batch size is determined in order to have 25,000 source and target tokens (symbols). . | 8 NVIDIA T100 GPUs are used to train the model. . Base models trained for 12 hours, big models for 3.5 days. | . | Adam optimizer is used with a special learning rate: . Linear warmup followed by inverse square root decay. | . | Regularization: Dropout of 0.1 applied to residual connections and sum of positional encoding and embeddings. Label smoothing is performed. . | The last 5 checkpoints are averaged (for the base model). Beam search is used to select the best translation. . | . The most interesting details are that the batch size is dynamic so that the source and target tokens number approximately 25,000. The summation of the last checkpoints and using a learning rate decay are also interesting. . Results . The details are finally out of the way! Let’s look at some pretty pictures and tables. . . As can be seen in the above figure, Transformer is able to set record BLEU scores in much less computation. Additionally, the “big” variant can even beat some expensive ensemble models! We can see the effect of different hyperparameters (such as those used in base versus big) on the performance in the following table: . . Alright, now time for some visualizations from the paper: . . . . . As shown in the visualizations, the attention heads learn different, meaningful tasks, such as anaphora resolution or connecting determiners and their objects. In fact, the authors show that Transformer can be used directly for this type of task—English constituency parsing (extracting the syntactic structure of a sentence in the form of a tree). . . Results are competitive to previous methods, even without task-specific fine-tuning. . Final Takeaway . Transformers precipitated a major change in the landscape of natural language processing and even other fields like computer vision. They lead to even more powerful general language models, such as BERT [4]. The paper can be summarized by the following points. . Motivation: RNNs are not easily parallelizable and don’t learn long dependencies well. . | Models that only use attention are more effective and train faster. . | Transformer can generalize to other tasks. . | Multi-head attention helps address some of the problems of traditional attention. It allows multiple different attention tasks to be learned. . | Transformers have a constant dependency path from one position to any other position. . | . References . [1] Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017. . [2] BLEU score definition: https://cloud.google.com/translate/automl/docs/evaluate . [3] https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ . [4] Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. . [5] Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780. . [6] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. . [7] Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W. J. (2002, July). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (pp. 311-318). . [8] Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., &amp; Kiela, D. (2021). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. arXiv preprint arXiv:2104.06644. .",
            "url": "https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog3.html",
            "relUrl": "/transformers/2021/09/14/AR3_blog3.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "AR3 Generating Long Sequences with Sparse Transformers",
            "content": "Generating Long Sequences with Sparse Transformers . Transformers and attention-based methods have skyrocketed in popularity in recent years. These models excel at modelling long-term dependencies and are highly parallelizable, overcoming the shortcomings of prior LSTM based models. However, vanilla transformers1 scale poorly with increasing sequence length; since the attention is done globally between all inputs, the computation grows quadratically with input length. In this post, I will go over the Sparse Transformer2 model which reduces the computation to O(nn)O(n sqrt n)O(nn . ​) where nnn is the sequence length. Additionally, unlike prior works that propose model for specific generation tasks, the sparse transformer model can be used to generate text, images, and audio! . Background . The sparse transformer is an autoregressive model. It models the joint probability distribution as a product of conditional probability distributions. The iiith output depends all on previous inputs x1,...,xi−1x_1, ..., x_{i-1}x1​,...,xi−1​. This autoregressive property is embedded into the attention operation, which cannot use future values to generate an output. Image taken from 3 . . . Factorized Self-Attention Intuition . To understand the motivation behind the sparse transformer model, we take a look at the learned attention patterns for a 128-layer dense transformer network on the CIFAR-10 dataset. The authors observed that the attention pattern of the early layers resembled convolution operations. For layers 19-20, the attention pattern is arranged in discrete rows and columns. In other layers, the attention pattern is extremely complex, global, and data dependent. Finally, in the the layers 64-128, the attention pattern is extremely sparse. Visualization taken from 2 . . Looking at these attention patterns, we observe that most attention patterns are sparse. The authors reasoned that to model high-dimensional data, dense global attention is not required. Instead, sparser attention operations can capture most of the information needed to model the underlying distribution. . Sparse Transformer Model . Factorized Self-Attention . The factorized self-attention operation forms the backbone of the sparse transformer model. The authors break down the dense self-attention operation with several sparse attention operations. In particular, an attention operation can be written as (equations taken from 2): . where SiS_iSi​ denotes the set of indices of input vectors which the iiith output vector attends. For dense self-attention, Si={j:j≤i}S_i= {j:j le i }Si​={j:j≤i} which allows every element to attend to all previous positions and its own position. This pattern can be visualized in the image given below. Visualization of attention taken from 2 . . The bottom image is the connectivity matrix where the i=ji=ji=j index represents the output and the other indices in the same row represent the input that the output attends to. . Instead, factorized self attention uses ppp separate attention heads each defining a subset of indices Ai(m)⊂{j:j≤i}A_i^{(m)} subset {j:j le i }Ai(m)​⊂{j:j≤i}. We want choice of AAA such that ∣Ai(m)∣∝ap|A_i^{(m)}| propto sqrt[p] a∣Ai(m)​∣∝pa . ​ so that our computation scales the way we want. This paper considers choices with two heads p=2p=2p=2. Additionally, we add the constraint that there is path from each input connection to all future output positions across ppp steps of attention (this will become more clear in the representations for the factorized attention patterns) so that all input signals are being propagated to output positions in a constant number of steps. . Strided Attention Pattern . In this factorized attention pattern, one head attends to lll previous locations and the other head attends to every lllth previous location. lll is the stride parameter and is chosen to be close to n sqrt nn . ​. More formally, the index sets are defined as Ai(1)={t:t,t+1,...,i}A_i^{(1)} = {t:t,t+1,...,i }Ai(1)​={t:t,t+1,...,i} for t=max(0,i−l)t=max(0,i-l)t=max(0,i−l) and Ai(2)={j:(i−j) mod l=0}A_i^{(2)} = {j:(i-j) , mod , l = 0 }Ai(2)​={j:(i−j)modl=0}. This strided attention pattern is visualized below. Visualization of attention taken from 2 . . This pattern works well when the data naturally has a structure that aligns with the stide. For example, images and audio have periodic structure that can be modeled effectively using strided attention patterns. However, for data without this natural structure such as text, the strided pattern does not perform well. . Fixed Attention Pattern . In the fixed attention pattern, we model the AAA matrices as follows: Ai(1)={j:(⌊j/l⌋=⌊i/l⌋)}A_i^{(1)} = {j: ( lfloor j/l rfloor = lfloor i/l rfloor) }Ai(1)​={j:(⌊j/l⌋=⌊i/l⌋)} and Ai(2)={j:j mod l∈{t:t,t+1,...,l}}A_i^{(2)} = {j:j , mod , l in {t:t,t+1,...,l } }Ai(2)​={j:jmodl∈{t:t,t+1,...,l}}. This pattern is easier to visualize than understanding the math. Visualization of attention taken from 2 . . This attention pattern works better for data without perdiodic structure like text. . Note: For both strided and fixed attention pattern, notice how every input signal is propagated to arbitrary output after 2 steps of attention, satisfying the constraints. . Incorporating Factorized Self-Attention . The authors proposed several ideas on how to incorporate these factorized attention models in the transformer network. These methods can be summarized in the image below. . . Gradient Recomputation during Backward Pass . During the gradient backpropagation step while training, the results from the forward pass are stored in memory and used for computation. However, for sparse attention, the memory usage to store the result is far greater than the computational cost of the forward pass. Hence, we don’t save all the forward pass results in memory but recompute the forward pass during training the compute the gradients. This reduces the memory usage of the model and enables networks with hundreds of layers and sequences of up to 16384 in length. . New residual block architecture . Transformers are notoriously difficult to scale to many layers. The authors of this paper experiment with using a different kind of residual connection which enables the sparse transformer model to scale to hundred of layers. The NNN layer transformer network is defined as follows. . . This residual architecture with the gradient recomputation is visualized in the image below. Visualization of model architecture taken from 2 . . . Results . Image generation examples taken from 2 . . . Sparse transformer NLL metrics on common datasets taken from paper 2 . . . . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017; http://arxiv.org/abs/1706.03762 arXiv:1706.03762. ↩︎ . | Child, Rewon, et al. “Generating Long Sequences with Sparse Transformers.” ArXiv:1904.10509 [Cs, Stat], Apr. 2019. arXiv.org, http://arxiv.org/abs/1904.10509 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Oord, Aaron van den, et al. “Pixel Recurrent Neural Networks.” ArXiv:1601.06759 [Cs], Aug. 2016. arXiv.org, http://arxiv.org/abs/1601.06759 ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog2.html",
            "relUrl": "/transformers/2021/09/14/AR3_blog2.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "AR3 An image is 16 x 16 words",
            "content": "An image is 16 x 16 words . Transformers and attention-based methods have skyrocketed in popularity in recent years. These models are the current state-of-the-art in natural language processing applications (BERT, GPT). However, in computer vision, convolutional patterns still remain dominant. Applying transformers directly to image pixels is not practical because the self-attention operation scales quadratically. Many recent works experiment with hybrid convolutional and attention based methods. Other works that replace convolutions for attention all together, like the Sparse Transformer, use specialized attention patterns that are difficult to scale on hardware accelerators. This paper demonstrates that the vanilla transformer 1, with minimal modifications, can achieve state-of-the-art performance in image classification when trained on large datasets. . Vision Transformer Architecture . Input format . The input image x∈RH×W×Cx in R^{H times W times C }x∈RH×W×C is reshaped into a sequence of flattened patches xp=RN×(P2C)x_p = R^{N times (P^2 C )}xp​=RN×(P2C) where PPP is that patch size. Since the transformer uses constant latent vector size D through all of its layers, the xpx_pxp​ flattened patches are linearly projected into DDD dimensions using a trainable linear layer. This forms the patch embeddings. . Positional Embeddings . Similar to 1, positional embeddings are added to the patch embeddings to convey positional information to the model. The authors use learnable 1D embeddings and found that 2D embeddings don’t improve performance. The 1D embeddings use the index of the patch row-by-row top-to-bottom. . Class Token . Similar to BERT 2, the authors prepend a learnable class token z00z_0^0z00​ embedding along with its learnable positional encoding to the input. The state of this token at the output zL0z_L^0zL0​ is the input of a classification head MLP which outputs the class probabilities for image classification. . Transformer Encoder . The inputs defined above are fed directly into the transformer encoder from1. The transformer encoder consists of alternating layers of multihead self-attention and MLP blocks. The image taken from paper 2 below summarizes the transformer encoder model: . . Hybrid Model . Instead of the image, the input patches can be formed from the output feature maps of a CNN. In this scheme, the linear projection is applied to the patches from the CNN to form the patch emdeddings. The other parts of the architecture remain the same. . Model Visualization . The image below shows the entire model for the vision transformer. Notice how the model is very similar to the transformer encoder form 1. . . Results . The authors evaluate 3 variations of their Vision Transformer (ViT) on image classification datasets. The image below summarizes the 3 ViT models. Additionally, the authors experiment with different patch sizes, reporting the results for 16×1616 times 1616×16 patches and 14×1414 times 1414×14 patches. . As seen from the image taken from 2 below, the Vision Transformer outperforms CNN based approaches on across multiple datasets. The image taken from 2 below shows how the ViT takes significantly less compute to pre-train than its CNN counterparts. . . Intuition . If you’ve read this paper so far, you must certainly be confused about the results of the ViT. How does the vanilla vision transformer working on image patches learn to solve computer vision task better than the state-of-the-art CNN models? The authors seems to think its because of the inherent inductive bias in CNNs. The convolution operation exploits locality and two-dimensional spatial structure of images. The idea of looking at neighboring pixels to extract meaningful representation of image data is what made CNNs rise in popularity many years ago. However, because CNNs are so highly specialized, they are not as good as transformers at learning features that do not depend on nearby pixels. Because of the global attention layers and minimal image-specific inductive bias, the vision transformer is able to learn features that the CNN model misses out because of its specialized convolution operation. The image below is a comparison of the linear embedding in the ViT to convolutional layers in CNN. Visualizations taken from paper 2. . As you can see, the learned linear layer closely resembles convolutional filters learned by CNNs. But the transformer model can also learn much more than that because it is not limited by convolutional operations. . It is to be noted that when training on mid-sized datasets, the CNN based model still outperforms ViT because the specialized convolutional operations quickly learn representations that frequently occur in images. However, when training with very large datasets, the transformer is able to learn features that the convolution misses out on because of observing enough samples. . . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017; http://arxiv.org/abs/1706.03762 arXiv:1706.03762. ↩︎ ↩︎ ↩︎ ↩︎ . | Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, 2018; http://arxiv.org/abs/1810.04805 arXiv:1810.04805. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog.html",
            "relUrl": "/transformers/2021/09/14/AR3_blog.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "AR2 Pixel recurrent neural networks",
            "content": "The Ultimate Goal . Generative models that could fit a distribution from samples and then generate more examples from it recently get a staggering development. Many generated images and audio clips are of amazing quality and realism. To be formal, given a random variable X∈XX in mathcal{X}X∈X, we would like to fit an approximate distribution pθ(xi)∝exp⁡(θi)p_ theta(x_i) propto exp{( theta_i)}pθ​(xi​)∝exp(θi​), where x1,…,xn∈Xx_1, dots,x_n in mathcal{X}x1​,…,xn​∈X is some discretization. . Most simply, this problem could be solved by minimizing the Kullback-Leibler (KL) divergence, essentially pulling the approximate and the real distribution close. θ∗=arg⁡min⁡θDKL(pX∥pθ) theta^*= arg min_ theta D_{KL}(p_X |p_ theta)θ∗=argminθ​DKL​(pX​∥pθ​). However, obviously, this is only tractable when the space X mathcal{X}X is small and low-dimensional. . Therefore, the development and the capacity of the state-of-the-art generation models are largely built upon the fundamental advances in autoregressive density estimations1, variational inference2, and generative adversarial networks3. Let us now look at how they approach this goal, and what are there common limitations. . What is Already There . The core problem is that to model a high-dimensional joint density distribution requires exponentially many parameters as the dimension going up. The following methods are different approaches to circumvent this issue by making assumptions, simplifications, or viewing it from another perspective. . Autoregressive Models (ARs) . Autoregressive models typically factorize the joint distribution by the product law, and imposes some conditional independence assumptions to reduce the number of conditionals needed. The following formula explains them all. σ:Nn→Nn sigma: mathbb{N}_n rightarrow mathbb{N}_nσ:Nn​→Nn​ is a permutation function of dimensions, which is included in the formula to make the indices general. . pX(x)=∏i=1DpXσ(i)(xσ(i)∣xσ(1),…,xσ(i−1)) p_X( mathbf{x})= prod_{i=1}^D p_{X_{ sigma(i)}} left(x_{ sigma(i)}|x_{ sigma(1)}, dots,x_{ sigma(i-1)} right) pX​(x)=i=1∏D​pXσ(i)​​(xσ(i)​∣xσ(1)​,…,xσ(i−1)​) . The model is usually straightforward, but usually there are some ordering issue. Also, the autoregressive nature tends to make generation slow. . Variational Autoencoders (VAEs) . Another perspective is to represent pθp_ thetapθ​ as the marginalization over a latent random variable Z∈ZZ in mathcal{Z}Z∈Z. Then with the relation below, maximizing the evidence lower bound makes the approximate pθp_ thetapθ​ close to pXp_XpX​. . log⁡pθ(x)≥−DKL(qθ(z∣x)∥p(z))+E[log⁡pθ(x∣z)] log{p_ theta(x)} geq-D_{KL} left(q_ theta(z|x) |p(z) right )+ mathbb{E} left[ log{p_ theta(x|z)} right ] logpθ​(x)≥−DKL​(qθ​(z∣x)∥p(z))+E[logpθ​(x∣z)] . VAEs are straightforward to implement and optimze, and efficient at generation and capturing structures in high-dimensional spaces. However, VAEs often miss fine-grained details. . Generative Adversarial Networks (GANs) . We could also tackle this problem from another perspective of a two-player zero sum game. We have two players, a generator GGG and a discriminator DDD. The generator tries to generate fake example from distribution pθp_ thetapθ​ that mimics the true distribution, and the discriminator tries to distinguish fake examples from real data points. The objective then could be written as the following, . arg⁡min⁡Gsup⁡D[EXlog⁡(D(X))+EXlog⁡(1−D(G(Z))))] underset{G}{ arg min} sup_D left[ underset{X}{ mathbb{E}} log left(D(X) right)+ underset{X}{ mathbb{E}} log left(1-D(G(Z))) right) right] Gargmin​Dsup​[XE​log(D(X))+XE​log(1−D(G(Z))))] . . Image Credits to Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch) | . This task is essentially minimizing the Jenson-Shannon divergence, still some function of KL-divergence. This model is infamous to train stably, and the initiation of the model training is also hard. . Hail to KL-divergence . . Image Credits to Understanding Cross-entropy for Machine Learning | . As we can see, all the state-of-the-art methods relies on the KL-divergence one way or another. Even the GANs are in effect minimizing some divergence deeply related to KL-divergence. However, it is known having some problem catching the low probability tails in the density function because it is essentially the expected deviation. . But is There Another Choice? . Sure. All above methods try to approximate the density function pXp_XpX​ directly. Why can’t we approximate other function deeply related to pXp_XpX​ such as the cumulative distribution function (CDF) or inverse CDF. In order to achieve this goal, let us look at what tools we already have. . Quantile . Let XXX be a random variable with CDF FX(x)=P(X≤x)F_X(x)= mathbb{P}(X leq x)FX​(x)=P(X≤x), the τ tauτ-th quantile of XXX is given by, QX(τ)=FX−1(τ)=inf⁡x{FX(x)≥τ}Q_X( tau)=F_X^{-1}( tau)= inf_x {F_X(x) geq tau }QX​(τ)=FX−1​(τ)=xinf​{FX​(x)≥τ} . This essential means that we need to find a sample point xxx so that there are τ tauτ portion of the data points lies below the xxx. To make the example more concrete, let us consider a Gaussian random variable X∼N(5;3)X sim mathcal{N}(5;3)X∼N(5;3), and 0.1-th quantile QX(0.1)≈1.155Q_X(0.1) approx1.155QX​(0.1)≈1.155, 0.9-th quantile QX(0.9)≈8.845Q_X(0.9) approx8.845QX​(0.9)≈8.845. . . Quantile Regression . What could quantile do? One step advance from linear regression. Given a dataset (X,Y)(X,Y)(X,Y), and a quantile τ∈(0,1) tau in(0,1)τ∈(0,1), approximate the conditional quantile function at τ tauτ: QY∣X(τ)=XβτQ_{Y|X}( tau)=X beta_ tauQY∣X​(τ)=Xβτ​, under the loss function ρτ(u)={(τ−1)uu≤0τuu&gt;0 rho_ tau(u)= begin{cases} ( tau-1) u &amp; u leq 0 tau u &amp; u&gt; 0 end{cases} ρτ​(u)={(τ−1)uτu​u≤0u&gt;0​ where u=Y−Xβτu=Y-X beta_ tauu=Y−Xβτ​ is the error. . As we can see, if we fix a τ tauτ, the formulation is essentially the same as linear regression other than the special loss function. How is this regression useful? Let us look an example. . Suppose you ordered UberEats, and you have the dataset of history delivery data between distance and delivery time. Now you need to give a time range estimate given the distance that covers 80% of the customers’ delivery time. We could fit a 0.1-th and a 0.9-th regression model and give a range between them. . . Quantile Loss . Take a look again at the expression of quantile loss, we could observe that the penalty for underestimation/overestimation is different, depending on τ tauτ. If we look at the loss function at τ=0.1 tau=0.1τ=0.1, we have ρ0.1(u)={−0.9uu≤00.1uu&gt;0 rho_{0.1}(u)= begin{cases} -0.9 u &amp; u leq 0 0.1 u &amp; u&gt; 0 end{cases} ρ0.1​(u)={−0.9u0.1u​u≤0u&gt;0​ For underestimation (u&gt;0u&gt;0u&gt;0), the penalty is 0.1, but for overestimation, the penalty is -0.9. If the regressor is at the middle of the data blob, then how should it move to minimize the loss? Obviously, to have more underestimation is good, so the regressor will move down to the red line, which is essentially how quantile regression works. . If the quantile loss at any τ tauτ is small though, we could conclude that we captured almost all the details of the distribution, even if the density is low. So could this be our substitute for the KL-divergence? . Modeling from Another Perspective . So, instead of modeling the density directly, we could approximate the inverse CDF. This is almost equivalent because we could deduct the density estimate from the inverse CDF. . Similar to approximating density functions, we have to decide on a factorization of the Quantile function (inverse CDF) in high-dimensional space to make it tractable. . If the CDF is of a single variable τ tauτ, the we need comonotonic property to ensure invertibility (obvious because there is no negative probability, CDF must be non-decreasing along any dimension, which is what comonotonic property essentially implies). FX−1(τ)=(FX1−1(τ),FX2−1(τ),…,FXn−1(τ)F_X^{-1}( tau)=(F_{X_1}^{-1}( tau), F_{X_2}^{-1}( tau), dots,F_{X_n}^{-1}( tau)FX−1​(τ)=(FX1​−1​(τ),FX2​−1​(τ),…,FXn​−1​(τ) is a very strong assumption, and could hardly be used broadly. | On the other hand, if we use a separate τi tau_iτi​ for each component, FX−1(τ⃗)=(FX1−1(τ1),FX2−1(τ2),…,FXn−1(τn)F_X^{-1}( vec{ tau})=(F_{X_1}^{-1}( tau_1), F_{X_2}^{-1}( tau_2), dots,F_{X_n}^{-1}( tau_n)FX−1​(τ . )=(FX1​−1​(τ1​),FX2​−1​(τ2​),…,FXn​−1​(τn​), we are assuming independence between all components, which is unrealisticly restrictive for many domains. | So we do the same, factorize the CDF, and make some assumptions on conditional independence. FX(x)=P(X1≤x1,…,Xn≤xn)=∏i=1nFXi∣Xi−1,…,X1(xi)FX−1(τjoint)=(FX1−1(τ1),…,FXn∣Xn−1,…−1(τn)) begin{align*} F_X(x)&amp;= mathbb{P}(X_1 leq x_1, dots,X_n leq x_n)= prod_{i=1}^n F_{X_i|X_{i-1}, dots,X_1}(x_i) F_X^{-1}( tau_ text{joint})&amp;= left(F_{X_1}^{-1}( tau_1), dots,F_{X_n|X_{n-1}, dots}^{-1}( tau_n) right) end{align*} FX​(x)FX−1​(τjoint​)​=P(X1​≤x1​,…,Xn​≤xn​)=i=1∏n​FXi​∣Xi−1​,…,X1​​(xi​)=(FX1​−1​(τ1​),…,FXn​∣Xn−1​,…−1​(τn​))​ . Let’s Reparameterize on Sampled Quantiles . Naturally, since we are approximating the quantile function (inverse CDF), we choose the quantile loss to minimize. However, does this loss really leads to some divergence metric between pθp_ thetapθ​ and pXp_XpX​? In other words, are we doing the correct thing, eventually approximating the density function? . Validity . Let us compute the expected quantile loss over the distribution for a quantile qqq, following these steps: . Expand the definition of Expectation. | Split the first integral, and merge one of them with the second. | Split the first integral again, and evaluate the second according to the definition of Expectation again. | Evaluate the first integral by the definition of CDF, and take the second integral by part, where u=xu=xu=x and dv=fP(x)dxdv=f_P(x)dxdv=fP​(x)dx. | Cancel the first two term, and we arrived at the final expression. | gτ(q)=EX∼P[ρτ(X−q)]=∫−∞q(x−q)(τ−1)fP(x)dx+∫q∞(x−q)τfP(x)dx=∫−∞q(q−x)fP(x)dx+∫−∞∞(x−q)τfP(x)dx=q∫−∞qfP(x)dx−∫−∞qxfP(x)dx+(EX∼P[X]−q)τ=qFP(q)−([xFP(x)]−∞q−∫−∞qFP(x)dx)+(EX∼P[X]−q)τ=∫−∞qFP(x)dx+(EX∼P[X]−q)τ begin{align*} g_ tau (q)&amp;= mathbb{E}_{X sim P} left[ rho_ tau left(X-q right ) right ] &amp;= int_{- infty }^{q}(x-q)( tau-1)f_P(x)dx+ int_{q}^{ infty}(x-q) tau f_P(x)dx &amp;= int_{- infty}^{q}(q-x)f_P(x)dx+ int_{- infty}^{ infty}(x-q) tau f_P(x)dx &amp;=q int_{- infty}^{q}f_P(x)dx- int_{- infty}^{q}xf_P(x)dx+ left( mathbb{E}_{X sim P} left[X right ]-q right ) tau &amp;=qF_P(q)- left( left[xF_P(x) right ]_{- infty}^q - int_{- infty}^q F_P(x)dx right )+ left( mathbb{E}_{X sim P} left[X right ]-q right ) tau &amp;= int_{- infty}^q F_P(x)dx+ left( mathbb{E}_{X sim P} left[X right ]-q right ) tau end{align*} gτ​(q)​=EX∼P​[ρτ​(X−q)]=∫−∞q​(x−q)(τ−1)fP​(x)dx+∫q∞​(x−q)τfP​(x)dx=∫−∞q​(q−x)fP​(x)dx+∫−∞∞​(x−q)τfP​(x)dx=q∫−∞q​fP​(x)dx−∫−∞q​xfP​(x)dx+(EX∼P​[X]−q)τ=qFP​(q)−([xFP​(x)]−∞q​−∫−∞q​FP​(x)dx)+(EX∼P​[X]−q)τ=∫−∞q​FP​(x)dx+(EX∼P​[X]−q)τ​ . Obviously, FP−1(τ)F_P^{-1}( tau)FP−1​(τ) is the true quantile function, so it minimizes the expected quantile loss over PPP. Let us get an expression on relative difference, . gτ(q)−gτ(FP−1(τ))=∫FP−1(τ)qFP(x)dx+(FP−1(τ)−q)τ=∫FP−1(τ)q(FP(x)−τ)dx begin{align*} g_ tau(q)-g_ tau(F_P^{-1}( tau))&amp;= int_{F_P^{-1}( tau)}^{q}F_P(x)dx+ left(F_P^{-1}( tau)-q right ) tau &amp;= int_{F_P^{-1}( tau)}^{q} left(F_P(x)- tau right )dx end{align*} gτ​(q)−gτ​(FP−1​(τ))​=∫FP−1​(τ)q​FP​(x)dx+(FP−1​(τ)−q)τ=∫FP−1​(τ)q​(FP​(x)−τ)dx​ . Suppose we have a distribution QQQ, whose quantile function is FQ−1(τ)F_Q^{-1}( tau)FQ−1​(τ), then the expected relative loss over all τ tauτ&#39;s are the following. Finally, we observed that there exists some metric on two distributions, called the Quantile Divergence. . Eτ∼U([0,1])[gτ(FQ−1(τ))−gτ(FP−1(τ))]=∫01[∫FP−1(τ)FQ−1(τ)(FP(x)−τ)dx]dτEτ∼U([0,1])[gτ(FQ−1(τ))]=∫01[∫FP−1(τ)FQ−1(τ)(FP(x)−τ)dx]dτundefinedQuantile&nbsp;divergence&nbsp;q(P,Q)+Eτ∼U([0,1])[gτ(FP−1(τ))]undefineddoes&nbsp;not&nbsp;depend&nbsp;on&nbsp;Q begin{align*} mathbb{E}_{ tau sim mathcal{U}([0,1])} left[g_ tau left(F_Q^{-1}( tau) right )-g_ tau left(F_P^{-1}( tau) right ) right ]&amp;= int_{0}^{1} left[ int_{F_P^{-1}( tau)}^{F_Q^{-1}( tau)} left(F_P(x)- tau right )dx right ]d tau mathbb{E}_{ tau sim mathcal{U}([0,1])} left[g_ tau left(F_Q^{-1}( tau) right ) right]&amp;= underbrace{ int_{0}^{1} left[ int_{F_P^{-1}( tau)}^{F_Q^{-1}( tau)} left(F_P(x)- tau right )dx right ]d tau}_{ text{Quantile divergence }q(P,Q)} &amp; quad+ underbrace{ mathbb{E}_{ tau sim mathcal{U}([0,1])} left[g_ tau left(F_P^{-1}( tau) right ) right ]}_{ text{does not depend on }Q} end{align*} Eτ∼U([0,1])​[gτ​(FQ−1​(τ))−gτ​(FP−1​(τ))]Eτ∼U([0,1])​[gτ​(FQ−1​(τ))]​=∫01​[∫FP−1​(τ)FQ−1​(τ)​(FP​(x)−τ)dx]dτ=Quantile&nbsp;divergence&nbsp;q(P,Q) . ∫01​[∫FP−1​(τ)FQ−1​(τ)​(FP​(x)−τ)dx]dτ​​+does&nbsp;not&nbsp;depend&nbsp;on&nbsp;Q . Eτ∼U([0,1])​[gτ​(FP−1​(τ))]​​​ . Quantile Divergence . This means that modeling the quantile function with quantile loss does lead to an eventual approximation to the true distribution. Let us have a closer look at how quantile divergence measures the difference between two distributions. . . Correction: The integrand should be Fp(x)−τF_p(x)- tauFp​(x)−τ, credits to 4. | . For a given τ tauτ, we can see the integral evaluates to a blue area, and we are summing them over all τ tauτ&#39;s. Therefore, it is obvious that this integral disappears if two quantile function match exactly for every τ tauτ, and this proves the statement that quantile loss will not miss any low density region of the distribution. . Unbiased Estimate . Finally, if we take the gradient over the expected relative quantile loss, we are getting an unbiased estimate to the gradient of quantile divergence. Once again, this proves that the new scheme works, leading to an approximation to the true distribution. . ∇θEτ∼U([0,1])[gτ(Qθˉ(τ))]=Eτ∼U([0,1])EX∼P[∇θρ(X−Qθˉ(τ))]=∇θq(P,Qθˉ) begin{align*} nabla_ theta mathbb{E}_{ tau sim mathcal{U}([0,1])} left[g_ tau left( bar{Q_ theta}( tau) right ) right ]&amp;= mathbb{E}_{ tau sim mathcal{U}([0,1])} mathbb{E}_{X sim P} left[ nabla_ theta rho left(X- bar{Q_ theta}( tau) right ) right ] &amp;= nabla_ theta q left(P, bar{Q_ theta} right ) end{align*} ∇θ​Eτ∼U([0,1])​[gτ​(Qθ​ˉ​(τ))]​=Eτ∼U([0,1])​EX∼P​[∇θ​ρ(X−Qθ​ˉ​(τ))]=∇θ​q(P,Qθ​ˉ​)​ . Source of Randomness . We know that, specfically for VAEs, there is a reparameterization trick that separates the source of randomness to a standard Gaussian distribution. Now we models the quantile function, how do we get samples from it? Where is the source of randomness now? . It is τ∼U([0,1]) tau sim mathcal{U}([0,1])τ∼U([0,1]). Since quantile functions are essentially inverse CDFs, taking uniform random τ tauτ, and feeds it to the model will give us a sample back. Here is an illustration on how it works. . . Results . Gated PixelCNN5 is a model that we try to modify. The original formulation have a location-dependent conditioning variable, which will not be used to condition on the random source τ tauτ. Therefore, the modified version PixelIQN will produce pixel values directly, instead of outputing a discreet distribution over 256 levels of RGB values each. . . PixelIQN Architecture, similar to Gated PixelCNN with τ tauτ taking the place of location-dependent conditioning. Credits to 4. | . The dataset used are CIFAR-10 and ImageNet 32x32, with metrics including Fréchet inception distance (FID) (lower is better) and Inception Score (IS) (higher is better). . Training and Performance . . Training curves. Dotted lines correspond to models trained with class-label conditioning. Credits to 4. | . . Inception score and FID for CIFAR-10 and ImageNet. PixelIQN(1) is the small 15-layer version of the model. Models marked * refer to class-conditional training. Credits to 4. | . Samples . . CIFAR-10: Real example images (left), samples generated by PixelCNN (center), and samples generated by PixelIQN (right). Credits to 4. | . . ImageNet 32x32: Real example images (left), samples generated by PixelCNN (center), and samples generated by PixelIQN (right). Credits to 4. | . Inpainting . . Small ImageNet inpainting examples. Left image is the input provided to the network at the beginning of sampling, right is the original image, columns in between show different completions. Credits to 4. | . Class Conditioning . . Class-conditional samples from PixelIQN. Credits to 4. | . Conclusion . Authors recognized that most current state-of-the-art models is built on top of development of Autoregressive modesl, VAEs, and GANs, which they all employ KL-divergence as the measure between two distributions. Now, we use the quantile function and quantile loss to achieve the same tasks, which may be more suitable for certain tasks that cares for low density region of the distrbution. . Although this new approach will not reduce training/inference time, it signifies an important perspective to look at the density estimation. . . van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks. In Proceedings of the International Conference on Machine Learning, 2016c. ↩︎ . | Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. ↩︎ . | Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672–2680, 2014. ↩︎ . | Ostrovski, Georg, Will Dabney, and Rémi Munos. Autoregressive quantile networks for generative modeling. In International Conference on Machine Learning. PMLR, 2018. ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., and Kavukcuoglu, K. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems, pp. 4790–4798, 2016b. ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/pixelrnn/quantiles/2021/09/09/AR2_blog2.html",
            "relUrl": "/pixelrnn/quantiles/2021/09/09/AR2_blog2.html",
            "date": " • Sep 9, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "AR2 Pixel recurrent neural networks",
            "content": "Pixel Recurrent Neural Networks . Modeling the distribution of high-dimensional data is a central problem in unsupervised machine learning. Since images are high-dimensional and highly structured, estimating their underlying distribution is notoriously challenging. With the recent advances in deep learning, there has been significant progress in developing expressive, scalable, and tractable methods to tackle generative modeling problems. In this blog, we are going to explore the PixelRNN 1 and GatedPixelRNN 2 models for generating images. . Related Work . Perhaps the most popular technique for generative modeling in recent years has been the Generative Adversarial Network. These models generate rich and sharp images. However, GANs are notoriously hard to train because of instability due to the adversarial nature of training.3 . On the other hand, stochastic latent variable models such as the Variational Auto-Encoder produce blurry samples due to the nature of its reconstruction loss. Additionally, the VAE exproximates a lower bound (ELBO) to the desired probability distribution. 4 . Previous methods that model the distribution as a product of conditionals such as NADE/MADE are limited because they lack sophisticated recurrent units like LSTM cells. 5 6 By using sophisticated auto-regressive modeling techniques, the PixelRNN is able to achieve state-of-the-art performance on image generation benchmarks. . Background . PixelRNN Model . In PixelRNN, each pixel is conditionally dependent on previous pixels from top to bottom and left to right. We model the joint probablity distribution of the image as a product of the conditional probabilities. Image taken from paper 1 . . In the following image, the pixel xix_ixi​ depends on all pixels [x1,x2,...,xi−1][x_1, x_2, ..., x_{i-1}][x1​,x2​,...,xi−1​] from top to bottom and left to right. Visualization taken from 7 . . Additionally, each channel RGB is conditionally dependent on previous channels. For example, the green channel is conditionally dependent on the red channel of the same pixel. Image taken from paper 1 . . As we will see later in this blog, ensuring this auto-regressive property holds requires clever masking of inputs in the network. . Discrete Softmax Output . The output layer of the network has a n×n×3×256n times n times 3 times 256n×n×3×256 shape. This can be interpreted as each channel for each pixel having a 256 channel output. This 256 channel output is normalized via softmax and represents the discrete multinomial probability distribution for channel values. The following is a visualization of the softmax output for one channel for one pixel. Image taken from paper 1 . . . PixelRNN Generation . Images are generated sequentially pixel-by-pixel and channel-by-channel from top-to-bottom and left-to-right. This makes the generation process extremely slow which is a big weakness of this model. However, training the PixelRNN model can be done in parallel since all the conditional inputs are present. The inputs just need to be masked to preserve the autoregressive property. Visualization taken from 7 . . . PixelRNN Network Architecture . The model always start with a 7×77 times 77×7 masked convolution. This is then followed by several residual blocks which can either be convolutional, RowLSTM, or DiagonalBiLSTM. Finally, there are two 1×11 times 11×1 convolutional layers to generate the final output. Image taken from paper 1 . . . Input Masking . There are two types of masks in the PixelRNN network. The first type (Mask A) exists to maintain the autoregressive property for the first 7×77 times 77×7 convolutional layer. In this mask, the output of the layer depends on all information from previous pixels and only information from previous channels of the same pixel. The second variant (Mask B) is applied to subsequent layers. This variant also allows the output of the layer to depend on the information from the same channel of the same pixel. This is because the channel values for subsequent layers only depends on inputs from previous channel and can be used without violating the autoregressive property. Below is a visualization of these two masking schemes. Image taken from paper 1 . . Below is a visualization of Mask A for the red, green, and blue channels respectively. We are masking the inputs to generate the center pixel for every channel. . Below is a visualization of Mask B for the red, green, and blue channels respectively. Note how the pixel of the same channel can be used this time. . . PixelCNN . In the PixelCNN each residual block is masked 3×33 times 33×3 convolution. PixelCNN is heavily parallelizable due to its convolutional layers. However, as we are only looking at a 3×33 times 33×3 neighborhood for the convolution, we are not capturing information from all previous pixels. While the receptive field of the convolution grows linearly with the depth of the network, in one particular layer, the masked 3×33 times 33×3 has a small receptive field. The image below is a visualization of the receptive field of the masked 3×33 times 33×3 convolution. Image taken from paper 1 . . . RowLSTM . RowLSTM generates its output row-by-row from top-to-bottom and left-to-right. To model the output, RowLSTM modifies the traditional LSTM cell to compute all hidden outputs via convolutions. RowLSTM uses 3×13 times 13×1 convolutions for the he state-to-state kernel KssK^{ss}Kss and input-to-state kernel KisK^{is}Kis. Note that the input-to-state component depends on the input and must be appropriately masked to ensure the autoregressive property. Additionally, since the input-to-state component depends only on the input, it can be computed for the entire n×nn times nn×n input in parallel. However, the state-to-state component of the RowLSTM convolution must be computed sequentially using previous hidden states. Below is the mathematical notation for the convolutional LSTM cell in RowLSTM. Image taken from paper 1 . . The image below is a visualization of the convolutions in the RowLSTM. The 3×13 times 13×1 convolution slides left-to-right row-by-row. Visualization taken from 7 . . Because of the sequential nature of the computation, RowLSTM is more computational intensive than convolutional layers. However, the hidden state for RowLSTM encapsulates a much larger context than convolutional layers. Specifically, the RowLSTM captures the entire triangular context above the output pixel. The image below is a visualization of the receptive field of the RowLSTM. Image taken from paper 1 . . . DiagonalBiLSTM . While the RowLSTM is an improvement on the convolutional layers in terms of receptive field, there is still room for improvement. This is where the DiagonalBiLSTM comes in. The goal of the DiagonalBiLSTM is to capture all the available context. In order to accomplish this, the DiagonalBiLSTM scans the diagonals of the image from two directions; top-left to bottom-right and top-right to bottom-left. The outputs from these two scans are added together for the final output. Similar to RowLSTM, the DiagonalBiLSTM uses a convolutional LSTM framework with a 1×11 times 11×1 convolution for the input-to-state kernel KisK^{is}Kis and a 2×12 times 12×1 convolution for the state-to-state kernel. Additionally, the KisK^{is}Kis convolution must be masked to preserve the autoregressive property and can be precomuted for the entire output. Visualization taken from 7 . . The above image shows how the DiagonalBiLSTM generates its output from the top-left to bottom-right diagonal. Implementing this diagonal 2×12 times 12×1 is tricky. In order to simplify the compute, the image is skewed to rearrange the convolution as shown in the image below. Visualization taken from 7 . . Note that the the DiagonalBiLSTM computes this operation for both diagonals. As a result, it is able to capture all the available context to generate outputs and has a complete receptive field. However, the DiagonalBiLSTM has even more computational overhead because of computing two outputs. Image taken from paper 1 . . The image above shows how the receptive field for the DiagonalBiLSTM is able to capture the entire available context to generate its output. . Residual Connections . As mentioned earlier, each of the blocks (convolutional, RowLSTM, DiagonalBiLSTM) are residual. Residual connections enable training deeper PixelRNN networks. These residual connections increase both convergence speed by propagating signals more directly through the network. The image below is a visualization on how residual connection are setup in the convolutional and LSTM cells. Image taken from paper 1 . . . PixelRNN Model Summary . This visualization summarizes the model architecture of the PixelCNN, RowLSTM, and Diagonal BiLSTM variants of the model. Image taken from paper 1 . . . Preliminary Results . As shown in the image below, PixelRNN variants achieve state-of-the-art performance in common datasets (MNIST, CIFAR-10, and ImageNet). The best variant model is the DiagonalBiLSTM, which is expected since it has the largest receptive field. Image taken from paper 1 . . . Gated PixelCNN Motivation . The authors of the PixelRNN paper released another paper shortly after the first one that improved upon the design on the PixelCNN. The authors reasoned that PixelRNN variants (RowLSTM and DiagonalBiLSTM) are outperforming PixelCNN for two main reasons: . The element-wise multiplicative units are able to model more complex interactions. The absence of multiplicative operations in PixelCNN is limiting its performance. | PixelRNN’s capture much larger receptive fields. While the receptive fields of the PixelCNN grows linearly with the number of layers, there is a blind spot that forms in the receptive field of masked CNNs (more information below). | The authors proposed modification to the PixelCNN architecture to fix its shortcomings. First, they added Gated Activation Units that contained multiplicative operations to add more sophistication to the model. Second, they fixed the receptive field blind-spot problem by splitting up the convolution into an unmasked vertical stack and a masked horizontal stack that takes the output of the vertical stack as input. . Horizontal and Vertical Stack . As mentioned above, the receptive field of the PixelCNN, while increasing linearly with depth, contains a growing blind-spot. Pixels in this blind-spot are never used as context regardless of how many layers are stacked. The blind-spot problem is caused because of the masking in the convolutions to maintain the autoregressive property. The image below is a visualization of the blind-spot problem. Image taken from paper 2 . . . To fix the receptive field blind-spot, the single masked convolution is replaced with a horizontal and vertical stack. The vertical stack is an unmasked operation that captures the entire receptive field in the rows above the output pixel. The horizontal stack is a masked operation that captures the context to the left of the output pixels and uses the vertical stack output as input. Also, the authors add an additional residual connection in the horizontal stack convolution. Splitting up the convolution in this way fixes the receptive field blind-spot as shown in the figure below. Image taken from paper 2 . . . Gated Activation Units . Additionally, the authors replace the ReLU activation between convolutional blocks with a more sophisticated gated activation unit. This new activation computes two different convolutions with half the feature maps. The output of the two convolutions are subjected to two different non-linear activation functions (tanh and sigmoid) and multiplied together element-wise for the final output. Image taken from paper 2 . . The image above shows the mathematical definition of the gated activation unit. The final convolutional block is shown in the image below. The green n×nn times nn×n convolution is the vertical stack and the green n×1n times 1n×1 convolution is the horizontal stack. The output of the vertical stack is fed into the horizontal stack as mentioned above. Image taken from paper 2 . . . Conditional PixelCNN . In my opinion, the most interesting part of the paper is adding the ability to conditionally generate images. You can condition the output probability distribution of the images on a high-dimensional latent vector hhh that behaves as the image description. For example, the hhh vector could be a one-hot encoded vector of class labels in the ImageNet dataset. The PixelCNN network would then learn to conditionally generate specific classes of ImageNet data. So passing in the latent vector hhh corresponding to the class “Dog” would generate images of dogs! The equation given below shows how the output probability is now conditionally dependent on hhh. Image taken from paper 2 . . . However, the latent vector hhh does not contain any spatial information about the object. So in the above example, while images of dogs would be generated, the dog could appear anywhere in the image. Fortunately, the authors of the paper had a solution to this problem. The latent vector hhh can be passed through a deconvolution network to produce output s=m(h)s = m(h)s=m(h) such that sss has the same spatial dimensions as the image but arbitrary channels. sss contains spatial information about the generated object. Now you can control where in the image the dog is generated! . The equations below show how the gated activation unit in the network is modified to accommodate conditional generation. For the hhh, the Vk,fV_{k,f}Vk,f​ is a linear layer and for sss the Vk,fV_{k,f}Vk,f​ is a 1×11 times 11×1 convolution. Image taken from paper 2 . . . Final Results . The image below shows the performance of the GatedPixelCNN on CIFAR-10 (left) and ImageNet (right) from paper 2 . . . Conditional Generation Examples . Here are some examples from the paper for condtional image generation from the ImageNet dataset from paper 2 . . . . Oord, Aaron van den, et al. “Pixel Recurrent Neural Networks.” ArXiv:1601.06759 [Cs], Aug. 2016. arXiv.org, http://arxiv.org/abs/1601.06759 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Oord, Aaron van den, et al. “Conditional Image Generation with PixelCNN Decoders.” ArXiv:1606.05328 [Cs], June 2016. arXiv.org, http://arxiv.org/abs/1606.05328 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . | Recent research has made progress in demystifying the problems in training GANs. However, when the initial PixelRNN paper was published in 2016, training GANs for generative modeling was still a daunting task). ↩︎ . | Advances in VAE have made it possible to generative sharp high-dimensional data by using hierarchical techniques and modifying the ELBO loss for better reconstructions. ↩︎ . | Uria, Benigno, et al. “Neural Autoregressive Distribution Estimation.” ArXiv:1605.02226 [Cs], May 2016. arXiv.org, http://arxiv.org/abs/1605.02226. ↩︎ . | Germain, Mathieu, et al. “MADE: Masked Autoencoder for Distribution Estimation.” ArXiv:1502.03509 [Cs, Stat], June 2015. arXiv.org, http://arxiv.org/abs/1502.03509. ↩︎ . | Slides from UCF PixelRNN presentation by Logan Lebanoff 2/22/17. https://www.crcv.ucf.edu/wp-content/uploads/2019/03/CAP6412_Spring2018_Pixel-Recurrent-Neural-Networks.pdf ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ . |",
            "url": "https://cs598ban.github.io/Fall2021/pixelrnn/quantiles/2021/09/09/AR2_blog.html",
            "relUrl": "/pixelrnn/quantiles/2021/09/09/AR2_blog.html",
            "date": " • Sep 9, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cs598ban.github.io/Fall2021/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}