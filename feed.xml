<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://cs598ban.github.io/Fall2021/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cs598ban.github.io/Fall2021/" rel="alternate" type="text/html" /><updated>2021-11-18T21:01:55-06:00</updated><id>https://cs598ban.github.io/Fall2021/feed.xml</id><title type="html">CS 598 Deep Generative and Dynamical Models</title><subtitle>Blogs created as a part of graduate course at UIUC on deep generative and dynamical models.</subtitle><entry><title type="html">Review on Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</title><link href="https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/28/VAE4_blog.html" rel="alternate" type="text/html" title="Review on Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations" /><published>2021-09-28T00:00:00-05:00</published><updated>2021-09-28T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/28/VAE4_blog</id><author><name></name></author><category term="Variational Autoencoder" /><summary type="html">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations This is the best paper [2] in ICML 2019, which incurred huge controversy at that time. It heavily criticizes the previous works on disentanglement, but some claims of it are regarded to be too strong. I will introduce those assumptions challenged by this paper. Although I find some arguments not well supported, most of the conclusions from this paper are actually valuable and inspiring for the later works on the disentanglement. Introduction to disentanglement There’s actually no formal definition of disentanglement right now. Intuitively, disentangled representation should be compact and interpretable, where each dimension of the representation is informative and independent. Consider two independent random variables aaa and bbb, then x=[a+b,a−b]\mathbf{x}=[a+b,a-b]x=[a+b,a−b] is an entangled representation while x=[a,b]\mathbf{x}=[a,b]x=[a,b] is a disentangled representation. These two representations actually contain the same information about aaa and bbb but the disentangled representation is expected to be more interpretable and more useful for downstream tasks, such as controllable sample generation and robot manipulation. For a long period, many VAE-based methods like β\betaβ-VAE [1], with additional tricks to encourage the dimension independence of the latent representation, have been proposed for disentanglement. But all these methods are based on some common assumptions and they are not carefully verified. Disentanglement is impossible without inductive bias This paper claims that, for an arbitrary generative model, the disentanglement is actually impossible. For each disentanglement representation zzz, there exists an inifinite family of bijective functions f(z)f(z)f(z), where f(z)f(z)f(z) is entangled but it shares the same marginal distribution with zzz. In other words, there are infinitely many generative models which have the same marginal distribution for the observation xxx, and without inductive bias, there’s no guarantee the one we obtain gives the disentangled representation. This theorem is also similar to the well-known “No free lunch theorem” [9]. Therefore, it’s necessary for each disentanglement method to clearly define its inductive bias. Challenging the common assumptions behind disentanglement learning This paper investigates several assumptions behind the disentanglement learning. It considers 6 distanglement methods, including β\betaβ-VAE [1], AnnealedVAE [6], FactorVAE [5], β\betaβ-TCVAE [3], DIP-VAE-I and DIP-AVE-II [4]. It also uses 6 metrics for measuring disentanglement, including BetaVAE metric [1], FactorVAE metric [5], Mutual Information GAP (MIG) [3], Modularity [7], DCI Disentanglement gap (named as “disentanglement metric” originally) [8], and SAP score [4]. The experiments are conducted on datasets dSprites, Cars3D, SmallNORB, Shapes3D, Color-dSprites, Noisy-dSprites and Scream-dSprites. Mean representation of the latent variables are correlated It’s a common practice to use the mean vector of the Gaussian encoder as the representation of the latent variable for evaluation. However, it turns out that although the samples from the Gaussian encoder have uncorrelated dimensions, the mean vector doesn’t internally have this property. Constrained by a stronger regularization, as shown in Fig 1, the total correlation, which measures the correlation among dimensions, of the sampled representation indeed goes down (left) but the total correlation of the mean representation increases (right) instead, except for DIP-VAE-I which directly optimizes the covariance matrix of the mean representation to be diagonal. Fig 1. Total correlation among dimensions of latent representations, mean representation (left) and sampled representation (right). Source: Locatello et al. [2] Disentanglement metrics are correlated The second question is whether all these metrics measuring the disentanglement are correlated. And the results give the positive answer. All metrics except Modularity are mildly correlated. Fig 2. Correlation among metrics. Source: Locatello et al. [2] Importance of models and hyperparameters All these methods claim that they get a better disentangled representation, but whether the improvement in their metrics is from more disentanglement remains unknown. In the experiment, each model is run over different random seeds, but it turns out that these methods have large overlappings (left in Fig 3) in their performances. In other words, a good random seed is more meaningful than a good objective. The same conclusion holds for the hyperparameter (right in Fig 3). Fig 3. Violin plots of disentanglement scores over random seeds for different models (left) and different hyperparameters (right). Source: Locatello et al. [2] Recipes for hyperparameter selection The paper now considers the strategy to select a good hyperparameter for a model. However, all these metrics require a substantial amount of labels or a full generative model, so we need to consider the hyperparameter selection in an unsupervised manner. Unfortunately, no model could dominate others all the time and there does not exist a hyperparameter selection strategy that works consistently well as shown in Fig 4. Additionally, there’s also no strategy to identify a good and a bad run for different random seeds. Fig 4. Model performances under different hyperparameters on different datasets. Source: Locatello et al. [2] Specifically, the paper investigates the unsupervised losses and transfer performances, which can also serve as a strategy to select the hyperparamter without supervision on the target dataset. For the unsupervised losses, including the reconstruction error, KL divergence between the prior and the approximate posterior, evidence lower bound (ELBO), and the estimated total correlation of the sampled representation, none of them are actually correlated with the disentanglement metrics (Fig 5). Fig 5. Correlation between disentanglement scores and unsupervised losses. Source: Locatello et al. [2] The transferring fails as well. When the model is transferred across the same metric and same dataset (different random seeds), there's 80.7% chance the model performance is not worse than the random model selection. However, this result drops to 59.3% for different datasets and further drops to 54.9% when metrics are also different. One example is shown in Fig 6. Fig 6. Model performance after transferring. Source: Locatello et al. [2] Benefits of disentanglement Finally, this paper explores the benefits of the disentanglement. The disentangled representation is intuitively believed to be more useful for downstream tasks, and able to reduce the sample complexity of learning. In the experiments, the downstream performances show high correlation with the disentanglement scores (Fig 7), but the authors are careful with the conclusion and doubts the source of the correlation, which could be either the disentanglement or the relevant information embedded in the representation. I think the experiments here are incomplete, where authors can actually build entangled representations from the disentangled ones and evaluate the performance of the entangled representations. This comparison could give the idea where the correlation comes from. Fig 7. Correlation between downstream performances and disentanglement scores. Source: Locatello et al. [2] Besides, the experimental results show no clear correlation between the disentanglement scores and sample efficiencies (Fig 8). Fig 8. Correlation between sample efficiencies and disentanglement scores. Source: Locatello et al. [2] Future directions This paper proposes three principles for the future work on the disentanglement based on the experiments before. Inductive biases and implicit and explicit supervision. As proved by this paper, the inductive bias is necessary for the disentangled methods, which should be made clear in the later works. Besides, it’s demonstrated by the experimental results that it’s impossible for the hyperparameter selection under no supervision, the supervision parts should also be explicitly specified. Concrete practical benefits of disentangled representations. Previous works take it for grant that disentangled representation is better, however, this paper points out its benefits is not clear yet and quite data dependent. Therefore, the concrete benefits of disentangled representations should be specified under each context. Experimental setup and diversity of data sets. It’s shown that no model can consistently outperform others on all datasets, so it’s questionable whether these models really improve the disentanglement. A sound, robust, and reproducible experimental setup on a diverse set of data sets is needed to demonstrate the advantage of a disentangled method. References [1] 2017 (ICLR): I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, A. Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. ICLR, 2017. [2] 2019 (ICML): F. Locatello, S. Bauer, M. Lucic, G. RÃ¤tsch, S. Gelly, B. Scholkopf, O. Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. ICML, 2019. [3] 2018 (NeurIPS): T. Chen, X. Li, R. Grosse, D. Duvenaud. Isolating sources of disentanglement in variational autoencoders. NeurIPS, 2018. [4] 2018 (ICLR): A. Kumar, P. Sattigeri, A. Balakrishnan. Variational inference of disentangled latent concepts from unlabeled observations. ICLR, 2018. [5] 2018 (ICML): H. Kim, A. Mnih. Disentangling by factorising. NIPS, 2017. [6] 2017 (NIPS): C. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, A. Lerchner. Understanding disentangling in β\betaβ-VAE. NIPS, 2017. [7] 2018 (NIPS): K. Ridgeway, M. Mozer. Learning deep disentangled embeddings with the f-statistic loss. NIPS, 2018. [8] 2018 (ICLR): C. Eastwood, C. Williams. A framework for the quantitative evaluation of disentangled representations. ICLR, 2018. [9] 1997 (IEEE): D. Wolpert, W. Macready. No Free Lunch Theorems for Optimization. IEEE Transactions on Evolutionary Computation, 1997.</summary></entry><entry><title type="html">Review on Diagnosing and Enhancing VAE Models</title><link href="https://cs598ban.github.io/Fall2021/variational%20autoencoders/2021/09/21/VAE2_blog.html" rel="alternate" type="text/html" title="Review on Diagnosing and Enhancing VAE Models" /><published>2021-09-21T00:00:00-05:00</published><updated>2021-09-21T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/variational%20autoencoders/2021/09/21/VAE2_blog</id><author><name></name></author><category term="Variational Autoencoders" /><summary type="html">Diagnosing and Enhancing VAE Models (ICLR '19)1 Introduction Even though variational autoencoders (VAEs)2 have a wide variety applications in deep generative models, many aspects of the underlying energy function remain poorly understod. It is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this paper, the authors rigorously analyzed that reaching the global optimum does not guarantee that if VAE model can learn the true distribution of data, i.e., there could exist alternative solutions that both reach the global optimum and yet do not assign the same probability measure as ground-truth probability distribution. And it also proposed a two-stage remedy model, i.e., a two-stage VAE model to address the above issues and enhance the original VAE so that any gloablly minimizing solution is uniquely matched to the ground-truth distribution. Problem Definition: The starting point is the desire to learn a probabilistic generative model of observable variables x∈Xx \in \mathcal Xx∈Xwhere X\mathcal XX is a r-dimensional manifold embedded in Rd\mathbb R ^dRd Denote a ground-truth probability measure on X\mathcal{X}X as μgt\mu_{gt}μgt​ where ∫Xμgtdx=1\int_{\mathcal{X}} \mu_{gt} d\mathbf{x} = 1∫X​μgt​dx=1 The canonical VAE attempts to approximate this ground-truth measure using parameterized density pθ(x)p_{\theta}(\mathbf{x})pθ​(x) where pθ(x)=∫pθ(x∣z)p(z)dzp_{\theta}(x) = \int p_{\theta}(x | z) p(z) dzpθ​(x)=∫pθ​(x∣z)p(z)dz, z∈Rκz \in \mathbb{R}^\kappaz∈Rκ with κ≈r\kappa \approx rκ≈r and p(z)=N(z∣0,I)p(z) = \mathcal{N}(z | 0, \mathbf{I})p(z)=N(z∣0,I) We will consider two situations where r&amp;lt;dr &amp;lt; dr&amp;lt;d and r=dr = dr=d to illustrate the aforementioned non-uniqueness issues. VAE Objective In the vanilla VAE model, we normally write the objective function to be optimized as evidence lower bound (ELBO): Lθ,ϕ(x)=−Eqϕ(z∣x)[log⁡pθ(x,z)−log⁡qϕ(z∣x)]=KL(qϕ(z∣x)∣∣pθ(z))+Eqϕ(z∣x)[−log⁡pθ(x,z)]\begin{align*} \mathcal{L}_{\theta, \phi}(x) &amp;amp; = -\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x, z) - \log q_{\phi}(z|x)] \\ &amp;amp; = \mathbb{KL}(q_{\phi}(z|x) || p_{\theta}(z)) + \mathbb{E}_{q_{\phi}(z|x)}[-\log p_{\theta}(x, z)] \end{align*}Lθ,ϕ​(x)​=−Eqϕ​(z∣x)​[logpθ​(x,z)−logqϕ​(z∣x)]=KL(qϕ​(z∣x)∣∣pθ​(z))+Eqϕ​(z∣x)​[−logpθ​(x,z)]​ In this case, based on the ground-truth probability measure μgt\mu_{gt}μgt​, we can rewrite it into: Lθ,ϕ(x)=∫X{−log⁡pθ(x)+KL[qϕ(z∣x)∣∣pθ(z∣x)]}μgtdx≥∫X−log⁡pθ(x)μgtdxLθ,ϕ(x)=∫X{−Eqϕ(z∣x)[log⁡pθ(z∣x)]+KL[qϕ(z∣x)∣∣p(z)]}μgtdx\begin{align*} \mathcal{L}_{\theta, \phi}(x) &amp;amp; = \int_{\mathcal{X}}\{-\log p_{\theta}(x) + \mathbb{KL}[q_{\phi}(z|x) || p_{\theta}(z|x)]\} \mu_{gt} dx \geq \int_{\mathcal{X}} -\log p_{\theta}(x)\mu_{gt} dx \\ \mathcal{L}_{\theta, \phi}(x) &amp;amp; = \int_{\mathcal{X}} \{-\mathbb{E}_{q_{\phi}(z|x)} [\log p_{\theta}(z|x)] + \mathbb{KL}[q_{\phi}(z|x) || p(z)]\} \mu_{gt} dx \end{align*}Lθ,ϕ​(x)Lθ,ϕ​(x)​=∫X​{−logpθ​(x)+KL[qϕ​(z∣x)∣∣pθ​(z∣x)]}μgt​dx≥∫X​−logpθ​(x)μgt​dx=∫X​{−Eqϕ​(z∣x)​[logpθ​(z∣x)]+KL[qϕ​(z∣x)∣∣p(z)]}μgt​dx​ In principle, qϕ(z∣x)q_{\phi}(z|x)qϕ​(z∣x) and pθ(x∣z)p_{\theta}(x|z)pθ​(x∣z) can be arbitrary distributions. In the practical implementation, a commonly adopted distributional assumption is that both distribution are Gaussian, which was previously considered as a limitation of VAE. Diagnosing the Non-uniqueness Ideas: Even with the stated Gaussian distributions, there exist parameters θ,ϕ\theta, \phiθ,ϕ that can simultaneously: Globally optimize the VAE object Recover the ground-truth probability measure in a certain sense Definition 1: A κ\kappaκ-simple VAE is defined as a VAE model with dim[z\mathbf{z}z] = κ\kappaκ latent dimensions, the Gaussian encoder qϕ(z∣X)=N(z∣μz,Σz)q_{\phi}(z|X) = \mathcal{N}(z | \mu_z, \Sigma_z)qϕ​(z∣X)=N(z∣μz​,Σz​) and the Gaussian decoder pθ(x∣z)=N(x∣μx,Σx)p_{\theta}(x|z) = \mathcal{N}(x | \mu_x, \Sigma_x)pθ​(x∣z)=N(x∣μx​,Σx​) With these definitions, we can now move to the discussion of κ\kappaκ-simple VAE with κ≥r\kappa \geq rκ≥r can achieve the above optimality criteria from the simpler case where r=dr = dr=d followed by the extended scenario with r&amp;lt;dr &amp;lt; dr&amp;lt;d. When r=d Assuming pgt(x)=μgt(dx)/dxp_{gt}(x) = \mu_{gt}(dx) / dxpgt​(x)=μgt​(dx)/dx exists everywhere in Rd\mathbb{R}^dRd, the minimal possible value of negative log-likelihood will necessarily occur if KL[qϕ(z∣x)∣∣pθ(z∣x)]=0&amp;nbsp;and&amp;nbsp;pθ(x)=pgt(x)&amp;nbsp;almost&amp;nbsp;everywhere\mathbb{KL}[q_{\phi}(z|x) || p_{\theta}(z|x)] = 0 \text{ and } p_{\theta}(x) = p_{gt}(x) \text{ almost everywhere}KL[qϕ​(z∣x)∣∣pθ​(z∣x)]=0&amp;nbsp;and&amp;nbsp;pθ​(x)=pgt​(x)&amp;nbsp;almost&amp;nbsp;everywhere Naturally we will conclude that Theorem 2: Suppose that r=dr=dr=d and there exists a density pgt(x)p_{gt}(x)pgt​(x) associated with the ground-truth measure μgt\mu_{gt}μgt​ that is nonzero everywhere on Rd\mathbb{R}^dRd. Then for any κ≥r\kappa \geq rκ≥r, there is a sequence of κ\kappaκ-simple VAE model parameters {θt⋆,ϕt⋆}\{\theta_t^\star, \phi_t^\star\}{θt⋆​,ϕt⋆​} such that lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&amp;nbsp;and&amp;nbsp;lim⁡t→∞pθt⋆(x)=pgt(x)&amp;nbsp;almost&amp;nbsp;everywhere \lim_{t\to\infty} \mathbb{KL}[q_{\phi_t^\star}(z|x) || p_{\theta_t^\star}(z|x)] = 0 \text{ and } \lim_{t\to\infty} p_{\theta_t^\star}(x) = p_{gt}(x) \text{ almost everywhere} t→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&amp;nbsp;and&amp;nbsp;t→∞lim​pθt⋆​​(x)=pgt​(x)&amp;nbsp;almost&amp;nbsp;everywhere The theorem implies that as long as latent dimension is sufficiently large (i.e., κ≥r\kappa \geq rκ≥r), the optimal ground-truth probability measure can be recovered, whether the encoder and decder has Gaussian assumptions or not, since the ground-truth probability measure being recovered almost everywhere is the necessary conditions for optimized objective value. When r &amp;lt; d When both qϕ(z∣x)q_\phi(z|x)qϕ​(z∣x) and pθ(x∣z)p_{\theta}(x|z)pθ​(x∣z) are arbitrary/unconstrained, i.e., without Gaussian assumptions, then inf⁡ϕ,θL(θ,ϕ)=−∞\inf_{\phi, \theta} \mathcal{L}(\theta, \phi) = - \inftyinfϕ,θ​L(θ,ϕ)=−∞ by forcing qϕ(z∣x)=pθ(z∣x)q_{\phi}(z|x) = p_{\theta}(z|x)qϕ​(z∣x)=pθ​(z∣x). To show that this does not need to happen, define a manifold density p~gt(x)\tilde p_{gt}(x)p~​gt​(x) as the probability density of μgt\mu_{gt}μgt​ with respect to the volume measure of the manifold X\mathcal{X}X. If d=rd = rd=r then this volume is the standard Lebesgue measure in Rd\mathbb{R}^dRd and p~gt(x)=pgt(x)\tilde p_{gt}(x) = p_{gt}(x)p~​gt​(x)=pgt​(x) since when r&amp;lt;dr &amp;lt; dr&amp;lt;d, pgt(x)p_{gt}(x)pgt​(x) may not exist everywhere in the ambient space. Theorem 3: Assume r&amp;lt;dr &amp;lt; dr&amp;lt;d and that there exists a manifold density p~gt(x)\tilde p_{gt}(x)p~​gt​(x) associated with the ground-truth measure μgt\mu_{gt}μgt​ that is nonzero everywhere on X\mathcal{X}X. Then for any κ≥r\kappa \geq rκ≥r, there is a sequence of κ\kappaκ-simple VAE model parameters {θt⋆,ϕt⋆}\{\theta_t^\star, \phi_t^\star\}{θt⋆​,ϕt⋆​} such that lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&amp;nbsp;and&amp;nbsp;lim⁡t→∞∫X−log⁡pθt⋆(x)μgtdx=−∞\lim_{t\to\infty} \mathbb{KL}[q_{\phi_t^\star}(z|x) || p_{\theta_t^\star}(z|x)] = 0 \text{ and } \lim_{t \to \infty} \int_{\mathcal{X}} -\log p_{\theta_t^\star}(x) \mu_{gt} dx = -\inftyt→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&amp;nbsp;and&amp;nbsp;t→∞lim​∫X​−logpθt⋆​​(x)μgt​dx=−∞ lim⁡t→∞∫X∈Apθt⋆(x)dx=μgt(A∪X)\lim_{t\to\infty} \int_{\mathcal{X} \in A} p_{\theta_t^\star} (x) dx = \mu_{gt} (A \cup \mathcal{X})t→∞lim​∫X∈A​pθt⋆​​(x)dx=μgt​(A∪X) for all measurable sets A⊆RdA \subseteq \mathbb{R}^dA⊆Rd with μgt(∂A∪X)=0\mu_{gt}(\partial A \cup \mathcal{X}) = 0μgt​(∂A∪X)=0 where ∂A\partial A∂A is the boundary of AAA. Implications of this theorem: From (1), the VAE Gaussian assumptions do not prevent minimization of L(θ,ϕ)\mathcal{L}(\theta, \phi)L(θ,ϕ) from converging to minus infinity. From (2), there exists solutions that assign a probability mass to most all measurable subsets of Rd\mathbb{R}^dRd that is distinguishable from the ground-truth measure. In r=dr = dr=d situation, the theorem necessitates that the ground-truth probability measure has been recovered almost everywhere. In r&amp;lt;dr &amp;lt; dr&amp;lt;d situation, we have not ruled out the possibility that a different set of parameters {θ,ϕ}\{\theta, \phi\}{θ,ϕ} can push the lost to −∞- \infty−∞ and not achieve (2), i.e., the VAE can reach the lower bound of negative log-likelihood but fail to closely approximate μgt\mu_{gt}μgt​. Optimal Solutions The necessary conditions for VAE optimal value would be induced from the following theorems. Theorem 4: Let {θγ⋆,ϕγ⋆}\{\theta^\star_\gamma, \phi_\gamma^\star\}{θγ⋆​,ϕγ⋆​} denote an optimal κ\kappaκ-simple VAE solution (with κ≥r\kappa \geq rκ≥r) where the decoder variance γ\gammaγ is fixed. Moreover, we assume that μgt\mu_{gt}μgt​ is not a Gaussian distribution when d=rd = rd=r. Then for any γ&amp;gt;0\gamma &amp;gt; 0γ&amp;gt;0, there exists a γ′&amp;lt;γ\gamma' &amp;lt; \gammaγ′&amp;lt;γ such that L(θγ′⋆,ϕγ′⋆)&amp;lt;L(θγ⋆,ϕγ⋆)\mathcal{L}(\theta_{\gamma'}^\star, \phi_{\gamma'}^\star) &amp;lt; \mathcal{L}(\theta_{\gamma}^\star, \phi_{\gamma}^\star)L(θγ′⋆​,ϕγ′⋆​)&amp;lt;L(θγ⋆​,ϕγ⋆​) The theorem implies that if γ\gammaγ is not constrained, it must be that γ→0\gamma \to 0γ→0 if we wish to minimize the VAE objective. While in existing practical VAE applications, it is standard to fix γ≈1\gamma \approx 1γ≈1 with the standard Gaussian assumptions during training. Theorem 5: Applying the same conditions and definitions in Theorem 4, then for all xxx drawn from μgt\mu_{gt}μgt​, we also have that lim⁡γ→0fμx[fμz(x;ϕγ⋆)+fSz(x;θγ⋆)ϵ;ϕγ⋆]=lim⁡γ→0fμx[fμz(x;ϕγ⋆);θγ⋆]=x,∀ϵ∈Rκ\lim_{\gamma \to 0} f_{\mu_x} [f_{\mu_z}(x; \phi_{\gamma}^\star) + f_{S_z}(x; \theta{\gamma}^\star) \epsilon; \phi_\gamma^\star] = \lim_{\gamma \to 0} f_{\mu_x}[f_{\mu_z}(x;\phi_\gamma^\star); \theta_\gamma^\star] = x, \forall \epsilon \in \mathbb{R}^\kappaγ→0lim​fμx​​[fμz​​(x;ϕγ⋆​)+fSz​​(x;θγ⋆)ϵ;ϕγ⋆​]=γ→0lim​fμx​​[fμz​​(x;ϕγ⋆​);θγ⋆​]=x,∀ϵ∈Rκ With this theorem, it indicates that any x∈X\mathbf{x} \in \mathcal{X}x∈X will be perfectly reconstructed by the VAE model at globally optimal solutions. Adding dimensions to latent dimension cannot improve the value of the VAE data term in meaningful way. In the training process, there are likely to be rrr eigenvalues of the decoder covariance converging to 0 and κ−r\kappa - rκ−r converging to one. This demonstrats that VAE has the ability to detect the manifold dimension and select the proper number of latent dimensionsin practical environments. If VAE model parameters have learned a near optimal mapping onto X\mathcal{X}X using γ≈0\gamma \approx 0γ≈0, then the VAE cost will scale as (d−r)log⁡γ(d - r) \log \gamma(d−r)logγ regardless of μgt\mu_{gt}μgt​. Two-Stage VAE Model The above analysis suggests the following two-stage remedy: Given nnn observed samples {x(i)}i=1n\{x^{(i)}\}^n_{i=1}{x(i)}i=1n​, train a κ\kappaκ-simple VAE, with κ≥r\kappa \geq rκ≥r, to estimate the unknown rrr-dimensional ground-truth manifold X\mathcal{X}X embedded in Rd\mathcal{R}^dRd using a minimal number of active latent dimensions. Generate latent samples {z(i)}i=1n\{z^{(i)}\}^n_{i=1}{z(i)}i=1n​ via z(i)∼qϕ(z∣x(i))z_{(i)} \sim q_{\phi}(z|x^{(i)})z(i)​∼qϕ​(z∣x(i)). Train a second κ\kappaκ-simple VAE, with independent parameters {θ′,ϕ′}\{\theta', \phi'\}{θ′,ϕ′} and latent representation uuu, to learn the unknown distribution qϕ(z)q_\phi(z)qϕ​(z) as a new ground-truth distribution and use samples {z(i)}i=1n\{z^{(i)}\}^n_{i=1}{z(i)}i=1n​ to learn it. Samples approximating the original ground-truth μgt\mu_{gt}μgt​ can then be formed via the extended ancestral process u∼N(u∣0,I),z∼pθ′(z∣u),x∼pθ(x∣z)u \sim \mathcal{N}(u | 0, \mathbf{I}), z \sim p_{\theta'}(z | u), x \sim p_{\theta}(x|z)u∼N(u∣0,I),z∼pθ′​(z∣u),x∼pθ​(x∣z) The structure of the first-stage of the Two-Stage VAE Model Analysis: If the first stage was successful, then even though they will not generally resemble N(z∣0,I)\mathcal{N}(z|0, \mathbf{I})N(z∣0,I), samples from qϕ(z)q_\phi(z)qϕ​(z) will have nonzero measure across the full ambient space Rκ\mathbb{R}^\kappaRκ. If κ&amp;gt;r\kappa &amp;gt; rκ&amp;gt;r, then the extra latent dimensions will be naturally filled in via randomness. Consequently, as long as we set κ≥r\kappa \geq rκ≥r, the operational regime of the second-stage VAE is effectively equivalent to the situation that the manifold dimension is equal to the ambient dimension, and reaching global optimum solutions would recover the ground-truth probability measure almost everywhere. Experiment Results The following table indicates the performance evaluation results of the experiments conducted on four significantly different datasets: MNIST, Fash-ion MNIST, CIFAR-10 and CelebA. The evaluation metrics used Frchet Inception Distance (FID)3 Score: used to assess the quality of images created by a generative model, comparing the generated images with the distribution of real images. Note: The training of two stages need to be separate. Concatenating two stages and jointly training does not improve the performance. Another set of experiments were conducted on the same datasets with different evaluation metrics. Kernel Inception Distance (KID)4 applies a polynomial-kernel Maximum Mean Discrepancy (MMD) measure to estimate the inception distance, as FID score is believed to exhibit bias in certain circumstances. Analysis of the Results: The second stage of Two-Stage VAE model can reduce the gap between q(z)q(z)q(z) and p(z)p(z)p(z), resulting in better manifold reconstruction. γ\gammaγ will converge to zero at any global minimum of the VAE objective, allowing for tighter image reconstructions with better manifold fit. Contributions and Conclusions This paper rigorously proved that VAE global optimum can in fact uniquely learn a mapping to the correct ground-truth manifold when r&amp;lt;dr &amp;lt; dr&amp;lt;d, but not necessarily the correct probability measure within this manifold. The proposed Two-Stage VAE model can resolve this issue and better recover the ground-truth manifold and reduce the gap between pθ(z∣x)p_\theta(z|x)pθ​(z∣x) and qϕ(z∣x)q_\phi(z|x)qϕ​(z∣x). And this is the first demonstration of a VAE pipeline that can produce stable FID scores that are comparable to at least some popular GAN models under neutral testing conditions. The two-stage mechanism can improve the reconstruction of original distribution so that it has comparable performance with GAN models. This work narrows the gap between VAE and GAN models in terms of the realism of generated samples so that VAEs are worth considering in a broader range of applications. No need Gaussian assumption in the canonical VAE model to achieve the optimal solutions. References Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In 7th International Conferenceon Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. ↩︎ Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. ↩︎ Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and SeppHochreiter. GANs trained by a two time-scale update rule converge to a local Nashequilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637,2017. ↩︎ Miko laj Bi ́nkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. arXiv:1801.01401, 2018 ↩︎</summary></entry><entry><title type="html">Review on Importance Weighted Autoencoders</title><link href="https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/16/VAE1_blog.html" rel="alternate" type="text/html" title="Review on Importance Weighted Autoencoders" /><published>2021-09-16T00:00:00-05:00</published><updated>2021-09-16T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/variational%20autoencoder/2021/09/16/VAE1_blog</id><author><name></name></author><category term="Variational Autoencoder" /><summary type="html">Importance Weighted Autoencoders: what makes a good ELBO in VAE? Variational AutoEncoders (VAE) [1] is a powerful generative model which combines the variational inference and autoencoders together. It approximates the posterior distribution with a simple and tractable one, and optimize the lower bound of the true data distribution, which is called evidence lower bound (ELBO). Althoug optimizing ELBO is effective in practice, this estimation is actually biased, and it’s shown that this bias actually cannot be eliminated in vanilla VAE. Here we introduce a work that tries to minimize this bias called Importance Weighted Autoencoders (IWAE) [2], along with its variants which combines the objective in VAE and IWAE. Introduction to VAE and ELBO VAE consists of the encoder qϕq_{\phi}qϕ​ and the decoder pθp_{\theta}pθ​. It first encodes each sample xxx into a distribution of the latent variables qϕ(⋅∣x)q_{\phi}(\cdot|x)qϕ​(⋅∣x). Then the latent variables are sampled from the distibution as z∼qϕ(z∣x)z\sim q_{\phi}(z|x)z∼qϕ​(z∣x). The latent variables serve as the input to the decoder where the reconstructed output is x^∼pθ(x∣z)\hat{x}\sim p_{\theta}(x|z)x^∼pθ​(x∣z). The overview of VAE is shown in Fig 1. Fig 1. Overview of VAE (source from [1]) The training objective of VAE is to maximize ELBO. There are multiple ways to derivate ELBO, and one way is through the Bayesian theory. log⁡pθ(x)\log{p_{\theta}(x)}logpθ​(x) can be rewritten as log⁡pθ(x)=Eqϕ(z∣x)[log⁡pθ(x)]=Eqϕ(z∣x)[log⁡pθ(x,z)pθ(z∣x)]=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)qϕ(z∣x)pθ(z∣x)]=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)]+Eqϕ(z∣x)[qϕ(z∣x)pθ(z∣x)].\begin{align} \log{p_{\theta}(x)}&amp;amp;=\mathbb{E}_{q_{\phi}(z|x)}[\log{p_{\theta}(x)}]\\ &amp;amp;=\mathbb{E}_{q_{\phi}(z|x)}[\log{\frac{p_{\theta}(x,z)}{p_{\theta}(z|x)}}]\\ &amp;amp;=\mathbb{E}_{q_{\phi}(z|x)}[\log{\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}}\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}]\\ &amp;amp;=\mathbb{E}_{q_{\phi}(z|x)}[\log{\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}}]+\mathbb{E}_{q_{\phi}(z|x)}[\frac{q_{\phi}(z|x)}{p_{\theta}(z|x)}]. \end{align}logpθ​(x)​=Eqϕ​(z∣x)​[logpθ​(x)]=Eqϕ​(z∣x)​[logpθ​(z∣x)pθ​(x,z)​]=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​pθ​(z∣x)qϕ​(z∣x)​]=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​]+Eqϕ​(z∣x)​[pθ​(z∣x)qϕ​(z∣x)​].​​ Here the second term in Equation (4) is actually the the KL divergence DKL(qϕ(z∣x)∥pθ(z∣x))D_{KL}(q_{\phi}(z|x)\|p_{\theta}(z|x))DKL​(qϕ​(z∣x)∥pθ​(z∣x)) that is always non-negative. Therefore, the first term Lθ,ϕ(x)=Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)]\mathcal{L}_{\theta,\phi}(x)=\mathbb{E}_{q_{\phi}(z|x)}[\log{\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}}]Lθ,ϕ​(x)=Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​] actually serve as an lower-bound of log⁡pθ(x)\log{p_{\theta}(x)}logpθ​(x), which is exactly the ELBO. Furthermore, ELBO can be written in the regularized reconstruction form as Lθ,ϕ(x)=−DKL(qϕ(z∣x)∥pθ(z))+Eqϕ(z∣x)[−log⁡pθ(x∣z)],\begin{align} \mathcal{L}_{\theta,\phi}(x)=-D_{KL}(q_{\phi}(z|x)\|p_{\theta}(z)) + \mathbb{E}_{q_{\phi}(z|x)}[-\log{p_{\theta}(x|z)}], \end{align}Lθ,ϕ​(x)=−DKL​(qϕ​(z∣x)∥pθ​(z))+Eqϕ​(z∣x)​[−logpθ​(x∣z)],​​ where the first term regularizes the posterior distribution towards the prior which is usually set as a standard normal distribution, and the second term corresponds to the reconstruction. Nonetheloss, the regularization term actually has a conflict with the second term in Equation (4). When the regularization term is perfectly optimized, qϕ(z∣x)q_{\phi}(z|x)qϕ​(z∣x) will stay close to the prior p(z)p(z)p(z), meanwhile, it makes hard for qϕ(z∣x)q_{\phi}(z|x)qϕ​(z∣x) to be close enough to the true posterior distribution pθ(z∣x)p_{\theta}(z|x)pθ​(z∣x). Therefore, the gap between ELBO and the true data distribution, namely DKL(qϕ(z∣x)∥pθ(z∣x))D_{KL}(q_{\phi}(z|x)\|p_{\theta}(z|x))DKL​(qϕ​(z∣x)∥pθ​(z∣x)), will always exists, which prevents ELBO from being a tighter lower bound. Fig 2. Example of a heavy penalization in VAE We can understand this in the other view. When a latent variable is sampled from the low-probability region of a latent distribution, it would inevitably lead to a bad reconstruction. For the example in Fig 2, if we unfortunately sample a latent variable from the distribution of digit “5” (red) in the orange point, it turns out that this latent variable actually lies in the high probability region of the latent distribution generated by digit “3” (black) and it’s highly possible that we get a final reconstruction more similar to “3” rather than “5”. To make the posterior distribution close to the normal distribution, the regularizer will penalize this sample heavily by decreasing the variance, leading to a small spearout of the latent distribution. This drawback motivates the work of Importance Weight Autoencoders (IWAE) to introduce the importance weights into VAE, where a sampled latent variable which is far away from the mean will get assigned a lower weight during updates since it is known to give a bad reconstruction with high probability. Importance Weighted Autoencoders Another way to derivate ELBO is through the Jensen’s Inequality. Since log⁡(⋅)\log{(\cdot)}log(⋅) is a concave function, we have log⁡pθ(x)=log⁡Eqϕ(z∣x)[pθ(x,z)qϕ(z∣x)]≥Eqϕ(z∣x)[log⁡pθ(x,z)qϕ(z∣x)]=Lθ,ϕ(x).\begin{align}\log{p_{\theta}(x)}&amp;amp;=\log{\mathbb{E}_{q_{\phi}(z|x)}[\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}]}\\ &amp;amp;\geq\mathbb{E}_{q_{\phi}(z|x)}[\log{\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}}]\\ &amp;amp;=\mathcal{L}_{\theta,\phi}(x).\end{align}logpθ​(x)​=logEqϕ​(z∣x)​[qϕ​(z∣x)pθ​(x,z)​]≥Eqϕ​(z∣x)​[logqϕ​(z∣x)pθ​(x,z)​]=Lθ,ϕ​(x).​​ A simple example is shown in Fig 3. Consider a random variable XXX taking value from {x1,x2}\{x_1, x_2\}{x1​,x2​}, and we want to estimate log⁡E[X]\log{\mathbb{E}[X]}logE[X]. If we use E[log⁡X]\mathbb{E}[\log{X}]E[logX] to estimate it, then the estimation will converge at log⁡x1+log⁡x22\frac{\log{x_1}+\log{x_2}}{2}2logx1​+logx2​​, and the bias term cannot be eliminated by simply increasing the sampling times. Fig 3. Bias in log expectation estimation If we instead use E[1k∑i=1klog⁡Xi]\mathbb{E}[\frac{1}{k}\sum_{i=1}^k{\log{X_i}}]E[k1​∑i=1k​logXi​] for estimation, when we gradually increase the sampling times kkk, the bias will become smaller. And when k→+∞k\rightarrow+\inftyk→+∞, the term inside the expectation actually becomes a constant which is exactly log⁡E[X]\log{\mathbb{E}[X]}logE[X], as shown in Fig 4. Fig 4. Reducing the bias in the log expectation If we apply this property on ELBO estimation, let wi=pθ(x,zi)qϕ(zi∣x)w_i=\frac{p_{\theta}(x,z_i)}{q_{\phi}(z_i|x)}wi​=qϕ​(zi​∣x)pθ​(x,zi​)​ and Lk=Eqϕ(z∣x)[log⁡1k∑i=1kwi]\mathcal{L}_k=\mathbb{E}_{q_{\phi}(z|x)}[\log{\frac{1}{k}\sum_{i=1}^kw_i}]Lk​=Eqϕ​(z∣x)​[logk1​∑i=1k​wi​], we actually have the theorem log⁡pθ(x)≥Lk+1≥Lk.\begin{align} \log{p_{\theta}(x)}\geq\mathcal{L}_{k+1}\geq\mathcal{L}_k. \end{align}logpθ​(x)≥Lk+1​≥Lk​.​​ And Lk\mathcal{L}_kLk​ will converge to log⁡pθ(x)\log{p_{\theta}(x)}logpθ​(x) when pθ(x,z)qϕ(z∣x)\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}qϕ​(z∣x)pθ​(x,z)​ is bounded. Equipped with this theorem, IWAE simply replace sthe objective in VAE with Lk\mathcal{L}_kLk​, where k&amp;gt;1k&amp;gt;1k&amp;gt;1. This gives a tighter lower bound compared with ELBO. And when k=1k=1k=1, IWAE is reduced to VAE. In the backward pass, the gradient of Lk\mathcal{L}_kLk​ can be written as ∇θ,ϕLk=Eqϕ(z∣x)[∇θ,ϕlog⁡1k∑i=1kwi]=Eqϕ(z∣x)[∑i=1kwi~log⁡∇θ,ϕwi],\begin{align}\nabla_{\theta,\phi}\mathcal{L}_k&amp;amp;=\mathbb{E}_{q_{\phi}(z|x)}[\nabla_{\theta,\phi}\log{\frac{1}{k}\sum_{i=1}^kw_i}]\\ &amp;amp;=\mathbb{E}_{q_{\phi}(z|x)}[\sum_{i=1}^k\tilde{w_i}\log{\nabla_{\theta,\phi}w_i}], \end{align}∇θ,ϕ​Lk​​=Eqϕ​(z∣x)​[∇θ,ϕ​logk1​i=1∑k​wi​]=Eqϕ​(z∣x)​[i=1∑k​wi​~​log∇θ,ϕ​wi​],​​ where wj~=wj∑i=1kwi\tilde{w_j}=\frac{w_j}{\sum_{i=1}^kw_i}wj​~​=∑i=1k​wi​wj​​ is the normalized importance weights, which makes the model name “Importance Weighted” Autoencoders. In VAE, wj~\tilde{w_j}wj​~​ takes the value 111. The meaning of the importance weights could be interpreted as this: if a latent sample itself has low probability in the latent distribution, then it should get assigned a lower weight in the gradient update since it’s known to cause a bad reconstruction with high probability. Introducing importance weights can effectively lower the risk shown in Fig 2. Variants of IWAE To make a straightforward comparison with ELBO in VAE, we fix the sampling times for both VAE and IWAE as kkk. The ELBO for IWAE and VAE become ELBOIWAE=log⁡1k∑i=1kwiELBOVAE=1k∑i=1klog⁡wi. \begin{align} \text{ELBO}_{\text{IWAE}}&amp;amp;=\log{\frac{1}{k}\sum_{i=1}^kw_i}\\ \text{ELBO}_{\text{VAE}}&amp;amp;=\frac{1}{k}\sum_{i=1}^k\log{w_i}. \end{align} ELBOIWAE​ELBOVAE​​=logk1​i=1∑k​wi​=k1​i=1∑k​logwi​.​​ The main difference here is the position of the average operation, either insider or outside log⁡(⋅)\log{(\cdot)}log(⋅). IWAE regards the sampling outside log⁡(⋅)\log{(\cdot)}log(⋅) as the variance reduction, and it’s shown that IWAE actually doesn’t suffer from the large variance, so IWAE puts all sampling inside log⁡(⋅)\log{(\cdot)}log(⋅) to reduce the bias as much as possible. However, a follow-up work [3] of IWAE theoretically proves that the sampling outside log⁡(⋅)\log{(\cdot)}log(⋅) is crucial to the training of the encoder. A tighter bound used by IWAE helps the generative network (decoder) but hurts the inference network (encoder). Based on this discovery, three new models combining ELBOIWAE\text{ELBO}_{\text{IWAE}}ELBOIWAE​ and ELBOVAE\text{ELBO}_{\text{VAE}}ELBOVAE​ are proposed. For the following text, we fix the total sampling times to be MKMKMK, where MMM is the sampling times outside log⁡(⋅)\log{(\cdot)}log(⋅) and KKK is the sampling times inside log⁡(⋅)\log{(\cdot)}log(⋅). MIWAE. MIWAE simply uses an ELBO objective with both M&amp;gt;1M&amp;gt;1M&amp;gt;1 and K&amp;gt;1K&amp;gt;1K&amp;gt;1, i.e., ELBOMIWAE=1M∑m=1Mlog⁡1K∑k=1Kwm,k. \begin{align} \text{ELBO}_{\text{MIWAE}}=\frac{1}{M}\sum_{m=1}^M\log{\frac{1}{K}\sum_{k=1}^Kw_{m,k}}. \end{align} ELBOMIWAE​=M1​m=1∑M​logK1​k=1∑K​wm,k​.​​ CIWAE. CIWAE uses a convex combination of two ELBOs, i.e., ELBOCIWAE=βELBOVAE+(1−β)ELBOIWAE. \begin{align} \text{ELBO}_{\text{CIWAE}}=\beta\text{ELBO}_{\text{VAE}}+(1-\beta)\text{ELBO}_{\text{IWAE}}. \end{align} ELBOCIWAE​=βELBOVAE​+(1−β)ELBOIWAE​.​​ PIWAE. PIWAE uses different objectives for the inference network and generative network. For the generative network, it keeps the objective of IWAE, ELBOIWAE\text{ELBO}_{\text{IWAE}}ELBOIWAE​. While for the inference network, it switches to the objective ELBOMIWAE\text{ELBO}_{\text{MIWAE}}ELBOMIWAE​. Experimental Results The experimental results demonstrate the advantage of IWAE against VAE as Table 1 shows. IWAE achieves lower negative log-likelihood (NLL) and more active units (active units captures data infomation) on all datasets and model architectures. And as kkk increases, the performance is better since the lower bound is tighter. Table 1. IWAE results For the qualitative analysis in Fig 5, it’s worth noting that for IWAE, it has a larger spredout of the latent distribution and sometimes different output digits, e.g., “6” for “0”. This demonstrates the relaxation of the heavy panelization on the outliers, contrary to the example in Fig 2. Fig 5. Ouput samples from VAE and IWAE In a grid search of different combinations of (M,K)(M,K)(M,K) with MKMKMK fixed as 646464, we can see neither M=1M=1M=1 or K=1K=1K=1 makes the optimal solution in Fig 6. In other words, the ELBO objective should consider both the sampling inside and outside log⁡(⋅)\log{(\cdot)}log(⋅), which are beneficial to the generative network and inference network respectively. Fig 6. Ouput samples from VAE and IWAE Conclusion IWAE uses a simple technique, moving the average operation inside log⁡(⋅)\log{(\cdot)}log(⋅), to achieve a tighter lower bound. The importance weights relaxes the heavy penalization on the posterior samples which fail to explain the observation. Although IWAE effectively reduces the bias, it’s shown that the sampling inside log⁡(⋅)\log{(\cdot)}log(⋅) is only beneficial to the generative network, but hurts the inference network. Therefore, to combine the ELBO in VAE and IWAE, the IWAE variants, MIWAE, CIWAE and PIWAE are proposed. The final results demonstrate that the optimal objective needs the sampling inside and outside log⁡(⋅)\log{(\cdot)}log(⋅) to be both greater than one. These works takes a deep look into the ELBO objective in VAE and reveal its role in the learning process. References [1] 2014 (ICLR): D. Kingma, M. Welling, Auto-Encoding Variational Bayes, ICLR, 2014. [2] 2016 (ICLR): Y. Burda, R. Grosse, R. Salakhutdinov. Importance Weighted Autoencoders. ICLR, 2016. [3] 2018 (ICML): T. Rainforth, A. Kosiorek, T. Le, C. Maddison, M. Igl, F. Wood, Y. Teh, Tighter Variational Bounds are Not Necessarily Better. ICML, 2018.</summary></entry><entry><title type="html">Review on Attention Is All You Need</title><link href="https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog3.html" rel="alternate" type="text/html" title="Review on Attention Is All You Need" /><published>2021-09-14T00:00:00-05:00</published><updated>2021-09-14T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog3</id><author><name></name></author><category term="Transformers" /><summary type="html">Attention Is All You Need Introduction In the last three years, the Transformer architecture has become an influential paradigm within deep learning. It has been applied prolifically within natural language processing (NLP), is beginning to see promising applications in computer vision (CV), and is also used within many other modalities and fields of deep learning. The paper which introduced the Transformer is “Attention is All You Need” [1] by Vaswani et al. Attention is All You Need (from here, AAYN) uses the Transformer architecture to perform machine translation. Historically, the work in AAYN was done when recurrent neural networks (RNN) were the dominant force in NLP. Common modifications of these included the Long short-term memory (LSTM) [5] and gated recurrent unit (GRU) [6]. However, these models have a big problem—they compute along the length of a sequence, so they cannot be parallelized easily. Additionally, RNNs struggle to learn long-term dependencies. In order to rectify this issue, the authors propose the key idea (and title): attention is all you need. Although there had been previous work on using attention, most of those papers combined it with RNNs, so it still had the drawbacks from that method. Task – Machine Translation In AAYN, the primary goal of the model is translation, making this a sequence-to-sequence generation problem. In particular, they focus on English to German and English to French tasks from WMT2014. Machine translation is trained on bitext – data where each sample consists of the same sentence in the source and the target language. Then, the model is evaluated on a test set where it has to translate sentences. The results are compared to several human reference translations which are used to compute the BLEU score. It is defined as follows [2,7] Definition of BLEU from [2] Here, the key things to note are the brevity and n-gram overlap. Note that if the model outputs something very short, then it has a high probability of completely overlapping with n-grams in a reference translation. To penalize this, the brevity penalty is added, so when the output translation is shorter than the reference translation, then the exponent will be to the power of a negative number making a smaller brevity term. If the reverse is true then the brevity will be 1 due to the minimum and be ignored. The other important term in BLEU is the n-gram overlap. This essentially measures how well an output matches the references. The different lengths of n-grams measure different things; unigrams measure adequacy and the longer n-grams measure fluency. Note that this definition allows the candidate output to combine parts from different reference translations and have a good score. BLEU score interpretation from [2] Preliminaries The Transformer model uses an encoder-decoder sequence-to-sequence architecture. It can be described as mathematically as follows: Input: Length nnn sequence of symbolic representations: (x1,...,xn)(x_1,...,x_n)(x1​,...,xn​) Encoder: produces latent representations: z=(z1,...,zn)z=(z_1,...,z_n)z=(z1​,...,zn​) Decoder: uses zzz to produce length mmm output sequence: (y1,...,ym)(y_1,...,y_m)(y1​,...,ym​) Model The Transformer is the following model: In the model, the encoder is on the left, and it feeds into the decoder on the right. In AAYN, the encoder and decoder are each stacked six times. We will examine the construction of both the encoder and decoder, so first let’s look at how the pieces of each layer are built. Scaled Dot-Product Attention Attention is at the key of the Transformer architecture. The intuition behind this approach is that it allows the model to decide what other symbols in the sequence are most important to look at for solving whatever problem. In AAYN, the attention mechanism is implemented using multiplicative attention (dot product). In addition, the major modification from the paper is to scale the dot products—this is done by dividing by the square root of the number of dimensions. This is due to the author’s observation that the the dot product grows too large in magnitude for a high number of dimensions, which would limit the model’s efficacy. Dot-product attention works by learning a query, key, and value projection from some input. The query and key values are used to compute the attention—how much weight the model gives each token in a sequence. The multiplication between QQQ and KTK^TKT produces a sequence length by sequence length array of logits. Then Softmax is applied, which essentially turns the logits into probability distributions (one for each symbol in the sequence). These probabilities are used to compute a weighted average of the values VVV. VVV contains a representation for each symbol in the sequence, so the attention distribution decides how much one symbol iii should pay to any other symbol jjj. A visual of this will be shown later in the results section. Multi-Head Attention The authors notice that the weighted average in attention prevents the model from looking at different representation subspaces—it can’t consider multiple different parts of the sequence without averaging them. To fix this, the authors propose multi-head attention. Multi-head attention essentially allows the model to look at multiple things at the same time. Each attention head can learn to look for different things, such as connecting adjectives to nouns or connecting verbs and objects. Naively using multi-head attention, however, would increase the computational costs of the model. To address this issue, the authors decrease the representation dimension of each head by hhh, where hhh is the number of heads. This results in the same total number of parameters in the attention mechanism. The following dimensions are used for each head: The output representation from each head is concatenated together to create a vector of the original length, dmodeld_{model}dmodel​. This is projected again. Positional Embeddings One key issue of only using attention is that, according to attention, one symbol in a sequence is the same distance away from any other symbol. To allow the model to determine distance, the authors introduce positional embeddings. Visualization of sinusoidal positional embeddings from [3]. Each column is a positional embedding. Equations for the sinusoidal position embeddings The authors selected a sinusoidal embedding function because the offset between embeddings can be represented as a linear function. However, many positional embeddings are possible and the authors also experiment with learned positional embeddings (achieving similar results). The sinusoidal embeddings are used in the paper because the authors hypothesize that they will allow the model to extrapolate to sequence lengths longer than those the model was trained on. Note that later work shows that position is not necessarily as important as intuition suggests [8]. Types of Attention In AAYN, three types of attention are used: Encoder-decoder attention: Queries QQQ come from the last layer of the decoder, keys KKK and values VVV come from encoder. This allows the decoder to look at the input source language in order to translate it. Encoder self-attention layer: Each position can attend to every other position in the previous layer of the encoder. Decoder self-attention layer: Same as encoder self-attention but also mask out all connections in the Softmax that cannot have been seen. This maintains the autoregressive property of the model by preventing the model from looking at words it hasn’t seen yet. Why Self-Attention? Self-attention allows the model to learn dependencies between different symbols in the sequence. This is shown in the following table: Note that self-attention achieves the best complexity in terms of both maximum path length and sequential operations (which indicates parallelizability). Additionally, the complexity per layer is low if nnn is significantly smaller than ddd, which often occurs in practice (although many researchers are working on models where this assumption no longer holds). Putting It All Together Alright, we’ve looked at all the pieces. Now, let’s put everything together! The Encoder The encoder combines all the parts we talked about. Then, it is stacked NNN times (6 in the base Transformer model). A couple notable things that we didn’t mention yet: The model uses either byte-pair or word-piece tokenization to create token sequences from raw character strings. The model uses residual connections by adding an earlier representation. It follows this up with layer normalization. The model uses a simple feed-forward network with two layers after attention. The Decoder The decoder is mostly the same as the encoder. However, it uses masking to ignore symbols that the model shouldn’t have seen yet from the input. For example, if I say “The cat sat” then the model would need to generate the next word. This is called autoregression. If this model generates “on”, then “The cat sat on” would be fed in the model to generate the next word (maybe “the”). When we’re training the model, we have the full sentence that the model is learning to generate, so we can’t let it know what’s coming. In addition, the decoder also has an attention mechanism that looks at what it’s trying to translate. This allows it to see what it should be doing, and the decoder can use attention to connect the source input to its translated output. For example, “gato”, or cat in Spanish, might be connected to “cat” in the example above using attention. Training The training details for the model are as follows: Sentences are encoded (convert a string of characters to a sequence of symbols): English-German uses BytePair encoding for 37,000 tokens on 4.5M sentence pairs. English-French uses WordPiece encoding for 32,000 tokens on 36M sentence pairs. Batch size is determined in order to have 25,000 source and target tokens (symbols). 8 NVIDIA T100 GPUs are used to train the model. Base models trained for 12 hours, big models for 3.5 days. Adam optimizer is used with a special learning rate: Linear warmup followed by inverse square root decay. Regularization: Dropout of 0.1 applied to residual connections and sum of positional encoding and embeddings. Label smoothing is performed. The last 5 checkpoints are averaged (for the base model). Beam search is used to select the best translation. The most interesting details are that the batch size is dynamic so that the source and target tokens number approximately 25,000. The summation of the last checkpoints and using a learning rate decay are also interesting. Results The details are finally out of the way! Let’s look at some pretty pictures and tables. As can be seen in the above figure, Transformer is able to set record BLEU scores in much less computation. Additionally, the “big” variant can even beat some expensive ensemble models! We can see the effect of different hyperparameters (such as those used in base versus big) on the performance in the following table: Alright, now time for some visualizations from the paper: As shown in the visualizations, the attention heads learn different, meaningful tasks, such as anaphora resolution or connecting determiners and their objects. In fact, the authors show that Transformer can be used directly for this type of task—English constituency parsing (extracting the syntactic structure of a sentence in the form of a tree). Results are competitive to previous methods, even without task-specific fine-tuning. Final Takeaway Transformers precipitated a major change in the landscape of natural language processing and even other fields like computer vision. They lead to even more powerful general language models, such as BERT [4]. The paper can be summarized by the following points. Motivation: RNNs are not easily parallelizable and don’t learn long dependencies well. Models that only use attention are more effective and train faster. Transformer can generalize to other tasks. Multi-head attention helps address some of the problems of traditional attention. It allows multiple different attention tasks to be learned. Transformers have a constant dependency path from one position to any other position. References [1] Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems. 2017. [2] BLEU score definition: https://cloud.google.com/translate/automl/docs/evaluate [3] https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ [4] Devlin, J., Chang, M. W., Lee, K., &amp;amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. [5] Hochreiter, S., &amp;amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780. [6] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp;amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. [7] Papineni, K., Roukos, S., Ward, T., &amp;amp; Zhu, W. J. (2002, July). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (pp. 311-318). [8] Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., &amp;amp; Kiela, D. (2021). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. arXiv preprint arXiv:2104.06644.</summary></entry><entry><title type="html">Review on Generating Long Sequences with Sparse Transformers</title><link href="https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog2.html" rel="alternate" type="text/html" title="Review on Generating Long Sequences with Sparse Transformers" /><published>2021-09-14T00:00:00-05:00</published><updated>2021-09-14T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/transformers/2021/09/14/AR3_blog2</id><author><name></name></author><category term="Transformers" /><summary type="html">Generating Long Sequences with Sparse Transformers Transformers and attention-based methods have skyrocketed in popularity in recent years. These models excel at modelling long-term dependencies and are highly parallelizable, overcoming the shortcomings of prior LSTM based models. However, vanilla transformers1 scale poorly with increasing sequence length; since the attention is done globally between all inputs, the computation grows quadratically with input length. In this post, I will go over the Sparse Transformer2 model which reduces the computation to O(nn)O(n\sqrt n)O(nn​) where nnn is the sequence length. Additionally, unlike prior works that propose model for specific generation tasks, the sparse transformer model can be used to generate text, images, and audio! Background The sparse transformer is an autoregressive model. It models the joint probability distribution as a product of conditional probability distributions. The iiith output depends all on previous inputs x1,...,xi−1x_1, ..., x_{i-1}x1​,...,xi−1​. This autoregressive property is embedded into the attention operation, which cannot use future values to generate an output. Image taken from 3 . Factorized Self-Attention Intuition To understand the motivation behind the sparse transformer model, we take a look at the learned attention patterns for a 128-layer dense transformer network on the CIFAR-10 dataset. The authors observed that the attention pattern of the early layers resembled convolution operations. For layers 19-20, the attention pattern is arranged in discrete rows and columns. In other layers, the attention pattern is extremely complex, global, and data dependent. Finally, in the the layers 64-128, the attention pattern is extremely sparse. Visualization taken from 2 . Looking at these attention patterns, we observe that most attention patterns are sparse. The authors reasoned that to model high-dimensional data, dense global attention is not required. Instead, sparser attention operations can capture most of the information needed to model the underlying distribution. Sparse Transformer Model Factorized Self-Attention The factorized self-attention operation forms the backbone of the sparse transformer model. The authors break down the dense self-attention operation with several sparse attention operations. In particular, an attention operation can be written as (equations taken from 2): where SiS_iSi​ denotes the set of indices of input vectors which the iiith output vector attends. For dense self-attention, Si={j:j≤i}S_i=\{j:j \le i\}Si​={j:j≤i} which allows every element to attend to all previous positions and its own position. This pattern can be visualized in the image given below. Visualization of attention taken from 2 . The bottom image is the connectivity matrix where the i=ji=ji=j index represents the output and the other indices in the same row represent the input that the output attends to. Instead, factorized self attention uses ppp separate attention heads each defining a subset of indices Ai(m)⊂{j:j≤i}A_i^{(m)} \subset \{j:j \le i\}Ai(m)​⊂{j:j≤i}. We want choice of AAA such that ∣Ai(m)∣∝ap|A_i^{(m)}| \propto \sqrt[p] a∣Ai(m)​∣∝pa​ so that our computation scales the way we want. This paper considers choices with two heads p=2p=2p=2. Additionally, we add the constraint that there is path from each input connection to all future output positions across ppp steps of attention (this will become more clear in the representations for the factorized attention patterns) so that all input signals are being propagated to output positions in a constant number of steps. Strided Attention Pattern In this factorized attention pattern, one head attends to lll previous locations and the other head attends to every lllth previous location. lll is the stride parameter and is chosen to be close to n\sqrt nn​. More formally, the index sets are defined as Ai(1)={t:t,t+1,...,i}A_i^{(1)} = \{t:t,t+1,...,i\}Ai(1)​={t:t,t+1,...,i} for t=max(0,i−l)t=max(0,i-l)t=max(0,i−l) and Ai(2)={j:(i−j) mod l=0}A_i^{(2)} = \{j:(i-j) \, mod \, l = 0\}Ai(2)​={j:(i−j)modl=0}. This strided attention pattern is visualized below. Visualization of attention taken from 2 . This pattern works well when the data naturally has a structure that aligns with the stide. For example, images and audio have periodic structure that can be modeled effectively using strided attention patterns. However, for data without this natural structure such as text, the strided pattern does not perform well. Fixed Attention Pattern In the fixed attention pattern, we model the AAA matrices as follows: Ai(1)={j:(⌊j/l⌋=⌊i/l⌋)}A_i^{(1)} = \{j: (\lfloor j/l \rfloor = \lfloor i/l \rfloor) \}Ai(1)​={j:(⌊j/l⌋=⌊i/l⌋)} and Ai(2)={j:j mod l∈{t:t,t+1,...,l}}A_i^{(2)} = \{j:j \, mod \, l \in \{t:t,t+1,...,l\}\}Ai(2)​={j:jmodl∈{t:t,t+1,...,l}}. This pattern is easier to visualize than understanding the math. Visualization of attention taken from 2 . This attention pattern works better for data without perdiodic structure like text. Note: For both strided and fixed attention pattern, notice how every input signal is propagated to arbitrary output after 2 steps of attention, satisfying the constraints. Incorporating Factorized Self-Attention The authors proposed several ideas on how to incorporate these factorized attention models in the transformer network. These methods can be summarized in the image below. Gradient Recomputation during Backward Pass During the gradient backpropagation step while training, the results from the forward pass are stored in memory and used for computation. However, for sparse attention, the memory usage to store the result is far greater than the computational cost of the forward pass. Hence, we don’t save all the forward pass results in memory but recompute the forward pass during training the compute the gradients. This reduces the memory usage of the model and enables networks with hundreds of layers and sequences of up to 16384 in length. New residual block architecture Transformers are notoriously difficult to scale to many layers. The authors of this paper experiment with using a different kind of residual connection which enables the sparse transformer model to scale to hundred of layers. The NNN layer transformer network is defined as follows. This residual architecture with the gradient recomputation is visualized in the image below. Visualization of model architecture taken from 2 . Results Image generation examples taken from 2 . Sparse transformer NLL metrics on common datasets taken from paper 2 . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017; http://arxiv.org/abs/1706.03762 arXiv:1706.03762. ↩︎ Child, Rewon, et al. “Generating Long Sequences with Sparse Transformers.” ArXiv:1904.10509 [Cs, Stat], Apr. 2019. arXiv.org, http://arxiv.org/abs/1904.10509 ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ ↩︎ Oord, Aaron van den, et al. “Pixel Recurrent Neural Networks.” ArXiv:1601.06759 [Cs], Aug. 2016. arXiv.org, http://arxiv.org/abs/1601.06759 ↩︎</summary></entry></feed>