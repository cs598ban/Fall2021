<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://cs598ban.github.io/Fall2021/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cs598ban.github.io/Fall2021/" rel="alternate" type="text/html" /><updated>2021-12-13T18:39:34-06:00</updated><id>https://cs598ban.github.io/Fall2021/feed.xml</id><title type="html">CS 598 Deep Generative and Dynamical Models</title><subtitle>Blogs created as a part of graduate course at UIUC on deep generative and dynamical models.</subtitle><entry><title type="html">DS1 Physics Informed Neural Networks</title><link href="https://cs598ban.github.io/Fall2021/ds/physics+ml/2021/11/18/DS1_blog2.html" rel="alternate" type="text/html" title="DS1 Physics Informed Neural Networks" /><published>2021-11-18T00:00:00-06:00</published><updated>2021-11-18T00:00:00-06:00</updated><id>https://cs598ban.github.io/Fall2021/ds/physics+ml/2021/11/18/DS1_blog2</id><author><name></name></author><category term="DS" /><category term="Physics+ML" /><summary type="html">Introduction This blog covers Physics Informed Neural Networks, which are basically neural networks that are constrained to obey physical constraints. The blog broadly discusses two directions: Data-driven solutions of PDEs Data-driven discovery of PDEs In particular, it deals with two distinct types of algorithms, namely A new family of data-efficient spatio-temporal function approximators Arbitrary accurate RK time steppers with potentially unlimited number of stages We know that deep neural networks are powerful function approximators. They can approximate a nonlinear map from a few – potentially very high dimensional – input and output pairs of data. However, the approximation does not take into account any underlying physical constraints imposed by fundamental principles (such as conservation laws) which are often given in terms of PDEs. PINNs precisely solve this problem by regularizing the loss functions with these constraints. The resulting approximation enjoys both the power of deep neural nets as function approximators and obeying the underlying physical conservation laws. PDEs Let us consider a parametric nonlinear PDE ut+N[u;λ]=0,x∈Ω⊂RD, t∈[0,T];(⋅)t:=∂(⋅)∂t \color{BrickRed} u_{t}+\mathcal{N}[u ; \lambda]=0, x \in \Omega \subset \mathbb{R}^D,\, t \in[0, T];\quad ({\cdot})_t := \frac{\partial ({\cdot})}{\partial t} ut​+N[u;λ]=0,x∈Ω⊂RD,t∈[0,T];(⋅)t​:=∂t∂(⋅)​ where u(t,x)u(t, x)u(t,x) denotes the latent (hidden) solution and N[⋅;λ]\mathcal{N}[\cdot ; \lambda]N[⋅;λ] is a nonlinear operator parametrized by λ\lambdaλ. This encapsulation covers a wide range of PDEs in math, physics, atmospheric sciences, biology, finance, including conservation laws, diffusion, reac-diff-advec. PDEs, kinetics etc. The burger’s equation in 222d is an apt starting step to investigate if indeed PINNs can efficiently solve PDEs, and therefore the authors begin by applying PINNs to it. N[u;λ]=λ1uux−λ2uxx&amp;nbsp;and&amp;nbsp;λ=(λ1,λ2);(⋅)x:=∂(⋅)∂x(⋅)xx:=∂2(⋅)∂x2 \mathcal{N}[u ; \lambda]=\lambda_{1} u u_{x}-\lambda_{2} u_{x x} \text { and } \lambda=\left(\lambda_{1}, \lambda_{2}\right);\quad ({\cdot})_x := \frac{\partial ({\cdot})}{\partial x} \quad ({\cdot})_{xx} := \frac{\partial^2 ({\cdot})}{\partial x^2} N[u;λ]=λ1​uux​−λ2​uxx​&amp;nbsp;and&amp;nbsp;λ=(λ1​,λ2​);(⋅)x​:=∂x∂(⋅)​(⋅)xx​:=∂x2∂2(⋅)​ The two directions highlighted above can now be restated in context of Burger’s equation as follows: Given λ\lambdaλ what is u(t,x)u(t, x)u(t,x) (data-driven solutions of PDEs) Find λ\lambdaλ that best describes observations u(ti,xj)u({t_i, x_j})u(ti​,xj​) (data-driven discovery of PDEs) PDEs + PINNs First, let us rewrite the PDE as f(u;t,x)=0f(u; t,x) = 0f(u;t,x)=0 f(u;t,x)≐ut+N[u],along&amp;nbsp;withu=uθ(t,x) f(u; t,x) \doteq u_{t}+\mathcal{N}[u], \quad \text{along with}\quad u = u_\theta ({t, x}) f(u;t,x)≐ut​+N[u],along&amp;nbsp;withu=uθ​(t,x) The next step consists of appending the loss function used for the training process (finding the parameters θ\thetaθ) as follows: L=Lu+Lf \mathcal{L}=\mathcal{L}_{u}+\mathcal{L}_{f} L=Lu​+Lf​ where Lu=1Nu∑i=1Nu∣u(tui,xui)−ui∣2 ;Lf=1Nf∑i=1Nf∣f(tfi,xfi)∣2. \mathcal{L}_{u}=\frac{1}{N_{u}} \sum_{i=1}^{N_{u}}\left|u\left(t_{u}^{i}, x_{u}^{i}\right)-u^{i}\right|^{2}\, ;\quad \mathcal{L}_{f}=\frac{1}{N_{f}} \sum_{i=1}^{N_{f}}\left|f\left(t_{f}^{i}, x_{f}^{i}\right)\right|^{2}. Lu​=Nu​1​i=1∑Nu​​∣∣​u(tui​,xui​)−ui∣∣​2;Lf​=Nf​1​i=1∑Nf​​∣∣​f(tfi​,xfi​)∣∣​2. Here Lu\mathcal{L}_{u}Lu​ consists of the approximation error at the boundary of the domain (initial and boundary conditions), and Lf\mathcal{L}_{f}Lf​ denotes the error incurred inside the domain. Also, {tfi,xfi}i=1Nf\left\{t_{f}^{i}, x_{f}^{i}\right\}_{i=1}^{N_{f}}{tfi​,xfi​}i=1Nf​​ specify the collocation points — the discrete points at which the physical constraints are imposed through a regularized (penalty) formulation. Another way to look at this is that Lu\mathcal{L}_uLu​ helps to enforce initial and boundary data accurately, while Lf\mathcal{L}_fLf​ imposes the structure of the PDE into the training process. An important thing to consider is the relative weighting of the two losses, namely, L=λ Lu+(2−λ) Lf \mathcal{L}=\lambda\,\mathcal{L}_{u}+(2-\lambda)\,\mathcal{L}_{f} L=λLu​+(2−λ)Lf​ where λ=1\lambda = 1λ=1 corresponds to the original loss above. Increasing λ\lambdaλ towards 222 leads to a an approximation that is accurate inside the domain but performs poorly on the boundaries. Similarly, λ→0+\lambda\rightarrow 0^+λ→0+ leads to an approximation that is accurate on the boundary but poor inside the domain. Implementation details and Code Since most of the examples considered here involve a small number of training data points, the authors choose to exploit quasi-second order optimization methods such as L-BFGS. They do not consider mini-batching for the same reason and consider the full batch on the update. There are no theoretical guarantees on the existence of the minimizer but the authors observe empirically that as long as the PDE is well-posed the optimization algorithm converges to the correct solution. The original implementation can be accessed at https://github.com/maziarraissi/PINNs which builds on top of Tensorflow. Corresponding PyTorch and Julia (Flux) implementations can be accessed at https://github.com/idrl-lab/idrlnet and https://neuralpde.sciml.ai/dev/ Examples: Schrodinger Equation As a first example, consider a complex-valued differential equation (h(t,x)=u(t,x)+i v(t,x)h(t, x) = u(t, x) + \mathrm{i}\, v(t,x)h(t,x)=u(t,x)+iv(t,x)), namely the Schrodinger equation, f≐ iht+0.5hxx+∣h∣2h=0,x∈[−5,5],t∈[0,π/2]h(0,x)=2sech⁡(x)h(t,−5)=h(t,5)hx(t,−5)=hx(t,5) \begin{aligned} f\doteq \, &amp;amp;i h_{t}+0.5 h_{x x}+|h|^{2} h=0, \quad x \in[-5,5], \quad t \in[0, \pi / 2] \\ &amp;amp;h(0, x)=2 \operatorname{sech}(x) \\ &amp;amp;h(t,-5)=h(t, 5) \\ &amp;amp;h_{x}(t,-5)=h_{x}(t, 5) \end{aligned} f≐​iht​+0.5hxx​+∣h∣2h=0,x∈[−5,5],t∈[0,π/2]h(0,x)=2sech(x)h(t,−5)=h(t,5)hx​(t,−5)=hx​(t,5)​ In order to generate the training data, the authors employ a classical pseudo-spectral solver in space (xxx) coupled with a high-order RK time stepper. The results are shown in the figure below (Fig. 1 in the paper). PINN provides an accurate solution to Schrodinger equation and can handle periodic boundary conditions, nonlinearities and complex valued nature of the PDE efficiently. Although PINN provides a good approximation to the solution above, training the network requires a large number of data points. This is where the adaptive time-stepping using RK methods comes into the picture. The authors propose an adaptive time-stepping with a neural net at each time-step. This significantly improves the approximation quality, and allows one to take much larger time-steps compared to traditional solvers. Examples: Allen-Cahn Equation In order to test the adaptive time-stepping scheme, the authors next take a look at the Allen-Cahn equation. ut−0.0001uxx+5u3−5u=0,x∈[−1,1],t∈[0,1],u(0,x)=x2cos⁡(πx),u(t,−1)=u(t,1),ux(t,−1)=ux(t,1). \begin{aligned} &amp;amp;u_{t}-0.0001 u_{x x}+5 u^{3}-5 u=0, \quad x \in[-1,1], \quad t \in[0,1], \\ &amp;amp;u(0, x)=x^{2} \cos (\pi x), \\ &amp;amp;u(t,-1)=u(t, 1), \\ &amp;amp;u_{x}(t,-1)=u_{x}(t, 1) . \end{aligned} ​ut​−0.0001uxx​+5u3−5u=0,x∈[−1,1],t∈[0,1],u(0,x)=x2cos(πx),u(t,−1)=u(t,1),ux​(t,−1)=ux​(t,1).​ The results show excellent agreement between the predicted and exact (numerical) solutions. The only difference here is that the authors consider the sum-of-squared errors as the loss function for training, instead of the MSE used before, i.e. SSEn=∑j=1q+1∑i=1Nn∣ujn(xn,i)−un,i∣2 S S E_{n}=\sum_{j=1}^{q+1} \sum_{i=1}^{N_{n}}\left|u_{j}^{n}\left(x^{n, i}\right)-u^{n, i}\right|^{2} SSEn​=j=1∑q+1​i=1∑Nn​​∣∣​ujn​(xn,i)−un,i∣∣​2 SSEb=∑i=1q∣un+ci(−1)−un+ci(1)∣2+∣un+1(−1)−un+1(1)∣2+∑i=1q∣uxn+ci(−1)−uxn+ci(1)∣2+∣uxn+1(−1)−uxn+1(1)∣2 \begin{aligned} S S E_{b} &amp;amp;= \sum_{i=1}^{q}\left|u^{n+c_{i}}(-1)-u^{n+c_{i}}(1)\right|^{2}+\left|u^{n+1}(-1)-u^{n+1}(1)\right|^{2} \\ &amp;amp;+ \sum_{i=1}^{q}\left|u_{x}^{n+c_{i}}(-1)-u_{x}^{n+c_{i}}(1)\right|^{2}+\left|u_{x}^{n+1}(-1)-u_{x}^{n+1}(1)\right|^{2} \end{aligned} SSEb​​=i=1∑q​∣∣​un+ci​(−1)−un+ci​(1)∣∣​2+∣∣​un+1(−1)−un+1(1)∣∣​2+i=1∑q​∣∣​uxn+ci​​(−1)−uxn+ci​​(1)∣∣​2+∣∣​uxn+1​(−1)−uxn+1​(1)∣∣​2​ Examples: Navier-Stokes Equation In this section, we take a look at how data-driven discovery of PDEs can be carried out using PINNs. Consider the NS equations in 2-dimensions, ut+λ1(uux+vuy)=−px+λ2(uxx+uyy)vt+λ1(uvx+vvy)=−py+λ2(vxx+vyy);where(⋅)x=∂(⋅)∂x. \begin{aligned} &amp;amp;u_{t}+\lambda_{1}\left(u u_{x}+v u_{y}\right)=-p_{x}+\lambda_{2}\left(u_{x x}+u_{y y}\right) \\ &amp;amp;v_{t}+\lambda_{1}\left(u v_{x}+v v_{y}\right)=-p_{y}+\lambda_{2}\left(v_{x x}+v_{y y}\right) \end{aligned}; \quad \text{where}\quad ({\cdot})_x = \frac{\partial({\cdot})}{\partial x}. ​ut​+λ1​(uux​+vuy​)=−px​+λ2​(uxx​+uyy​)vt​+λ1​(uvx​+vvy​)=−py​+λ2​(vxx​+vyy​)​;where(⋅)x​=∂x∂(⋅)​. Here u(t,x,y)u(t, x, y)u(t,x,y) denotes the xxx-component of the velocity, v(t,x,y)v(t, x, y)v(t,x,y) denotes the yyy component and p(t,x,y)p(t, x, y)p(t,x,y) the pressure field. Conservation of mass requires ux+vy=0  ⟹  u_{x}+v_{y}=0 \impliesux​+vy​=0⟹ u=ψy,v=−ψxu=\psi_{y}, \quad v=-\psi_{x}u=ψy​,v=−ψx​ Given a set of observations: {ti,xi,yi,ui,vi}i=1N\left\{t^{i}, x^{i}, y^{i}, u^{i}, v^{i}\right\}_{i=1}^{N}{ti,xi,yi,ui,vi}i=1N​ f≐ut+λ1(uux+vuy)+px−λ2(uxx+uyy)g≐vt+λ1(uvx+vvy)+py−λ2(vxx+vyy) \begin{aligned} &amp;amp;f \doteq u_{t}+\lambda_{1}\left(u u_{x}+v u_{y}\right)+p_{x}-\lambda_{2}\left(u_{x x}+u_{y y}\right) \\ &amp;amp;g \doteq v_{t}+\lambda_{1}\left(u v_{x}+v v_{y}\right)+p_{y}-\lambda_{2}\left(v_{x x}+v_{y y}\right) \end{aligned} ​f≐ut​+λ1​(uux​+vuy​)+px​−λ2​(uxx​+uyy​)g≐vt​+λ1​(uvx​+vvy​)+py​−λ2​(vxx​+vyy​)​ The goal then is to learn λ={λ1,λ2}\lambda = \{\lambda_1, \lambda_2\}λ={λ1​,λ2​}, and pressure field p(t,x,y)p(t, x, y)p(t,x,y) by jointly approximating [ψ(t,x,y)p(t,x,y)][\psi(t, x, y) \quad p(t, x, y)][ψ(t,x,y)p(t,x,y)] with a single NN with two outputs. The total loss function is given by L≐1N∑i=1N(∣u(ti,xi,yi)−ui∣2+∣v(ti,xi,yi)−vi∣2)+1N∑i=1N(∣f(ti,xi,yi)∣2+∣g(ti,xi,yi)∣2) \begin{aligned} \mathcal{L} &amp;amp;\doteq \frac{1}{N} \sum_{i=1}^{N}\left(\left|u\left(t^{i}, x^{i}, y^{i}\right)-u^{i}\right|^{2}+\left|v\left(t^{i}, x^{i}, y^{i}\right)-v^{i}\right|^{2}\right) \\ &amp;amp;+ \frac{1}{N} \sum_{i=1}^{N}\left(\left|f\left(t^{i}, x^{i}, y^{i}\right)\right|^{2}+\left|g\left(t^{i}, x^{i}, y^{i}\right)\right|^{2}\right) \end{aligned} L​≐N1​i=1∑N​(∣∣​u(ti,xi,yi)−ui∣∣​2+∣∣​v(ti,xi,yi)−vi∣∣​2)+N1​i=1∑N​(∣∣​f(ti,xi,yi)∣∣​2+∣∣​g(ti,xi,yi)∣∣​2)​ The training is carried out using a spectral solver NekTar and then randomly sampling points out of the grid for collocation. The figure on the bottom shows the locations of the training data-points. PINN is able to successfully predict the pressure field with just 1%1\%1% of the available data as collocation points as shown below (and also able to learn the parameters λj\lambda_jλj​ in the process) Examples: KDv Equation As a final example illustrating data-driven discovery of PDEs, the authors choose an equation with higher order derivatives, namely the Korteweg-de Vries equation, that is encountered in the modeling of shallow water waves, ut+λ1uux+λ2uxxx=0 u_{t}+\lambda_{1} u u_{x}+\lambda_{2} u_{x x x}=0 ut​+λ1​uux​+λ2​uxxx​=0 The problem is to learn the parameters in a similar fashion as done for Navier-Stokes Equation N[un+cj]=λ1un+cjuxn+cj−λ2uxxxn+cj \mathcal{N}\left[u^{n+c_{j}}\right]=\lambda_{1} u^{n+c_{j}} u_{x}^{n+c_{j}}-\lambda_{2} u_{x x x}^{n+c_{j}} N[un+cj​]=λ1​un+cj​uxn+cj​​−λ2​uxxxn+cj​​ PINN again is able to learn the parameters to the desired accuracy and provide an accurate resolution of the dynamics of the system. Conclusion The major take-away of the paper can be summarized in the following concluding remarks. The authors introduced PINNs, a new class of universal function approximators that are capable of encoding any underlying physical laws that govern a given data-set (described by PDEs) They also prove a design for data-driven algorithms that can be used for inferring solutions to general nonlinear PDEs, and constructing computationally efficient physics-informed surrogate models. They also rightly point to some lingering questions that still remain unanswered in the original paper: How deep/wide should the neural network be ? How much data is really needed ? Why does the algorithm converge to unique values for the parameters of the differential operators, i.e., why is the algorithm not suffering from local optima for the parameters of the differential operator? Does the network suffer from vanishing gradients for deeper architectures and higher order differential operators? Could this be mitigated by using different activation functions? Can we improve on initializing the network weights or normalizing the data? How about the choices for the loss function choices (MSE, SSE)? What about the robustness of these networks especially when applied to solve chaotic PDEs/ODEs? References Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M.Raissi, P.Perdikaris, G.E.Karniadakis Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations M.Raissi, P.Perdikaris, G.E.Karniadakis Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations M.Raissi, P.Perdikaris, G.E.Karniadakis</summary></entry><entry><title type="html">DS1 Discovering Equations</title><link href="https://cs598ban.github.io/Fall2021/ds/physics+ml/2021/11/18/DS1_blog.html" rel="alternate" type="text/html" title="DS1 Discovering Equations" /><published>2021-11-18T00:00:00-06:00</published><updated>2021-11-18T00:00:00-06:00</updated><id>https://cs598ban.github.io/Fall2021/ds/physics+ml/2021/11/18/DS1_blog</id><author><name></name></author><category term="DS" /><category term="Physics+ML" /><summary type="html">Abstract Today we’ll talk about on how we can use compressive sensive and optimization to learn the dynamics of an underlying physical system. This blog is based on the paper from Prof. Kutz’s lab - Discovering governing equations from data by sparse identification of nonlinear dynamical systems. The paper develops a dictionary learning framework with overrepresented dictionary which enables us for interpretable non-linearity. We’ll discuss about it in the details below. History : Modeling Dynamics from Data Science started from astronomy, which started by observing the movements of stars, planets and heavenly bodies. Copernicus presented heliocentric theory which proposed sun as the center of the universe instead of earth. Later, Kepler gathered huge amount of planetary motion data. This enabled him to come up with three Kepler’s laws, Finally, Newton developed a unified theory of motion across the universe. While Kepler’s contribution was primarily in generating detailed data, Newton’s contribution was in developing a unified theory which explains data. Background: Symbolic Regression and Compressive Sensing Symbolic regression which is a generalization of regression, aims to learn the underlying interaction in data. It searches over the space of all possible mathematical formulas which best predict the output variable, starting from a set of base functions like addition, trigonometric functions, and exponentials. However, it is computationally expensive, does not clearly scale well to large-scale dynamical systems of interest, and may be prone to over-fitting. This paper utilizes ideas from sparse regression which has been used to find solutions for underdetermined linear systems. Sparsity helps us break the Nyquist–Shannon sampling theorem. In sparse regression, signal xxx is K sparse, x=Ψsx = \Psi sx=Ψs l1l1l1 norm is often imposed as regularization for sparsity. Extensive research in this area has been done often referred as Compressive Sensing. Terrence Tao, who is a professor at UCLA actively works on this. Their methods bypasees our need to perform a combinatorially intractable bruteforce search for sparse solution. It is found with high probability using convex methods that scale to large problems. Problem Set up: Dynamical Systems ddtx(t)=f(x(t))\frac{d}{dt}\mathbf{x}(t) = f(\mathbf{x}(t))dtd​x(t)=f(x(t)) Let’s assume x(t)\mathbf{x}(t)x(t) denotes state at time ttt, fff denotes with dynamics f(\mathbf{x}(t). In most physical systems, only a few nonlinear terms in the dynamics exist. Therefore it’s sparse in a high-dimensional nonlinear function space. So, if we can perform non-linear transformation, we can cast problem of identification of dynamical system as sparse regresssion. Dynamical Systems as Sparse Regression Let’s denote X\mathbf{X}X and X˙\dot{\mathbf{X}}X˙ as state and dynamic matrix. kkk -th row of X\mathbf{X}X contains observation vector at time kkk. Now, let’s do many transformation of X\mathbf{X}X , such as linear, polynomial, cosine and exponentials. This should be guided by the applications where the possible non-linearities is known. Finally we append these matrices to contain a big matrix Θ\ThetaΘ. Then we can solve the following problem, X˙=ΘW\dot{\mathbf{X}} =\Theta WX˙=ΘW where, weight WWW has sparse coefficients. It can be seen that each row has separate optimization. Therefore, complexity increaseslinearly with the number of time instants and not on the number of non-linear transformations. Basis functions are really important here. So we should test many different function bases and use the sparsity and accuracy of the resulting model as a diagnostic tool to determine the correct basis to represent the dynamics. Approximating Derivatives In reality we only observe discrete values of X\mathbf{X}X and X˙\dot{\mathbf{X}}X˙ is not known. Therefore, we need to approximate it. Total variation regularization is used to denoise derivative. To counter the noise due to approximation, we should rather solve the following problem, X˙=ΘW+σZ\dot{\mathbf{X}} = \Theta W + \sigma ZX˙=ΘW+σZ where, ZZZ is iid and normally distributed. Traditionally, people use Lasso for sparse regression. However, it is computationally expensive. The paper uses Sequential thresholded least-squares algorithm to find it. Depending on the noise, it may be necessary to filter X\mathbf{X}X and X˙\dot{\mathbf{X}}X˙. Dynamical Systems with PDE Most of the physical systems are PDE and not ODE. This method poses a serious problem since numerical discretization on a spatial grid is exponentially large. For example, in fluid Dynamics, a simple 2D and 3D flows may require tens of thousands up to billions of variables to represent the discretized system. Therefore, current formulation is ill suited, since each row has separate optimization. However, the good news that many high-dimensional systems of interest evolve on a low dimensional manifold or attractor that is well-approximated using a low-rank basis. Therefore, we can first use SVD to decompose X\mathbf{X}X. This has attracted decades of research for fluid dymaics. Proper orthogonal decomposition is a state of the art method for decomposing the flow governed by the Navier stokes formulation. Results: Lorenz System The paper shows that proposed method can identify lorenz attractor. It can capture rich and chaotic dynamics that evolve on an attractor. Only a few terms in the right-hand side of the equation are required Results: Fluid Dynamics The paper simulates fluid flow data, Data are collected for the fluid flow past a cylinder using direct numerical simulations of the 2D Navier–Stokes equations. It then transforms to a lower dimension and perform sparse regression. We can see that it is able to identify dynamics. Extensions There can be a scenario where dynamics is also dependent on other parameters. In these scenario, we can solve, x˙=f(x,μ)\dot{x} = f(x,\mu)x˙=f(x,μ) References Brunton, S. L., Proctor, J. L., &amp;amp; Kutz, J. N. (2016). Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15), 3932-3937.</summary></entry><entry><title type="html">NODE2 Augmented Neural ODE</title><link href="https://cs598ban.github.io/Fall2021/normalizing%20flows/2021/11/04/NODE2_blog.html" rel="alternate" type="text/html" title="NODE2 Augmented Neural ODE" /><published>2021-11-04T00:00:00-05:00</published><updated>2021-11-04T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/normalizing%20flows/2021/11/04/NODE2_blog</id><author><name></name></author><category term="Normalizing Flows" /><summary type="html"></summary></entry><entry><title type="html">NODE1 Neural ODE</title><link href="https://cs598ban.github.io/Fall2021/node/2021/11/02/NODE1_blog.html" rel="alternate" type="text/html" title="NODE1 Neural ODE" /><published>2021-11-02T00:00:00-05:00</published><updated>2021-11-02T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/node/2021/11/02/NODE1_blog</id><author><name></name></author><category term="NODE" /><summary type="html">Abstract Today we’ll talk about the popular Neural ODE paper which won best paper award in Neurips 2018. This paper develops a foundational framework for infinitely many layered deep neural networks. The framework allows us to take advantage of the extensive research on ODE Solvers. Background : Ordinary Differential Equations In physics, Ordinary Differential Equations(ODEs) have been often used to describe the dynamics and referred as a vector field of the underlying physical system. A system of ODEs can be represented as, dydt=f(t,y(t))\frac{dy}{dt} = f(t,y(t))dtdy​=f(t,y(t)) Neural ODEs aims to replace explicit ODEs by an ODE with learnable parameters. In past, extensive research has been done on explicit and implicit ODESolvers which aims to solve an ODE(forward pass). The simplest ODE solver Forward Euler method works by moving along the gradient from a starting point: yn+1=yn+δf(tn,yn)y_{n+1} = y_n + \delta f(t_n,y_n)yn+1​=yn​+δf(tn​,yn​) Sophisticated higher order explicit ODE solvers like Rungakutta have low error margin. An example of implicit ODE Solver is Backward Euler Method: yn+1=yn+δf(tn+1,yn+1)y_{n+1} = y_n + \delta f(t_{n+1},y_{n+1})yn+1​=yn​+δf(tn+1​,yn+1​) The implicit solvers are computationally expensive but often has better approximation guarantees. Various adaptive-step size solvers have been developed which provide better error handling. Problem Setup: Supervised learning Traditional Machine learning solves: y=f(x)y = f(x)y=f(x), where, yyy is label and xxx is input features. Neural ODEs views the same problem as an ODE with Initial value problem, dydt=f′(x),y(0)=x\frac{dy}{dt} = f'(x), y(0) = xdtdy​=f′(x),y(0)=x where the value at initial time point is input features xxx and the label yyy is the value at final time point. For now, Let’s assume that xxx and yyy have same dimensionality for simplicity. Neural ODE aims to learn an invertible transformation between xxx and yyy. The idea of viewing supervised learning as an ODE system came by observing the updates in Resnet. In Resnet, the updates in t+1t+1t+1-th layer is as follows, ht+1=ht+f(ht,θt)\mathbf{h}_{t+1} = \mathbf{h}_t + f(\mathbf{h}_t, \theta_t)ht+1​=ht​+f(ht​,θt​) We can view above update as the euler discretization of a continuous dynamic system, dh(t)dt=f(h(t),t,θ)\frac{d\mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta)dtdh(t)​=f(h(t),t,θ) We can therefore view each layer as an euler update in an ODE solver with dynamics fff. Infinite layers: ODE Solver as foward pass Did you ever think that a DNN can have infinite layers? If we try to implement it naively by having different parameters in each layer, then there are two prominent issues we’ll be facing, i)Overfitting ii)Memory issues. This is because each layer with learnable parameters in DNN needs to store its input until the backward pass. So, it’s not practically feasible in traditional setting. A natural question arises that “why can’t we have infinite updates in ODE solver instead of lll updates?”, where lll denotes number of euler steps or layers. We just need to use state of the art adaptive ODE Solvers which is memory efficient (more discussed later). Neural Ordinary Differential equations Here we summarize the underlying idea of Neural ODE. Instead of trying to solve for y=F(x)y = F(x)y=F(x), Neural ODE solves, y=z(t1)y = z(t_1)y=z(t1​), given the initial condition z(0)=xz(0) = xz(0)=x. The parametrization is done as, dz(t)dt=f(z(t),t,θ)\frac{dz(t)}{dt} = f(z(t), t, \theta)dtdz(t)​=f(z(t),t,θ) The existing ODE Solvers are used for forward pass. Backpropagation through Neural ODE Ultimately we want to optimize some loss w.r.t. parameters θ,t0,t1\theta, t_0, t_1θ,t0​,t1​, L(z(t1))=L(z(t0)+∫t0Tf(z(t),t,θ)=L(ODESolve(z(t0),t0,t1,θ)L(z(t_1)) = L(z(t_0) + \int_{t_0}^{T} f(z(t), t, \theta) = L (ODESolve(z(t_0), t_0, t_1, \theta)L(z(t1​))=L(z(t0​)+∫t0​T​f(z(t),t,θ)=L(ODESolve(z(t0​),t0​,t1​,θ) If loss is mean squared error, then we can write, L(z(t1))=E((y−z(t1))2L(z(t_1)) = E((y-z(t_1))^2L(z(t1​))=E((y−z(t1​))2. Without loss of generality, we’ll primary focus on computing dLdθ\frac{d L}{d \theta}dθdL​. If we know the ODE solver, then we can backprop through the solver using automatic differentiation. However, there are two issues with this approach, i) Backpropagation is dependent on ODE Solvers, this is not desirable. Ideally, we would like to treat ODE Solver as a black box. ii) If we use “implicit” solvers which often perform inner optimization, then the backpropagation using automatic differentiation is memory intensive. Adjoint sensitivity analysis: Reverse-mode Autodiff This paper borrows an age old idea of adjoint based methods from ODE literature to perform backprobagation with respect to parameters θ\thetaθ in constant memory and without having the knowledge of ODE Solver. To compute dLdθ\frac{d L}{d \theta}dθdL​ we need to compute dLdz(t)\frac{d L}{d \mathbf{z}(t)}dz(t)dL​. Therefore, let’s define the adjoint state a(t)=dLdz(t)\mathbf{a}(t) = \frac{d L}{d \mathbf{z}(t)}a(t)=dz(t)dL​. In the case of Resnet, the adjoint state has following updates during backpropagation, a(t)=a(t+h)+ha(t+h)df(z(t))dz(t)a(t) = a(t+h) + h a(t+h) \frac{d f(z(t))}{dz(t)}a(t)=a(t+h)+ha(t+h)dz(t)df(z(t))​ You might have observed that updates of adjoint a(t)a(t)a(t) is an euler step in backward direction with a known dynamics. You guessed it, right. The adjoint state in continuous setting follows following dynamics in backward direction, a(t)=a(t+1)+∫t+1ta(t)df(z(t),t,θ)dza(t) = a(t+1) + \int_{t+1}^t a(t)\frac{d f(\mathbf{z}(t), t, \theta)}{d \mathbf{z}}a(t)=a(t+1)+∫t+1t​a(t)dzdf(z(t),t,θ)​ da(t)dt=−a(t)Tdf(z(t),t,θ)dz\frac{d \mathbf{a}(t) }{dt} = - \mathbf{a}(t)^T \frac{d f(\mathbf{z}(t), t, \theta)}{d \mathbf{z}}dtda(t)​=−a(t)Tdzdf(z(t),t,θ)​ For Resnet, the updates of loss is, dLdθ=ha(t+h)df(z(t),t,θ)dθ\frac{d L}{d \theta} = h a(t+h)\frac{d f(\mathbf{z}(t), t, \theta)}{d\theta}dθdL​=ha(t+h)dθdf(z(t),t,θ)​ Similarly, loss in continuous setting follows a dynamics in the backward direction, dLdθ=∫t1t0a(t)df(z(t),t,θ)dθ\frac{d L}{d \theta} = \int_{t_1}^{t_0}a(t) \frac{d f(\mathbf{z}(t), t, \theta)}{d\theta}dθdL​=∫t1​t0​​a(t)dθdf(z(t),t,θ)​ Thus, during the backward pass of z(t)z(t)z(t), we also need to do a backward pass on a(t)a(t)a(t) and dLdθ\frac{d L}{d \theta}dθdL​. The vector jacobian products a(t)T∂f(z(t),t,θ)∂z\mathbf{a}(t)^T \frac{\partial f(\mathbf{z}(t), t, \theta)}{\partial \mathbf{z}}a(t)T∂z∂f(z(t),t,θ)​ and a(t)T∂f(z(t),t,θ)∂θ\mathbf{a}(t)^T \frac{\partial f(\mathbf{z}(t), t, \theta)}{\partial \theta }a(t)T∂θ∂f(z(t),t,θ)​ can be computed using automatic differentiation in similar time cost as of fff. The full algorithms for all three backward dynamics is shown below, Results Experiment 1 : Supervised Learning The paper performs a supervised learning on MNIST data. It uses resnet and multi-layered perceptron as baselines. The result is shown below, RK-Net uses Runga kutta for forward pass and automatic differentiation for backward pass. We can see that ODENet has fewer parameters with similar accuracy and constant memory. Experiment 2 : Normalizing flows The paper proposes a continuous version of normalizing flows. Traditionally normalizing flows is enabled using change of variable formula. This paper proposes instantaneous change of variables formula using ODE dynamics. The results are shown belo, Time Series Latent ODE Traditional Rnns can only utilize regular time interval signals in its vanilla form. Neural ODE allows us to sample from a continuous dynamic system, The result for both the RNN and neural ODE for continuous time series for a spiral synthetic data is shown below. It can be seen that RNNs learn very stiff dynamics and have exploding gradients while ODEs are guaranteed to be smooth. Conclusion Personally, I believe that this is a phenomonal paper which has enabled us to have infinite layers, handle continuous time series and normalizing flow in constant memory by using the state of the art ODE Solvers. It’s impact on learning dynamics for physical and biophysical system would be immense in future. References Chen, R. T., Rubanova, Y., Bettencourt, J., &amp;amp; Duvenaud, D. (2018). Neural ordinary differential equations. arXiv preprint arXiv:1806.07366.</summary></entry><entry><title type="html">GAN5 Generalization and Equilibrium in Generative Adversarial Nets (GANs)</title><link href="https://cs598ban.github.io/Fall2021/gan/2021/10/28/GAN5_blog.html" rel="alternate" type="text/html" title="GAN5 Generalization and Equilibrium in Generative Adversarial Nets (GANs)" /><published>2021-10-28T00:00:00-05:00</published><updated>2021-10-28T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/gan/2021/10/28/GAN5_blog</id><author><name></name></author><category term="GAN" /><summary type="html">Intro This blog post focuses on the paper Generalization and Equilibrium in Generative Adversarial Nets (GANs) and focuses on the concept of generalization with applications to GANs. The paper introduces the concept of generalization and the fact that the GAN model does not perform good generalization. The paper introduces a new type of distance measure to better calculate the generalization factor. The paper subsequently comes up within a new model called the MIX-GAN which focuses on demonstrating the importance of generalization when it comes to GAN training. The paper aims to help reader’s understand the purpose of the GAN model and how the model can be improved through the implementation of generalization while training the GAN. ​ ​ Background GANs were first introduced by Goodfellow et al. 2014 and the Goodfellow GAN can be described as follows: GANs are type of generative model used to come up with images or other samples to imitate real life samples. In more detail, the objective of the GAN when it comes to training a generator deep net whose input is a standard Gaussian, and whose output is a sample from some real distribution. GANs work by comparing two distributions, as mentioned above, where the generator net part of the GAN attempts to “fool” the discriminator function into thinking that the generated image is a real image. This dual system is trained through backpropagation of the result from the discriminator that feeds into the generator. Thus the relationship between the generator and the discriminator can be thought of as a game between two players solving a min max scenario. This concept is further explored within the paper. The min max game is critical to understanding GANs and how they are trained. (traditional GAN training objective from Goodfellow 2014) ​ In more detail the GAN objective function aims to minimize the output from the generator but the opposite is true for the discriminator which aims to maximize the result. The result is the label assigned to the training sample and then re-evaluated in the long term. ​ Generalization The definition of generalization in terms of GANs is related to how well the GAN is able to imitate the real distribution. A lot of the times the discriminator will be fooled but the derived distribution from the GAN model is weak and very ill representative of the actual distribution. Standard metrics are not as useful as measuring whether the distribution is close or not. Often there are times the GAN is able to train successfully to imitate the real distribution but according to standard metrics the trained distribution is deemed extremely different than the actual distribution, even though it might not be. Generalization is important when it comes to describing when the generator has won. At this point you need to consider is the created distribution and the real distribution the same or at least similar? This is tough to do when you have complicated distributions that have many valleys and peaks. Take for example this distribution: The distribution as mentioned has many peaks and valleys and when considering this distribution with high dimensionality there could be an exponential number of extrema. Sampling becomes a big issue. How do you sample so that all the extrema are accurately represented? How large would the sample size need to be for this to work? These questions are the underlying issue when it comes to modeling and working high dimensionality and complex distributions. The simple answer boils down to having a sample size of exponential size as well. However, that isn’t feasible and thus needs to be made more realistic. Generalization Math Let x1…xm be the training examples and let D^real be the uniform distribution for x1…xm. Also let Gu(h1)…Gu(hr) be a set of r examples from the generated distribution DG. In training, we use: to approximate the value of: Attempt to minimize the distance (or divergence) between D^real and DG. This can be done to a certain extreme and so the final simplified version of this equation can be represented as: The equation above gives a more concrete mathematical equation for Generalization and it can explained as the following: “generalization in GANs means that the population distance between the true and generated distribution is close to the empirical distance between the empirical distributions. Our target is to make the former distance small, whereas the latter one is what we can access and minimize in practice. The definition allows only polynomial number of samples from the generated distribution because the training algorithm should run in polynomial time.” The concept is a little confusing but can be simplified to this: in an ideal world we can draw an infinite number of samples from the true distribution to get the value for Dreal, however; in practicality that is not possible and so we have the empirical distance with a finite number of samples to estimate the expected value of the real distribution as seen above. The equation above takes it a step further and explicates that the distance between the empirical true distribution and empirical generated distribution is as close as it can be to the population distance between the population true distribution and the population generated distribution. The population distance is calculated theoretically and used in the calculation. Thus, the calculation is completed with an error bound that acts as measurement for the difference between the two distances. If the empirical distance is close enough to the population distance within this error bound, we can consider the empirical distance to be the same as the population distance. Neural Net Distance The paper brings up a problem with current metrics and that they are a bad method of measuring how well the generated distribution follows and imitates the original distribution. So, the authors behind the paper came up with a new measuring metrics strictly to see how well two distributions match each other. This metric is the neural net distance metric. Neural Net Distance is derived from the f-divergence concept and follows the derivation of the f-divergence function closely. For example when φ(t) is log(t) and F = {all functions from Rd mapped to [0, 1]} then the derived equation above is equal to JS divergence. However, when φ(t) is t and F = {all 1-Lipschitz functions from Rd mapped to [0, 1]} then the equation above is equal to Wasserstein distance. (Lipschitz functions are mentioned and Lipschitz refers to the training parameters and indicates that changing a parameter by delta changes the output of the deep net by less than constant * delta.) All this is to indicate that the distance metric is variable and changes with what kind of base GAN you are working with / what kind of distribution you’d like to measure. GAN training uses F to be a class of neural nets with a bound p on the number of parameters. An assumption can be made regarding the measuring function such that it takes in values between [−∆, ∆] and that it is Lφ-Lipschitz. Even more so, F is a class of discriminators that is L-Lipschitz to the target distribution where p denotes the number of parameters in the target distribution. In a general sense you get the equation: The downside of the Neural Net Distance is that it will return a small value even if u and v are are far from each other. This is due to the capacity being limited by p. Equilibrium &amp;amp; Mixtures The idea of equilibrium stems from the GAN min max problem. When trying to train and optimize a GAN the generator must be minimized and the discriminator needs to be maximized. Equilibrium seeks a state where both the discriminator and the generator cannot be optimized further. The thing to keep in mind is that the saddle point reached is not necessarily zero. This paper puts a spin on that and recognizes an “equilibrium” value where generator is always less than or equal to this value. Conversely, the discriminator is always greater or equal to this value as demonstrated below. The S value seen in the theorem is used to represent the strategies that the discriminator and generator use respectively. The strategies are unchanged and so there is no changing of strategies once the other player’s strategy is revealed. Each player still performs to the best of their ability keeping in mind their initial strategy. The paper focuses only on the generator side as opposed to the disciminator. The reasons for the focus on the generator only is as follows: Payoff is generated by the generator: The payoff is generated by the generator first sample u ∼ Su, h ∼ Dh. This results in an example that is generated x = Gu(h). Mixed Generator Composition vs. Mixed Discriminator: The mixed generator is compromised of a linear mixture of generators. The mixed discriminator is more complicated since the objective function of the discriminator may not be linear. Thus, when accounting for the objective function it escapes a linear space; otherwise the mixed discriminator would also be linear. Output of Discriminator is not important: The output of the discriminator mixture is not important for consideration since the mixture is also unable to differentiate between the generated and actual distribution effectively. All these scenarios are resolved using a mixture generator and approximating the difference between the two distributions to a certain error value (as seen with all the formulas above). This is what the strategies come out to be after accounting for a small margin of error. The basic idea here is that it takes O§ (where p is the number of parameters in the sample set) to approximate a quality distribution that is able to imitate the real distribution while accounting for the error. There is a more complex version of this that takes into account pure strategies. The paper does not go into details about pure strategies but discusses a short overview about them. The idea boils down to the concept of mixtures, there is not much change from what is already written in this blog besides the fact that if a pure strategy is attempted then the complexity increases to O(p2) since each network compromises of O§ size and then those are used to make the mixture generator in a linear fashion. MIX-GAN A mix GAN is a GAN that makes use of the idea of a mixture of generators within reason. So they make use of a mixture of T components, where T is constrained by GPU memory (&amp;lt; 5). Maintain T generators and T discriminators. All the respective weights for each generator is maintained and everything is trained through backwards propagation. The specific softmax used for training is listed below. The softmax function is critical for defining the weights which in turn are needed for the payoff function for the MIX+GAN. You can think of it as the original min max problem for the GAN as the structure of the equations are similar. Empirical Results The examples above show the comparison between a typical DC-GAN and a MIX-DCGAN. The MIX-GAN is represented by the A side and the normal DCGAN is represented by the B side. THe interesting thing to note is the quality of the results of the GANs. The MIX-GAN performed slightly better in terms of more clear output compared to the results of the regular GAN. This is further evidenced by the results of the CIFAR-10 test results. The MIX-GAN is able to compete to the same performance level of the GAN that it models. At times it does better than the GAN model that it is imitating. The most curious thing about the MIX-GAN is ability to be small and retain very little parameters yet be able to compete with more complex and larger GANs, as seen with the MIX-DCGAN and DCGAN (5x time) size scores. Conclusions The concept of the MIX-GAN leads to more efficient GANs. They are typically smaller and perform the same as the normal version of the particular GAN model that they are attempting to turn into a mixture through the measuring function. The MIX-GAN allows for more quality samples and more diverse samples from the generated distribution because it is able to better reflect the true distribution unlike other GANs. On a theoretical standpoint this paper focused mainly improving the GAN’s ability to come to a good stopping point that properly reflects the true distribution. Many GANs fail to do this hence the team’s research into generalization and the neural net distance metric for a better way to determine if the generated distribution and real distribution are similar or not. References 2017 (ICML): S. Arora, R. Ge, Y. Liang, T. Ma, Y. Zhang. Generalization and equilibrium in generative adversarial nets (GANs). ICML, 2017.</summary></entry></feed>