<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://cs598ban.github.io/Fall2021/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cs598ban.github.io/Fall2021/" rel="alternate" type="text/html" /><updated>2021-11-12T18:52:02-06:00</updated><id>https://cs598ban.github.io/Fall2021/feed.xml</id><title type="html">CS 598 Deep Generative and Dynamical Models</title><subtitle>Blogs created as a part of graduate course at UIUC on deep generative and dynamical models.</subtitle><entry><title type="html">Review on Diagnosing and Enhancing VAE Models (ICLR ‘19)</title><link href="https://cs598ban.github.io/Fall2021/variational%20autoencoders/2021/09/21/VAE2_blog.html" rel="alternate" type="text/html" title="Review on Diagnosing and Enhancing VAE Models (ICLR ‘19)" /><published>2021-09-21T00:00:00-05:00</published><updated>2021-09-21T00:00:00-05:00</updated><id>https://cs598ban.github.io/Fall2021/variational%20autoencoders/2021/09/21/VAE2_blog</id><author><name></name></author><category term="Variational Autoencoders" /><summary type="html">Diagnosing and Enhancing VAE Models (ICLR '19)1 Introduction Even though variational autoencoders (VAEs)2 have a wide variety applications in deep generative models, many aspects of the underlying energy function remain poorly understod. It is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this paper, the authors rigorously analyzed that reaching the global optimum does not guarantee that if VAE model can learn the true distribution of data, i.e., there could exist alternative solutions that both reach the global optimum and yet do not assign the same probability measure as ground-truth probability distribution. And it also proposed a two-stage remedy model, i.e., a two-stage VAE model to address the above issues and enhance the original VAE so that any gloablly minimizing solution is uniquely matched to the ground-truth distribution. Problem Definition: The starting point is the desire to learn a probabilistic generative model of observable variables x∈Xx \in \mathcal Xx∈Xwhere X\mathcal XX is a r-dimensional manifold embedded in Rd\mathbb R ^dRd Denote a ground-truth probability measure on X\mathcal{X}X as μgt\mu_{gt}μgt​ where ∫Xμgtdx=1\int_{\mathcal{X}} \mu_{gt} d\mathbf{x} = 1∫X​μgt​dx=1 The canonical VAE attempts to approximate this ground-truth measure using parameterized density pθ(x)p_{\theta}(\mathbf{x})pθ​(x) where pθ(x)=∫pθ(x∣z)p(z)dzp_{\theta}(x) = \int p_{\theta}(x | z) p(z) dzpθ​(x)=∫pθ​(x∣z)p(z)dz, z∈Rκz \in \mathbb{R}^\kappaz∈Rκ with κ≈r\kappa \approx rκ≈r and p(z)=N(z∣0,I)p(z) = \mathcal{N}(z | 0, \mathbf{I})p(z)=N(z∣0,I) We will consider two situations where r&amp;lt;dr &amp;lt; dr&amp;lt;d and r=dr = dr=d to illustrate the aforementioned non-uniqueness issues. VAE Objective In the vanilla VAE model, we normally write the objective function to be optimized as evidence lower bound (ELBO): Lθ,ϕ(x)=−Eqϕ(z∣x)[log⁡pθ(x,z)−log⁡qϕ(z∣x)]=KL(qϕ(z∣x)∣∣pθ(z))+Eqϕ(z∣x)[−log⁡pθ(x,z)]\begin{align*} \mathcal{L}_{\theta, \phi}(x) &amp;amp; = -\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x, z) - \log q_{\phi}(z|x)] \\ &amp;amp; = \mathbb{KL}(q_{\phi}(z|x) || p_{\theta}(z)) + \mathbb{E}_{q_{\phi}(z|x)}[-\log p_{\theta}(x, z)] \end{align*}Lθ,ϕ​(x)​=−Eqϕ​(z∣x)​[logpθ​(x,z)−logqϕ​(z∣x)]=KL(qϕ​(z∣x)∣∣pθ​(z))+Eqϕ​(z∣x)​[−logpθ​(x,z)]​ In this case, based on the ground-truth probability measure μgt\mu_{gt}μgt​, we can rewrite it into: Lθ,ϕ(x)=∫X{−log⁡pθ(x)+KL[qϕ(z∣x)∣∣pθ(z∣x)]}μgtdx≥∫X−log⁡pθ(x)μgtdxLθ,ϕ(x)=∫X{−Eqϕ(z∣x)[log⁡pθ(z∣x)]+KL[qϕ(z∣x)∣∣p(z)]}μgtdx\begin{align*} \mathcal{L}_{\theta, \phi}(x) &amp;amp; = \int_{\mathcal{X}}\{-\log p_{\theta}(x) + \mathbb{KL}[q_{\phi}(z|x) || p_{\theta}(z|x)]\} \mu_{gt} dx \geq \int_{\mathcal{X}} -\log p_{\theta}(x)\mu_{gt} dx \\ \mathcal{L}_{\theta, \phi}(x) &amp;amp; = \int_{\mathcal{X}} \{-\mathbb{E}_{q_{\phi}(z|x)} [\log p_{\theta}(z|x)] + \mathbb{KL}[q_{\phi}(z|x) || p(z)]\} \mu_{gt} dx \end{align*}Lθ,ϕ​(x)Lθ,ϕ​(x)​=∫X​{−logpθ​(x)+KL[qϕ​(z∣x)∣∣pθ​(z∣x)]}μgt​dx≥∫X​−logpθ​(x)μgt​dx=∫X​{−Eqϕ​(z∣x)​[logpθ​(z∣x)]+KL[qϕ​(z∣x)∣∣p(z)]}μgt​dx​ In principle, qϕ(z∣x)q_{\phi}(z|x)qϕ​(z∣x) and pθ(x∣z)p_{\theta}(x|z)pθ​(x∣z) can be arbitrary distributions. In the practical implementation, a commonly adopted distributional assumption is that both distribution are Gaussian, which was previously considered as a limitation of VAE. Diagnosing the Non-uniqueness Ideas: Even with the stated Gaussian distributions, there exist parameters θ,ϕ\theta, \phiθ,ϕ that can simultaneously: Globally optimize the VAE object Recover the ground-truth probability measure in a certain sense Definition 1: A κ\kappaκ-simple VAE is defined as a VAE model with dim[z\mathbf{z}z] = κ\kappaκ latent dimensions, the Gaussian encoder qϕ(z∣X)=N(z∣μz,Σz)q_{\phi}(z|X) = \mathcal{N}(z | \mu_z, \Sigma_z)qϕ​(z∣X)=N(z∣μz​,Σz​) and the Gaussian decoder pθ(x∣z)=N(x∣μx,Σx)p_{\theta}(x|z) = \mathcal{N}(x | \mu_x, \Sigma_x)pθ​(x∣z)=N(x∣μx​,Σx​) With these definitions, we can now move to the discussion of κ\kappaκ-simple VAE with κ≥r\kappa \geq rκ≥r can achieve the above optimality criteria from the simpler case where r=dr = dr=d followed by the extended scenario with r&amp;lt;dr &amp;lt; dr&amp;lt;d. When r=d Assuming pgt(x)=μgt(dx)/dxp_{gt}(x) = \mu_{gt}(dx) / dxpgt​(x)=μgt​(dx)/dx exists everywhere in Rd\mathbb{R}^dRd, the minimal possible value of negative log-likelihood will necessarily occur if KL[qϕ(z∣x)∣∣pθ(z∣x)]=0&amp;nbsp;and&amp;nbsp;pθ(x)=pgt(x)&amp;nbsp;almost&amp;nbsp;everywhere\mathbb{KL}[q_{\phi}(z|x) || p_{\theta}(z|x)] = 0 \text{ and } p_{\theta}(x) = p_{gt}(x) \text{ almost everywhere}KL[qϕ​(z∣x)∣∣pθ​(z∣x)]=0&amp;nbsp;and&amp;nbsp;pθ​(x)=pgt​(x)&amp;nbsp;almost&amp;nbsp;everywhere Naturally we will conclude that Theorem 2: Suppose that r=dr=dr=d and there exists a density pgt(x)p_{gt}(x)pgt​(x) associated with the ground-truth measure μgt\mu_{gt}μgt​ that is nonzero everywhere on Rd\mathbb{R}^dRd. Then for any κ≥r\kappa \geq rκ≥r, there is a sequence of κ\kappaκ-simple VAE model parameters {θt⋆,ϕt⋆}\{\theta_t^\star, \phi_t^\star\}{θt⋆​,ϕt⋆​} such that lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&amp;nbsp;and&amp;nbsp;lim⁡t→∞pθt⋆(x)=pgt(x)&amp;nbsp;almost&amp;nbsp;everywhere \lim_{t\to\infty} \mathbb{KL}[q_{\phi_t^\star}(z|x) || p_{\theta_t^\star}(z|x)] = 0 \text{ and } \lim_{t\to\infty} p_{\theta_t^\star}(x) = p_{gt}(x) \text{ almost everywhere} t→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&amp;nbsp;and&amp;nbsp;t→∞lim​pθt⋆​​(x)=pgt​(x)&amp;nbsp;almost&amp;nbsp;everywhere The theorem implies that as long as latent dimension is sufficiently large (i.e., κ≥r\kappa \geq rκ≥r), the optimal ground-truth probability measure can be recovered, whether the encoder and decder has Gaussian assumptions or not, since the ground-truth probability measure being recovered almost everywhere is the necessary conditions for optimized objective value. When r &amp;lt; d When both qϕ(z∣x)q_\phi(z|x)qϕ​(z∣x) and pθ(x∣z)p_{\theta}(x|z)pθ​(x∣z) are arbitrary/unconstrained, i.e., without Gaussian assumptions, then inf⁡ϕ,θL(θ,ϕ)=−∞\inf_{\phi, \theta} \mathcal{L}(\theta, \phi) = - \inftyinfϕ,θ​L(θ,ϕ)=−∞ by forcing qϕ(z∣x)=pθ(z∣x)q_{\phi}(z|x) = p_{\theta}(z|x)qϕ​(z∣x)=pθ​(z∣x). To show that this does not need to happen, define a manifold density p~gt(x)\tilde p_{gt}(x)p~​gt​(x) as the probability density of μgt\mu_{gt}μgt​ with respect to the volume measure of the manifold X\mathcal{X}X. If d=rd = rd=r then this volume is the standard Lebesgue measure in Rd\mathbb{R}^dRd and p~gt(x)=pgt(x)\tilde p_{gt}(x) = p_{gt}(x)p~​gt​(x)=pgt​(x) since when r&amp;lt;dr &amp;lt; dr&amp;lt;d, pgt(x)p_{gt}(x)pgt​(x) may not exist everywhere in the ambient space. Theorem 3: Assume r&amp;lt;dr &amp;lt; dr&amp;lt;d and that there exists a manifold density p~gt(x)\tilde p_{gt}(x)p~​gt​(x) associated with the ground-truth measure μgt\mu_{gt}μgt​ that is nonzero everywhere on X\mathcal{X}X. Then for any κ≥r\kappa \geq rκ≥r, there is a sequence of κ\kappaκ-simple VAE model parameters {θt⋆,ϕt⋆}\{\theta_t^\star, \phi_t^\star\}{θt⋆​,ϕt⋆​} such that lim⁡t→∞KL[qϕt⋆(z∣x)∣∣pθt⋆(z∣x)]=0&amp;nbsp;and&amp;nbsp;lim⁡t→∞∫X−log⁡pθt⋆(x)μgtdx=−∞\lim_{t\to\infty} \mathbb{KL}[q_{\phi_t^\star}(z|x) || p_{\theta_t^\star}(z|x)] = 0 \text{ and } \lim_{t \to \infty} \int_{\mathcal{X}} -\log p_{\theta_t^\star}(x) \mu_{gt} dx = -\inftyt→∞lim​KL[qϕt⋆​​(z∣x)∣∣pθt⋆​​(z∣x)]=0&amp;nbsp;and&amp;nbsp;t→∞lim​∫X​−logpθt⋆​​(x)μgt​dx=−∞ lim⁡t→∞∫X∈Apθt⋆(x)dx=μgt(A∪X)\lim_{t\to\infty} \int_{\mathcal{X} \in A} p_{\theta_t^\star} (x) dx = \mu_{gt} (A \cup \mathcal{X})t→∞lim​∫X∈A​pθt⋆​​(x)dx=μgt​(A∪X) for all measurable sets A⊆RdA \subseteq \mathbb{R}^dA⊆Rd with μgt(∂A∪X)=0\mu_{gt}(\partial A \cup \mathcal{X}) = 0μgt​(∂A∪X)=0 where ∂A\partial A∂A is the boundary of AAA. Implications of this theorem: From (1), the VAE Gaussian assumptions do not prevent minimization of L(θ,ϕ)\mathcal{L}(\theta, \phi)L(θ,ϕ) from converging to minus infinity. From (2), there exists solutions that assign a probability mass to most all measurable subsets of Rd\mathbb{R}^dRd that is distinguishable from the ground-truth measure. In r=dr = dr=d situation, the theorem necessitates that the ground-truth probability measure has been recovered almost everywhere. In r&amp;lt;dr &amp;lt; dr&amp;lt;d situation, we have not ruled out the possibility that a different set of parameters {θ,ϕ}\{\theta, \phi\}{θ,ϕ} can push the lost to −∞- \infty−∞ and not achieve (2), i.e., the VAE can reach the lower bound of negative log-likelihood but fail to closely approximate μgt\mu_{gt}μgt​. Optimal Solutions The necessary conditions for VAE optimal value would be induced from the following theorems. Theorem 4: Let {θγ⋆,ϕγ⋆}\{\theta^\star_\gamma, \phi_\gamma^\star\}{θγ⋆​,ϕγ⋆​} denote an optimal κ\kappaκ-simple VAE solution (with κ≥r\kappa \geq rκ≥r) where the decoder variance γ\gammaγ is fixed. Moreover, we assume that μgt\mu_{gt}μgt​ is not a Gaussian distribution when d=rd = rd=r. Then for any γ&amp;gt;0\gamma &amp;gt; 0γ&amp;gt;0, there exists a γ′&amp;lt;γ\gamma' &amp;lt; \gammaγ′&amp;lt;γ such that L(θγ′⋆,ϕγ′⋆)&amp;lt;L(θγ⋆,ϕγ⋆)\mathcal{L}(\theta_{\gamma'}^\star, \phi_{\gamma'}^\star) &amp;lt; \mathcal{L}(\theta_{\gamma}^\star, \phi_{\gamma}^\star)L(θγ′⋆​,ϕγ′⋆​)&amp;lt;L(θγ⋆​,ϕγ⋆​) The theorem implies that if γ\gammaγ is not constrained, it must be that γ→0\gamma \to 0γ→0 if we wish to minimize the VAE objective. While in existing practical VAE applications, it is standard to fix γ≈1\gamma \approx 1γ≈1 with the standard Gaussian assumptions during training. Theorem 5: Applying the same conditions and definitions in Theorem 4, then for all xxx drawn from μgt\mu_{gt}μgt​, we also have that lim⁡γ→0fμx[fμz(x;ϕγ⋆)+fSz(x;θγ⋆)ϵ;ϕγ⋆]=lim⁡γ→0fμx[fμz(x;ϕγ⋆);θγ⋆]=x,∀ϵ∈Rκ\lim_{\gamma \to 0} f_{\mu_x} [f_{\mu_z}(x; \phi_{\gamma}^\star) + f_{S_z}(x; \theta{\gamma}^\star) \epsilon; \phi_\gamma^\star] = \lim_{\gamma \to 0} f_{\mu_x}[f_{\mu_z}(x;\phi_\gamma^\star); \theta_\gamma^\star] = x, \forall \epsilon \in \mathbb{R}^\kappaγ→0lim​fμx​​[fμz​​(x;ϕγ⋆​)+fSz​​(x;θγ⋆)ϵ;ϕγ⋆​]=γ→0lim​fμx​​[fμz​​(x;ϕγ⋆​);θγ⋆​]=x,∀ϵ∈Rκ With this theorem, it indicates that any x∈X\mathbf{x} \in \mathcal{X}x∈X will be perfectly reconstructed by the VAE model at globally optimal solutions. Adding dimensions to latent dimension cannot improve the value of the VAE data term in meaningful way. In the training process, there are likely to be rrr eigenvalues of the decoder covariance converging to 0 and κ−r\kappa - rκ−r converging to one. This demonstrats that VAE has the ability to detect the manifold dimension and select the proper number of latent dimensionsin practical environments. If VAE model parameters have learned a near optimal mapping onto X\mathcal{X}X using γ≈0\gamma \approx 0γ≈0, then the VAE cost will scale as (d−r)log⁡γ(d - r) \log \gamma(d−r)logγ regardless of μgt\mu_{gt}μgt​. Two-Stage VAE Model The above analysis suggests the following two-stage remedy: Given nnn observed samples {x(i)}i=1n\{x^{(i)}\}^n_{i=1}{x(i)}i=1n​, train a κ\kappaκ-simple VAE, with κ≥r\kappa \geq rκ≥r, to estimate the unknown rrr-dimensional ground-truth manifold X\mathcal{X}X embedded in Rd\mathcal{R}^dRd using a minimal number of active latent dimensions. Generate latent samples {z(i)}i=1n\{z^{(i)}\}^n_{i=1}{z(i)}i=1n​ via z(i)∼qϕ(z∣x(i))z_{(i)} \sim q_{\phi}(z|x^{(i)})z(i)​∼qϕ​(z∣x(i)). Train a second κ\kappaκ-simple VAE, with independent parameters {θ′,ϕ′}\{\theta', \phi'\}{θ′,ϕ′} and latent representation uuu, to learn the unknown distribution qϕ(z)q_\phi(z)qϕ​(z) as a new ground-truth distribution and use samples {z(i)}i=1n\{z^{(i)}\}^n_{i=1}{z(i)}i=1n​ to learn it. Samples approximating the original ground-truth μgt\mu_{gt}μgt​ can then be formed via the extended ancestral process u∼N(u∣0,I),z∼pθ′(z∣u),x∼pθ(x∣z)u \sim \mathcal{N}(u | 0, \mathbf{I}), z \sim p_{\theta'}(z | u), x \sim p_{\theta}(x|z)u∼N(u∣0,I),z∼pθ′​(z∣u),x∼pθ​(x∣z) The structure of the first-stage of the Two-Stage VAE Model Analysis: If the first stage was successful, then even though they will not generally resemble N(z∣0,I)\mathcal{N}(z|0, \mathbf{I})N(z∣0,I), samples from qϕ(z)q_\phi(z)qϕ​(z) will have nonzero measure across the full ambient space Rκ\mathbb{R}^\kappaRκ. If κ&amp;gt;r\kappa &amp;gt; rκ&amp;gt;r, then the extra latent dimensions will be naturally filled in via randomness. Consequently, as long as we set κ≥r\kappa \geq rκ≥r, the operational regime of the second-stage VAE is effectively equivalent to the situation that the manifold dimension is equal to the ambient dimension, and reaching global optimum solutions would recover the ground-truth probability measure almost everywhere. Experiment Results The following table indicates the performance evaluation results of the experiments conducted on four significantly different datasets: MNIST, Fash-ion MNIST, CIFAR-10 and CelebA. The evaluation metrics used Frchet Inception Distance (FID)3 Score: used to assess the quality of images created by a generative model, comparing the generated images with the distribution of real images. Note: The training of two stages need to be separate. Concatenating two stages and jointly training does not improve the performance. Another set of experiments were conducted on the same datasets with different evaluation metrics. Kernel Inception Distance (KID)4 applies a polynomial-kernel Maximum Mean Discrepancy (MMD) measure to estimate the inception distance, as FID score is believed to exhibit bias in certain circumstances. Analysis of the Results: The second stage of Two-Stage VAE model can reduce the gap between q(z)q(z)q(z) and p(z)p(z)p(z), resulting in better manifold reconstruction. γ\gammaγ will converge to zero at any global minimum of the VAE objective, allowing for tighter image reconstructions with better manifold fit. Contributions and Conclusions This paper rigorously proved that VAE global optimum can in fact uniquely learn a mapping to the correct ground-truth manifold when r&amp;lt;dr &amp;lt; dr&amp;lt;d, but not necessarily the correct probability measure within this manifold. The proposed Two-Stage VAE model can resolve this issue and better recover the ground-truth manifold and reduce the gap between pθ(z∣x)p_\theta(z|x)pθ​(z∣x) and qϕ(z∣x)q_\phi(z|x)qϕ​(z∣x). And this is the first demonstration of a VAE pipeline that can produce stable FID scores that are comparable to at least some popular GAN models under neutral testing conditions. The two-stage mechanism can improve the reconstruction of original distribution so that it has comparable performance with GAN models. This work narrows the gap between VAE and GAN models in terms of the realism of generated samples so that VAEs are worth considering in a broader range of applications. No need Gaussian assumption in the canonical VAE model to achieve the optimal solutions. References Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In 7th International Conferenceon Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. ↩︎ Diederik Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014. ↩︎ Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and SeppHochreiter. GANs trained by a two time-scale update rule converge to a local Nashequilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637,2017. ↩︎ Miko laj Bi ́nkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. arXiv:1801.01401, 2018 ↩︎</summary></entry></feed>