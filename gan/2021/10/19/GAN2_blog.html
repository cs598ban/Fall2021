<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>GAN2 Understanding Deep Convolutional Generative Adversarial Networks | CS 598 Deep Generative and Dynamical Models</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="GAN2 Understanding Deep Convolutional Generative Adversarial Networks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Brandon Theodorou (bpt3@illinois.edu)" />
<meta property="og:description" content="Brandon Theodorou (bpt3@illinois.edu)" />
<link rel="canonical" href="https://cs598ban.github.io/Fall2021/gan/2021/10/19/GAN2_blog.html" />
<meta property="og:url" content="https://cs598ban.github.io/Fall2021/gan/2021/10/19/GAN2_blog.html" />
<meta property="og:site_name" content="CS 598 Deep Generative and Dynamical Models" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-19T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://cs598ban.github.io/Fall2021/gan/2021/10/19/GAN2_blog.html","@type":"BlogPosting","headline":"GAN2 Understanding Deep Convolutional Generative Adversarial Networks","dateModified":"2021-10-19T00:00:00-05:00","datePublished":"2021-10-19T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cs598ban.github.io/Fall2021/gan/2021/10/19/GAN2_blog.html"},"description":"Brandon Theodorou (bpt3@illinois.edu)","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Fall2021/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cs598ban.github.io/Fall2021/feed.xml" title="CS 598 Deep Generative and Dynamical Models" /><link rel="shortcut icon" type="image/x-icon" href="/Fall2021/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Fall2021/">CS 598 Deep Generative and Dynamical Models</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Fall2021/search/">Search</a><a class="page-link" href="/Fall2021/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">GAN2 Understanding Deep Convolutional Generative Adversarial Networks</h1><p class="page-description">Brandon Theodorou (bpt3@illinois.edu)</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-19T00:00:00-05:00" itemprop="datePublished">
        Oct 19, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Fall2021/categories/#GAN">GAN</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="stackedit__html">
<p>Today we will discuss the seminal Deep Convolutional Generative Adversarial Network (DCGAN) architecture as well as the paper and experiments by Alec Radford, Luke Metz, and Somit Chantal which introduced the architecture. DCGANs were representative of both the problems and promise of GANs during their early formulation, requiring tons of experimentation and tuning to properly train yet offering incredible and state of the art quality in terms of both their learned representation and image generation. The architecture worked wonderfully and continued to be used as the main GAN architecture for years after as additional innovations were built upon it. Hopefully this post allows you to understand DCGANs in terms of how they work and also the results they offered.</p>
<h2 id="background">Background</h2>
<p>GANs were first introduced by Goodfellow et al. and quickly caught on as an innovative yet powerful generative model. GANs are defined by a two player min-max game played between a pair of different trained models, typically parameterized as neural networks. These models are the Generator and the Discriminator. The Generator takes random noise and outputs a generated sample mirroring the desired data distribution, commonly an image. The Discriminator then takes samples, both real and fake, and attempts to predict whether they are real or not. In competing to be able to fool or correctly adjudicate one another, the models both improve until the Generator is able to output high quality images.</p>
<p><img src="/Fall2021/images/gan2blog1/GANs.png" alt="GAN training setup" /></p>
<p>The DCGAN utilizes this training setup, offering a new architecture for the two models. In its framing during the presentation by Radford et al., the DCGAN lies at the intersection of two major and well studied fields of Artificial Intelligence, and it seeks the build upon both. The first is unsupervised representation learning, which seeks to learn strong representations of samples within a dataset in order to understand and manipulate the relevant traits as well as perform downstream tasks. At the point of time when the DCGAN was introduced, this topic was already considered important, but the leading models and algorithms such as k-means, autoencoders, ladder networks, and deep belief networks were still relatively unrefined compared to modern approaches. Similarly, the second field is that of generating natural images. This too was already considered important and well-researched but was not yet overly impressive as the produced images were wobbly and blurry across models such as VAEs, RNNs with deconvolutions, and even the existing GANs. While the DCGAN did not jump all the way to modern quality in the two fields, it succeeded in taking a large step forward in both.</p>
<h2 id="the-architecture">The Architecture</h2>
<p>Up to this point, while GANs were undoubtedly an exciting architecture and idea, they had failed to consistently produce crisp output quality and were falling victim to a phenomenom termed mode collapse in which they produced a limited number of strong samples instead of a diverse representation of the entire distribution. Numerous papers including the introduction of Conditional GANs by Mirza and Osindero, a laplacian pyramid extension to GANs by Denton et al., a reccurent approach by Gregor et al., and a deconvolutional approach by Dosovitskiy et al. all were promising but struggled with either blurry or homogenous generation of natural images. A specific problem believed to be underlying these issues was that GANs were yet to successfully use the network most commonly used for image related tasks, CNNs. DCGANs fixed many of the downstream issues by addressing that underlying issue and utilizing CNNs as the core of their architecture, and they achieved that via a ton of experimentation and variation before settling on the following 4 innovations.</p>
<ul>
<li><strong>Eliminating Pooling Layers:</strong> Most CNN-based architectures at the time utilized maxpooling or simple repetition to downscaling and upscaling their representations to different sizes throughout the architecture. However, the DCGAN architecture utilized only strided convolutions and fractionally-strided convolutions for the same purpose in order to allow the network to learn its own upscaling and downscaling algorithms.<br />
<img src="/Fall2021/images/gan2blog1/FractionalStriding.png" alt="Fractional Striding" /></li>
<li><strong>Removing Fully Connected Layers:</strong> Most CNN-based architectures at the time also typically utilized fully connected layers as a head on top of their CNN model or otherwise within the model mixed into other blocks. However, the DCGAN eliminated as many such layers as possible, leaving only a single matrix multiplication at the start of the Generator to reshape the noise vector as well as a single sigmoid layer at the end of the Discriminator.</li>
<li><strong>Using Batch Normalization:</strong> The DCGAN also utilized the recently introduced batch normalization layer throughout their architecture. BatchNorm layers normalize the input to each layer which comes after them to be centered at zero and have unit variance in order to stabilize training and aid in gradient flow. They found that this helped with the problem of mode collapse and so applied it to all but the last Generator layer and first Discriminator layer.</li>
<li><strong>Adjusting the Activation Functions:</strong> The final augmentation or novelty offered by the DCGAN was finely tuned activation functions. They used Tanh for the Generator output, ReLU for the other Generator layers, and LeakyReLU throughout the Discriminator. This contrasted with the previously used Maxout for other GANs.<br />
<img src="/Fall2021/images/gan2blog1/Activations.png" alt="Activations" /></li>
</ul>
<p>These simple innovations were the result of a large amount of experimentation and combined to form the “all-CNN” DCGAN architecture which was able to stabilize training and improve both the generative capacity and also the learned representations.</p>
<p><img src="/Fall2021/images/gan2blog1/DCGAN.png" alt="DCGAN" /></p>
<h2 id="experiments">Experiments</h2>
<p>Upon arriving at their final DCGAN architecture, Goodfellow et al. continued to explore its power and effectiveness via a series of innovative and often fun experiments. The highlights of those experiments are described below.</p>
<h3 id="analysis-of-possible-memorization">Analysis of Possible Memorization</h3>
<p>First, the authors were concerned that given the power of their DCGAN architecture, their trained models might just be memorizing the training data. So, they performed a number of experiments and analyses to ensure and argue that they were not. First, they trained for a single epoch with a small learning rate before qualitatively looking at the already strong generative samples and making the argument that the model could not be memorizing data yet and so must instead be learning. Similarly, they built a simple hashing model in order to match images to more quantitatively show that their generated images do not match images in the training dataset. So, the authors concluded that their model was learning rather than memorizing.</p>
<h3 id="using-learned-feature-for-supervised-learning">Using Learned Feature for Supervised Learning</h3>
<p>They then continued to explore what exactly it was that their model was learning. They set up an experiment on both the CIFAR-10 and Street View House Numbers datasets in which they fully trained a DCGAN, extracted each of the Discriminator features maxpooled into a 4x4 representation, flattened, and concatenated, and finally trained a linear model on top of it for supervised classification. They then compared the results of the classification task as a proxy for the quality of the underlying representation, achieving near state of the art results on CIFAR-10 (and beating what they characterized as a strong k-means benchmark) and setting the new state of the art for the SVHN dataset.</p>
<p><img src="/Fall2021/images/gan2blog1/SVHN_Experiment.png" alt="SVHN Experiment Results" /></p>
<h3 id="exploring-the-latent-space">Exploring the Latent Space</h3>
<p>Next, they explored the latent space by picking 10 pairs of points for the original noise vectors and generating outputs at a series of points along the line connecting each pair. They found that generations shifted steadily from one image to another while remaining semantically sound, demonstrating a strong underlying representation as well as no signs of memorization (which would likely yield sharp jumps from one image to another).</p>
<p><img src="/Fall2021/images/gan2blog1/LatentSpace.png" alt="Exploration of the Latent Space" /></p>
<h3 id="removing-features-in-generations">Removing Features in Generations</h3>
<p>Next, using manual analysis and a logistic regression model, the authors identified the features which corresponded to windows in the LSUN bedroom dataset. Then, during forward passes they dropped all positive values from these features and replaced them with noise, seeing how it affected the final generations. They impressively found that while the generations did suffer and get blurrier, they remained semantically sound and crucially did not contain any windows, instead typically replacing them with walls or mirrors.</p>
<p><img src="/Fall2021/images/gan2blog1/ForgettingWindows.png" alt="Forgetting Windows" /></p>
<h3 id="performing-vector-arithmetic">Performing Vector Arithmetic</h3>
<p>Finally, the authors also performed vector arithmetic within the latent space. Instead of using a single point, they averaged points three or four points which shared a desired characteristic and then operated over that as the representation of the characteristic. In the model of the famous Word2Vec example in which the vector representation for King minus the representation for Man plus that for Woman  is extremely close to Queen’s representation, they were able to similarly perform constructions to create a smiling man, woman with glasses, and a vector which represented faces turned at a variety of angles.</p>
<p><img src="/Fall2021/images/gan2blog1/FaceArithmetic.png" alt="Face Vector Arithmetic" /></p>
<h2 id="wrapping-up">Wrapping Up</h2>
<p>So, we have described the field and landscape that the DCGAN entered into, the core features and novelties of the architecture, and the series of experiments which explored and characterized its abilities. We have seen that it was a fairly simple architecture that did not introduce any complex or even novel innovations but rather iterated on many different innovations present in the literature of the day until finding an extremely effective combination and architecture. In doing so, they arrived at the DCGAN which was able to stabilize training and produce a wide array of crisp, realistic images on a variety of datasets as well as learn a strong underlying image representation.</p>
<h2 id="reference">Reference</h2>
<p>Radford, A., Metz, L., and Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.</p>
</div>

  </div><a class="u-url" href="/Fall2021/gan/2021/10/19/GAN2_blog.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Fall2021/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Fall2021/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Fall2021/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blogs created as a part of graduate course at UIUC on deep generative and dynamical models.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/Fall2021/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/Fall2021/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
