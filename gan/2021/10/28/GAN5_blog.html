<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>GAN5 Generalization and Equilibrium in Generative Adversarial Nets (GANs) | CS 598 Deep Generative and Dynamical Models</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="GAN5 Generalization and Equilibrium in Generative Adversarial Nets (GANs)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Vivek Bhatt (vivekb2@illinois.edu)" />
<meta property="og:description" content="Vivek Bhatt (vivekb2@illinois.edu)" />
<link rel="canonical" href="https://cs598ban.github.io/Fall2021/gan/2021/10/28/GAN5_blog.html" />
<meta property="og:url" content="https://cs598ban.github.io/Fall2021/gan/2021/10/28/GAN5_blog.html" />
<meta property="og:site_name" content="CS 598 Deep Generative and Dynamical Models" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-28T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://cs598ban.github.io/Fall2021/gan/2021/10/28/GAN5_blog.html","@type":"BlogPosting","headline":"GAN5 Generalization and Equilibrium in Generative Adversarial Nets (GANs)","dateModified":"2021-10-28T00:00:00-05:00","datePublished":"2021-10-28T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cs598ban.github.io/Fall2021/gan/2021/10/28/GAN5_blog.html"},"description":"Vivek Bhatt (vivekb2@illinois.edu)","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Fall2021/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cs598ban.github.io/Fall2021/feed.xml" title="CS 598 Deep Generative and Dynamical Models" /><link rel="shortcut icon" type="image/x-icon" href="/Fall2021/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Fall2021/">CS 598 Deep Generative and Dynamical Models</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Fall2021/search/">Search</a><a class="page-link" href="/Fall2021/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">GAN5 Generalization and Equilibrium in Generative Adversarial Nets (GANs)</h1><p class="page-description">Vivek Bhatt (vivekb2@illinois.edu)</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-28T00:00:00-05:00" itemprop="datePublished">
        Oct 28, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Fall2021/categories/#GAN">GAN</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div class="stackedit__html">
<h2 id="intro">Intro</h2>
<p>This blog post focuses on the paper Generalization and Equilibrium in Generative Adversarial Nets (GANs) and focuses on the concept of generalization with applications to GANs. The paper introduces the concept of generalization and the fact that the GAN model does not perform good generalization. The paper introduces a new type of distance measure to better calculate the generalization factor. The paper subsequently comes up within a new model called the MIX-GAN which focuses on demonstrating the importance of generalization when it comes to GAN training. The paper aims to help reader’s understand the purpose of the GAN model and how the model can be improved through the implementation of generalization while training the GAN.</p>
<p>​<br /><br />
​<br /></p>
<h2 id="background">Background</h2>
<p>GANs were first introduced by Goodfellow et al. 2014 and the Goodfellow GAN can be described as follows:<br />
GANs are type of generative model used to come up with images or other samples to imitate real life samples. In more detail, the objective of the GAN when it comes to training a generator deep net whose input is a standard Gaussian, and whose output is a sample from some real distribution.</p>
<p>GANs work by comparing two distributions, as mentioned above, where the generator net part of the GAN attempts to “fool” the discriminator function into thinking that the generated image is a real image. This dual system is trained through backpropagation of the result from the discriminator that feeds into the generator. Thus the relationship between the generator and the discriminator can be thought of as a game between two players solving a min max scenario. This concept is further explored within the paper. The min max game is critical to understanding GANs and how they are trained.</p>
<br />
<br />
<p><img src="/Fall2021/images/gan5blog1/gan_equation.png" alt="GAN training equation" /></p>
<center>(traditional GAN training objective from Goodfellow 2014)</center>
<br />
<br />
<p>​<br />
In more detail the GAN objective function aims to minimize the output from the generator but the opposite is true for the discriminator which aims to maximize the result. The result is the label assigned to the training sample and then re-evaluated in the long term.<br />
​<br />
<br /><br />
<br /></p>
<h2 id="generalization">Generalization</h2>
<p>The definition of generalization in terms of GANs is related to how well the GAN is able to imitate the real distribution. A lot of the times the discriminator will be fooled but the derived distribution from the GAN model is weak and very ill representative of the actual distribution. Standard metrics are not as useful as measuring whether the distribution is close or not. Often there are times the GAN is able to train successfully to imitate the real distribution but according to standard metrics the trained distribution is deemed extremely different than the actual distribution, even though it might not be. Generalization is important when it comes to describing when the generator has won. At this point you need to consider is the created distribution and the real distribution the same or at least similar? This is tough to do when you have complicated distributions that have many valleys and peaks. Take for example this distribution:<br />
<br /><br />
<br /></p>
<p><img src="/Fall2021/images/gan5blog1/gan_model_dis.png" alt="GAN distribution" /></p>
<br />
<p>The distribution as mentioned has many peaks and valleys and when considering this distribution with high dimensionality there could be an exponential number of extrema. Sampling becomes a big issue. How do you sample so that all the extrema are accurately represented? How large would the sample size need to be for this to work? These questions are the underlying issue when it comes to modeling and working high dimensionality and complex distributions. The simple answer boils down to having a sample size of exponential size as well. However, that isn’t feasible and thus needs to be made more realistic.</p>
<br />
<br />
<h2 id="generalization-math">Generalization Math</h2>
<p>Let x1…xm be the training examples and let  D<sup>^</sup><sub>real</sub> be the uniform distribution for x<sub>1</sub>…x<sub>m</sub>. Also let G<sub>u</sub>(h1)…G<sub>u</sub>(hr) be a set of r examples from the generated distribution D<sub>G</sub>. In training, we use:</p>
<p><img src="/Fall2021/images/gan5blog1/gan_approx.png" alt="GAN_Approx" /></p>
<p>to approximate the value of:</p>
<p><img src="/Fall2021/images/gan5blog1/gan_actual.png" alt="GAN_Actual" /></p>
<p>Attempt to minimize the distance (or divergence) between D<sup>^</sup><sub>real</sub> and D<sub>G</sub>. This can be done to a certain extreme and so the final simplified version of this equation can be represented as:</p>
<p><img src="/Fall2021/images/gan5blog1/gan_full.png" alt="GAN_full" /></p>
<p>The equation above gives a more concrete mathematical equation for  Generalization and it can explained as the following: “generalization in GANs means that the population distance between the true and generated distribution is close to the empirical distance between the empirical distributions. Our target is to make the former distance small, whereas the latter one is what we can access and minimize in practice. The definition allows only polynomial number of samples from the generated distribution because the training algorithm should run in polynomial time.” The concept is a little confusing but can be simplified to this: in an ideal world we can draw an infinite number of samples from the true distribution to get the value for D<sub>real</sub>, however; in practicality that is not possible and so we have the empirical distance with a finite number of samples to estimate the expected value of the real distribution as seen above. The equation above takes it a step further and explicates that the distance between the empirical true distribution and empirical generated distribution is as close as it can be to the population distance between the population true distribution and the population generated distribution. The population distance is calculated theoretically and used in the calculation. Thus, the calculation is completed with an error bound that acts as measurement for the difference between the two distances. If the empirical distance is close enough to the population distance within this error bound, we can consider the empirical distance to be the same as the population distance.</p>
<br />
<br />
<h2 id="neural-net-distance">Neural Net Distance</h2>
<p>The paper brings up a problem with current metrics and that they are a bad method of measuring how well the generated distribution follows and imitates the original distribution. So, the authors behind the paper came up with a new measuring metrics strictly to see how well two distributions match each other. This metric is the neural net distance metric.</p>
<br />
<p>Neural Net Distance is derived from the f-divergence concept and follows the derivation of the f-divergence function closely.</p>
<p><img src="/Fall2021/images/gan5blog1/nnd.png" alt="Neural_Net_Distanace" /></p>
<p>For example when φ(t) is log(t) and F = {all functions from R<sup>d</sup> mapped to [0, 1]} then the derived equation above is equal to JS divergence. However, when φ(t) is t and F = {all 1-Lipschitz functions from R<sup>d</sup> mapped to [0, 1]} then the equation above is equal to Wasserstein distance.</p>
<ul>
<li>
<h3 id="lipschitz-functions-are-mentioned-and-lipschitz-refers-to-the-training-parameters-and-indicates-that-changing-a-parameter-by-delta-changes-the-output-of-the-deep-net-by-less-than-constant--delta.">(Lipschitz functions are mentioned and Lipschitz refers to the training parameters and indicates that changing a parameter by delta changes the output of the deep net by less than constant * delta.)</h3>
</li>
</ul>
<br />
<p>All this is to indicate that the distance metric is variable and changes with what kind of base GAN you are working with / what kind of distribution you’d like to measure.</p>
<p>GAN training uses F to be a class of neural nets with a bound p on the number of parameters. An assumption can be made regarding the measuring function such that it takes in values between  [−∆, ∆] and that it is L<sub>φ</sub>-Lipschitz.</p>
<!-- ![GAN_lipschitz](/Fall2021/images/gan5blog1/lip.png) -->
<!-- &lt;img src=&#34;/Fall2021/images/gan5blog1/lip.png&#34; alt=&#34;GAN_lipschitz&#34; width=&#34;37%&#34;/&gt; -->
<p>Even more so, F is a class of discriminators that is L-Lipschitz to the target distribution where p denotes the number of parameters in the target distribution.</p>
<p><img src="/Fall2021/images/gan5blog1/t3.1.png" alt="GAN_nnd_theorem" /></p>
<p>In a general sense you get the equation:</p>
<!-- ![GAN_nnd_eq](/Fall2021/images/gan5blog1/nnd_eq.png) -->
<img src="/Fall2021/images/gan5blog1/nnd_eq.png" alt="GAN_nnd_eq" width="30%" />
<p>The downside of the Neural Net Distance is that it will return a small value even if u and v are are far from each other. This is due to the capacity being limited by p.</p>
<br />
<br />
<h2 id="equilibrium--mixtures">Equilibrium &amp; Mixtures</h2>
<p>The idea of equilibrium stems from the GAN min max problem. When trying to train and optimize a GAN the generator must be minimized and the discriminator needs to be maximized. Equilibrium seeks a state where both the discriminator and the generator cannot be optimized further. The thing to keep in mind is that the saddle point reached is not necessarily zero. This paper puts a spin on that and recognizes an “equilibrium” value where generator is always less than or equal to this value. Conversely, the discriminator is always greater or equal to this value as demonstrated below.</p>
<br />
<p><img src="/Fall2021/images/gan5blog1/t4.1.png" alt="GAN_t4.1" /></p>
<br />
<p>The S value seen in the theorem is used to represent the strategies that the discriminator and generator use respectively. The strategies are unchanged and so there is no changing of strategies once the other player’s strategy is revealed. Each player still performs to the best of their ability keeping in mind their initial strategy.</p>
<br />
<p>The paper focuses only on the generator side as opposed to the disciminator. The reasons for the focus on the generator only is as follows:</p>
<ul>
<li>
<h3 id="payoff-is-generated-by-the-generator-br--br--the-payoff-is-generated-by-the-generator-first-sample-u-∼-ssubusub-h-∼-dsubhsub.-this-results-in-an-example-that-is-generated-x--gsubusubh.">Payoff is generated by the generator: <br /> <br /> The payoff is generated by the generator first sample u ∼ S<sub>u</sub>, h ∼ D<sub>h</sub>. This results in an example that is generated x = G<sub>u</sub>(h).</h3>
</li>
</ul>
<br />
<ul>
<li>
<h3 id="mixed-generator-composition-vs.-mixed-discriminator-br--br--the-mixed-generator-is-compromised-of-a-linear-mixture-of-generators.-the-mixed-discriminator-is-more-complicated-since-the-objective-function-of-the-discriminator-may-not-be-linear.-thus-when-accounting-for-the-objective-function-it-escapes-a-linear-space-otherwise-the-mixed-discriminator-would-also-be-linear.">Mixed Generator Composition vs. Mixed Discriminator: <br /> <br /> The mixed generator is compromised of a linear mixture of generators. The mixed discriminator is more complicated since the objective function of the discriminator may not be linear. Thus, when accounting for the objective function it escapes a linear space; otherwise the mixed discriminator would also be linear.</h3>
</li>
</ul>
<br />
<ul>
<li>
<h3 id="output-of-discriminator-is-not-important-br--br--the-output-of-the-discriminator-mixture-is-not-important-for-consideration-since-the-mixture-is-also-unable-to-differentiate-between-the-generated-and-actual-distribution-effectively.">Output of Discriminator is not important: <br /> <br /> The output of the discriminator mixture is not important for consideration since the mixture is also unable to differentiate between the generated and actual distribution effectively.</h3>
</li>
</ul>
<p>All these scenarios are resolved using a mixture generator and approximating the difference between the two distributions to a certain error value (as seen with all the formulas above).</p>
<br />
<p><img src="/Fall2021/images/gan5blog1/def3.png" alt="GAN_def3" /></p>
<br />
<p>This is what the strategies come out to be after accounting for a small margin of error. The basic idea here is that it takes O§ (where p is the number of parameters in the sample set) to approximate a quality distribution that is able to imitate the real distribution while accounting for the error.</p>
<p>There is a more complex version of this that takes into account pure strategies. The paper does not go into details about pure strategies but discusses a short overview about them. The idea boils down to the concept of mixtures, there is not much change from what is already written in this blog besides the fact that if a pure strategy is attempted then the complexity increases to O(p<sup>2</sup>) since each network compromises of O§ size and then those are used to make the mixture generator in a linear fashion.</p>
<br />
<br />
<h2 id="mix-gan">MIX-GAN</h2>
<p>A mix GAN is a GAN that makes use of the idea of a mixture of generators within reason. So they make use of a mixture of T components, where T is constrained by GPU memory (&lt; 5). Maintain T generators and T discriminators. All the respective weights for each generator is maintained and everything is trained through backwards propagation. The specific softmax used for training is listed below.</p>
<p><img src="/Fall2021/images/gan5blog1/softmax.png" alt="GAN_softmax" /></p>
<p>The softmax function is critical for defining the weights which in turn are needed for the payoff function for the MIX+GAN. You can think of it as the original min max problem for the GAN as the structure of the equations are similar.</p>
<p><img src="/Fall2021/images/gan5blog1/payoff.png" alt="GAN_payoff" /></p>
<br />
<br />
<h2 id="empirical-results">Empirical Results</h2>
<br />
<p><img src="/Fall2021/images/gan5blog1/result1.png" alt="GAN_result1" /></p>
<br />
<p><img src="/Fall2021/images/gan5blog1/result2.png" alt="GAN_result2" /></p>
<br />
<p>The examples above show the comparison between a typical DC-GAN and a MIX-DCGAN. The MIX-GAN is represented by the A side and the normal DCGAN is represented by the B side. THe interesting thing to note is the quality of the results of the GANs. The MIX-GAN performed slightly better in terms of more clear output compared to the results of the regular GAN.</p>
<br />
<p><img src="/Fall2021/images/gan5blog1/result3.png" alt="GAN_result3" /></p>
<br />
<p>This is further evidenced by the results of the CIFAR-10 test results. The MIX-GAN is able to compete to the same performance level of the GAN that it models. At times it does better than the GAN model that it is imitating. The most curious thing about the MIX-GAN is ability to be small and retain very little parameters yet be able to compete with more complex and larger GANs, as seen with the MIX-DCGAN and DCGAN (5x time) size scores.</p>
<br />
<br /> 
<h2 id="conclusions">Conclusions</h2>
<p>The concept of the MIX-GAN leads to more efficient GANs. They are typically smaller and perform the same as the normal version of the particular GAN model that they are attempting to turn into a mixture through the measuring function. The MIX-GAN allows for more quality samples and more diverse samples from the generated distribution because it is able to better reflect the true distribution unlike other GANs. On a theoretical standpoint this paper focused mainly improving the GAN’s ability to come to a good stopping point that properly reflects the true distribution. Many GANs fail to do this hence the team’s research into generalization and the neural net distance metric for a better way to determine if the generated distribution and real distribution are similar or not.</p>
<br />
<br /> 
<h2 id="references">References</h2>
<p>2017 (ICML): S. Arora, R. Ge, Y. Liang, T. Ma, Y. Zhang. Generalization and equilibrium in generative adversarial nets (GANs). ICML, 2017.</p>
</div>

  </div><a class="u-url" href="/Fall2021/gan/2021/10/28/GAN5_blog.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Fall2021/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Fall2021/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Fall2021/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blogs created as a part of graduate course at UIUC on deep generative and dynamical models.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/Fall2021/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/Fall2021/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
